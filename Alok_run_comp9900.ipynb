{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":52957,"status":"ok","timestamp":1730784171258,"user":{"displayName":"ALOK THOTTAKATHU PRASANNAKUMAR","userId":"10483471326433970042"},"user_tz":-660},"id":"Q89EJdGXhDCj","outputId":"9e531008-ed7b-4112-f7a6-79f13d5e1256"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","# Mount the Google Drive at /content/drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"collapsed":true,"executionInfo":{"elapsed":3547,"status":"ok","timestamp":1730628990926,"user":{"displayName":"ALOK THOTTAKATHU PRASANNAKUMAR","userId":"10483471326433970042"},"user_tz":-660},"id":"eMci0igOtRnD","outputId":"ec1e940c-e8cf-489a-c8a2-aaee0798885a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torch>=1.8 in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/ML/requirement.txt (line 1)) (2.5.0+cu121)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/ML/requirement.txt (line 2)) (1.26.4)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/ML/requirement.txt (line 3)) (2.2.2)\n","Requirement already satisfied: blobfile in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/ML/requirement.txt (line 4)) (3.0.0)\n","Requirement already satisfied: nibabel in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/ML/requirement.txt (line 5)) (5.3.2)\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/ML/requirement.txt (line 6)) (4.10.0.84)\n","Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/ML/requirement.txt (line 7)) (0.24.0)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/ML/requirement.txt (line 8)) (3.8.0)\n","Requirement already satisfied: batchgenerators in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/ML/requirement.txt (line 9)) (0.25)\n","Requirement already satisfied: visdom in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/ML/requirement.txt (line 10)) (0.2.4)\n","Requirement already satisfied: torchsummary in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/ML/requirement.txt (line 11)) (1.5.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8->-r /content/drive/MyDrive/ML/requirement.txt (line 1)) (3.16.1)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8->-r /content/drive/MyDrive/ML/requirement.txt (line 1)) (4.12.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8->-r /content/drive/MyDrive/ML/requirement.txt (line 1)) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8->-r /content/drive/MyDrive/ML/requirement.txt (line 1)) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8->-r /content/drive/MyDrive/ML/requirement.txt (line 1)) (2024.10.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8->-r /content/drive/MyDrive/ML/requirement.txt (line 1)) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.8->-r /content/drive/MyDrive/ML/requirement.txt (line 1)) (1.3.0)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->-r /content/drive/MyDrive/ML/requirement.txt (line 3)) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->-r /content/drive/MyDrive/ML/requirement.txt (line 3)) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->-r /content/drive/MyDrive/ML/requirement.txt (line 3)) (2024.2)\n","Requirement already satisfied: pycryptodomex>=3.8 in /usr/local/lib/python3.10/dist-packages (from blobfile->-r /content/drive/MyDrive/ML/requirement.txt (line 4)) (3.21.0)\n","Requirement already satisfied: urllib3<3,>=1.25.3 in /usr/local/lib/python3.10/dist-packages (from blobfile->-r /content/drive/MyDrive/ML/requirement.txt (line 4)) (2.2.3)\n","Requirement already satisfied: lxml>=4.9 in /usr/local/lib/python3.10/dist-packages (from blobfile->-r /content/drive/MyDrive/ML/requirement.txt (line 4)) (5.3.0)\n","Requirement already satisfied: importlib-resources>=5.12 in /usr/local/lib/python3.10/dist-packages (from nibabel->-r /content/drive/MyDrive/ML/requirement.txt (line 5)) (6.4.5)\n","Requirement already satisfied: packaging>=20 in /usr/local/lib/python3.10/dist-packages (from nibabel->-r /content/drive/MyDrive/ML/requirement.txt (line 5)) (24.1)\n","Requirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.10/dist-packages (from scikit-image->-r /content/drive/MyDrive/ML/requirement.txt (line 7)) (1.13.1)\n","Requirement already satisfied: pillow>=9.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->-r /content/drive/MyDrive/ML/requirement.txt (line 7)) (10.4.0)\n","Requirement already satisfied: imageio>=2.33 in /usr/local/lib/python3.10/dist-packages (from scikit-image->-r /content/drive/MyDrive/ML/requirement.txt (line 7)) (2.36.0)\n","Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.10/dist-packages (from scikit-image->-r /content/drive/MyDrive/ML/requirement.txt (line 7)) (2024.9.20)\n","Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.10/dist-packages (from scikit-image->-r /content/drive/MyDrive/ML/requirement.txt (line 7)) (0.4)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r /content/drive/MyDrive/ML/requirement.txt (line 8)) (1.3.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r /content/drive/MyDrive/ML/requirement.txt (line 8)) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r /content/drive/MyDrive/ML/requirement.txt (line 8)) (4.54.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r /content/drive/MyDrive/ML/requirement.txt (line 8)) (1.4.7)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r /content/drive/MyDrive/ML/requirement.txt (line 8)) (3.2.0)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from batchgenerators->-r /content/drive/MyDrive/ML/requirement.txt (line 9)) (1.5.2)\n","Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from batchgenerators->-r /content/drive/MyDrive/ML/requirement.txt (line 9)) (1.0.0)\n","Requirement already satisfied: unittest2 in /usr/local/lib/python3.10/dist-packages (from batchgenerators->-r /content/drive/MyDrive/ML/requirement.txt (line 9)) (1.1.0)\n","Requirement already satisfied: threadpoolctl in /usr/local/lib/python3.10/dist-packages (from batchgenerators->-r /content/drive/MyDrive/ML/requirement.txt (line 9)) (3.5.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from visdom->-r /content/drive/MyDrive/ML/requirement.txt (line 10)) (2.32.3)\n","Requirement already satisfied: tornado in /usr/local/lib/python3.10/dist-packages (from visdom->-r /content/drive/MyDrive/ML/requirement.txt (line 10)) (6.3.3)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from visdom->-r /content/drive/MyDrive/ML/requirement.txt (line 10)) (1.16.0)\n","Requirement already satisfied: jsonpatch in /usr/local/lib/python3.10/dist-packages (from visdom->-r /content/drive/MyDrive/ML/requirement.txt (line 10)) (1.33)\n","Requirement already satisfied: websocket-client in /usr/local/lib/python3.10/dist-packages (from visdom->-r /content/drive/MyDrive/ML/requirement.txt (line 10)) (1.8.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8->-r /content/drive/MyDrive/ML/requirement.txt (line 1)) (3.0.2)\n","Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch->visdom->-r /content/drive/MyDrive/ML/requirement.txt (line 10)) (3.0.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->visdom->-r /content/drive/MyDrive/ML/requirement.txt (line 10)) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->visdom->-r /content/drive/MyDrive/ML/requirement.txt (line 10)) (3.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->visdom->-r /content/drive/MyDrive/ML/requirement.txt (line 10)) (2024.8.30)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->batchgenerators->-r /content/drive/MyDrive/ML/requirement.txt (line 9)) (1.4.2)\n","Collecting argparse (from unittest2->batchgenerators->-r /content/drive/MyDrive/ML/requirement.txt (line 9))\n","  Using cached argparse-1.4.0-py2.py3-none-any.whl.metadata (2.8 kB)\n","Requirement already satisfied: traceback2 in /usr/local/lib/python3.10/dist-packages (from unittest2->batchgenerators->-r /content/drive/MyDrive/ML/requirement.txt (line 9)) (1.4.0)\n","Requirement already satisfied: linecache2 in /usr/local/lib/python3.10/dist-packages (from traceback2->unittest2->batchgenerators->-r /content/drive/MyDrive/ML/requirement.txt (line 9)) (1.0.0)\n","Using cached argparse-1.4.0-py2.py3-none-any.whl (23 kB)\n","Installing collected packages: argparse\n","Successfully installed argparse-1.4.0\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["argparse"]},"id":"eafd615d00e5435e8e4cea5729a19df5"}},"metadata":{}}],"source":["!pip install -r /content/drive/MyDrive/ML/requirement.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5671,"status":"ok","timestamp":1729419513358,"user":{"displayName":"ALOK THOTTAKATHU PRASANNAKUMAR","userId":"10483471326433970042"},"user_tz":-660},"id":"bjLkgDgJboEo","outputId":"501f886f-e8a8-408a-9b2d-3d5deda53d22"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting py-spy\n","  Downloading py_spy-0.3.14-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl.metadata (16 kB)\n","Downloading py_spy-0.3.14-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (3.0 MB)\n","\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/3.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/3.0 MB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m55.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: py-spy\n","Successfully installed py-spy-0.3.14\n"]}],"source":["!pip install py-spy"]},{"cell_type":"markdown","metadata":{"id":"HiTuFoHJOShH"},"source":["Training"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":389861,"status":"ok","timestamp":1729419987661,"user":{"displayName":"ALOK THOTTAKATHU PRASANNAKUMAR","userId":"10483471326433970042"},"user_tz":-660},"id":"Aj4UGuTHu5IS","outputId":"fed83b73-c96c-4728-9972-449db76ea9ac"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00% 100.00%    1.60s    103.2s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    1.51s     1.73s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K  0.00%   0.00%    1.09s     1.09s   _compile_bytecode (<frozen importlib._boots\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m17000\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m101.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K 65.00%  65.00%   84.02s    84.02s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K 30.00%  30.00%    8.06s     8.06s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    3.85s     3.85s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    2.79s     2.81s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  4.00%  69.00%    2.05s    72.85s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00% 100.00%    1.60s    104.2s   forward (guided_diffusion/unet.py)\n","\u001b[2K  1.00%   1.00%    1.52s     1.74s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K  0.00%   0.00%    1.09s     1.09s   _compile_bytecode (<frozen importlib._boots\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m17100\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m100.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K 92.00%  92.00%   84.94s    84.94s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  8.00%   8.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    3.85s     3.85s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    2.79s     2.81s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.05s    72.85s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00% 100.00%    1.60s    105.2s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    1.52s     1.74s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K  0.00%   0.00%    1.09s     1.09s   _compile_bytecode (<frozen importlib._boots\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m17200\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m100.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K100.00% 100.00%   85.94s    85.94s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    3.85s     3.85s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    2.79s     2.81s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.05s    72.85s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00% 100.00%    1.60s    106.2s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    1.52s     1.74s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K  0.00%   0.00%    1.09s     1.09s   _compile_bytecode (<frozen importlib._boots\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m17300\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m100.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K100.00% 100.00%   86.94s    86.94s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    3.85s     3.85s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    2.79s     2.81s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.05s    72.85s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00% 100.00%    1.60s    107.2s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    1.52s     1.74s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K  0.00%   0.00%    1.09s     1.09s   _compile_bytecode (<frozen importlib._boots\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m17400\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m102.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K100.00% 100.00%   87.94s    87.94s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    3.85s     3.85s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    2.79s     2.81s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.05s    72.85s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00% 100.00%    1.60s    108.2s   forward (guided_diffusion/unet.py)\n","\u001b[2K  2.00%   2.00%    1.54s     1.76s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K  0.00%   0.00%    1.09s     1.09s   _compile_bytecode (<frozen importlib._boots\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m17500\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m102.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K 79.00%  79.00%   88.73s    88.73s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    3.85s     3.85s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    2.79s     2.81s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.05s    72.85s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K 21.00% 100.00%    1.81s    109.2s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  2.00%   2.00%    1.56s     1.78s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K  0.00%   0.00%    1.09s     1.09s   _compile_bytecode (<frozen importlib._boots\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m17600\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m107.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   88.73s    88.73s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    3.85s     3.85s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K 59.00%  59.00%    3.38s     3.40s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K 41.00% 100.00%    2.22s    110.2s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%  59.00%    2.05s    73.44s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  7.00%   7.00%    1.63s     1.85s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K  0.00%   0.00%    1.09s     1.09s   _compile_bytecode (<frozen importlib._boots\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m17700\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m108.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   88.73s    88.73s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K 67.00%  67.00%    4.52s     4.52s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K 33.00%  33.00%    3.71s     3.73s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00% 100.00%    2.22s    111.2s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00% 100.00%    2.05s    74.44s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  8.00%   8.00%    1.71s     1.93s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K  0.00%   0.00%    1.09s     1.09s   _compile_bytecode (<frozen importlib._boots\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m17800\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m100.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K 24.00%  24.00%   88.97s    88.97s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K 76.00%  76.00%    5.28s     5.28s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    3.71s     3.73s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00% 100.00%    2.22s    112.2s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00% 100.00%    2.05s    75.44s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.71s     1.93s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K  0.00%   0.00%    1.09s     1.09s   _compile_bytecode (<frozen importlib._boots\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m17900\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m104.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K100.00% 100.00%   89.97s    89.97s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    5.28s     5.28s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    3.71s     3.73s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00% 100.00%    2.22s    113.2s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00% 100.00%    2.05s    76.44s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  4.00%   4.00%    1.75s     1.97s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K  0.00%   0.00%    1.09s     1.09s   _compile_bytecode (<frozen importlib._boots\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m18000\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m101.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K100.00% 100.00%   90.97s    90.97s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    5.28s     5.28s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    3.71s     3.73s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00% 100.00%    2.22s    114.2s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00% 100.00%    2.05s    77.44s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  1.00%   1.00%    1.76s     1.98s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K  0.00%   0.00%    1.09s     1.09s   _compile_bytecode (<frozen importlib._boots\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m18100\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m107.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K100.00% 100.00%   91.97s    91.97s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    5.28s     5.28s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    3.71s     3.73s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00% 100.00%    2.22s    115.2s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00% 100.00%    2.05s    78.44s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  4.00%   7.00%    1.80s     2.05s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K  0.00%   0.00%    1.09s     1.09s   _compile_bytecode (<frozen importlib._boots\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m18200\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m100.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K100.00% 100.00%   92.97s    92.97s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    5.28s     5.28s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    3.71s     3.73s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00% 100.00%    2.22s    116.2s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00% 100.00%    2.05s    79.44s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.80s     2.05s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K  0.00%   0.00%    1.09s     1.09s   _compile_bytecode (<frozen importlib._boots\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m18300\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m100.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K100.00% 100.00%   93.97s    93.97s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    5.28s     5.28s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    3.71s     3.73s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00% 100.00%    2.22s    117.2s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00% 100.00%    2.05s    80.44s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.80s     2.05s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K  0.00%   0.00%    1.09s     1.09s   _compile_bytecode (<frozen importlib._boots\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m18400\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m100.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K100.00% 100.00%   94.97s    94.97s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    5.28s     5.28s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    3.71s     3.73s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00% 100.00%    2.22s    118.2s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00% 100.00%    2.05s    81.44s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.80s     2.05s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K  0.00%   0.00%    1.09s     1.09s   _compile_bytecode (<frozen importlib._boots\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m18500\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m100.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K100.00% 100.00%   95.97s    95.97s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    5.28s     5.28s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    3.71s     3.73s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00% 100.00%    2.22s    119.2s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00% 100.00%    2.05s    82.44s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.80s     2.05s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K  0.00%   0.00%    1.09s     1.09s   _compile_bytecode (<frozen importlib._boots\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m18600\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m102.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K100.00% 100.00%   96.97s    96.97s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    5.28s     5.28s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    3.71s     3.73s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00% 100.00%    2.22s    120.2s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00% 100.00%    2.05s    83.44s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  2.00%   2.00%    1.82s     2.07s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K  0.00%   0.00%    1.09s     1.09s   _compile_bytecode (<frozen importlib._boots\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m18700\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m101.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K100.00% 100.00%   97.97s    97.97s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    5.28s     5.28s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    3.71s     3.73s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00% 100.00%    2.22s    121.2s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00% 100.00%    2.05s    84.44s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   1.00%    1.82s     2.08s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K  0.00%   0.00%    1.09s     1.09s   _compile_bytecode (<frozen importlib._boots\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m18800\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m100.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K100.00% 100.00%   98.97s    98.97s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    5.28s     5.28s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    3.71s     3.73s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00% 100.00%    2.22s    122.2s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00% 100.00%    2.05s    85.44s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.82s     2.08s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K  0.00%   0.00%    1.09s     1.09s   _compile_bytecode (<frozen importlib._boots\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m18900\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m100.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K100.00% 100.00%   99.97s    99.97s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    5.28s     5.28s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    3.71s     3.73s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00% 100.00%    2.22s    123.2s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00% 100.00%    2.05s    86.44s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.82s     2.08s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K  0.00%   0.00%    1.09s     1.09s   _compile_bytecode (<frozen importlib._boots\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m19000\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m100.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K 91.00%  91.00%   100.9s    100.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    5.28s     5.28s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    3.71s     3.73s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00% 100.00%    2.22s    124.2s   forward (guided_diffusion/unet.py)\n","\u001b[2K  9.00% 100.00%    2.14s    87.44s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.82s     2.08s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K  0.00%   0.00%    1.09s     1.09s   _compile_bytecode (<frozen importlib._boots\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m19100\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m100.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   100.9s    100.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K 49.00%  49.00%    5.77s     5.77s   silu (torch/nn/functional.py)\n","\u001b[2K 28.00%  28.00%    3.99s     4.01s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K 23.00% 100.00%    2.37s    88.44s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00% 100.00%    2.22s    125.2s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.82s     2.08s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K  0.00%   0.00%    1.09s     1.09s   _compile_bytecode (<frozen importlib._boots\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m19200\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m101.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K 82.00%  82.00%   101.7s    101.7s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K 18.00%  18.00%    5.95s     5.95s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    3.99s     4.01s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00% 100.00%    2.37s    89.44s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00% 100.00%    2.22s    126.2s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  1.00%   1.00%    1.83s     2.09s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K  0.00%   0.00%    1.09s     1.09s   _compile_bytecode (<frozen importlib._boots\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m19300\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m101.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K100.00% 100.00%   102.7s    102.7s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    5.95s     5.95s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    3.99s     4.01s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00% 100.00%    2.37s    90.44s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00% 100.00%    2.22s    127.2s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   1.00%    1.83s     2.10s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K  0.00%   0.00%    1.09s     1.09s   _compile_bytecode (<frozen importlib._boots\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m19400\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m104.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K100.00% 100.00%   103.7s    103.7s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    5.95s     5.95s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    3.99s     4.01s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00% 100.00%    2.37s    91.44s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00% 100.00%    2.22s    128.2s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  2.00%   4.00%    1.85s     2.14s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K  0.00%   0.00%    1.09s     1.09s   _compile_bytecode (<frozen importlib._boots\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m19500\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m100.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K100.00% 100.00%   104.7s    104.7s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    5.95s     5.95s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    3.99s     4.01s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00% 100.00%    2.37s    92.44s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00% 100.00%    2.22s    129.2s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.85s     2.14s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K  0.00%   0.00%    1.09s     1.09s   _compile_bytecode (<frozen importlib._boots\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m19600\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m100.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K100.00% 100.00%   105.7s    105.7s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    5.95s     5.95s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    3.99s     4.01s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00% 100.00%    2.37s    93.44s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00% 100.00%    2.22s    130.2s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.85s     2.14s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K  0.00%   0.00%    1.09s     1.09s   _compile_bytecode (<frozen importlib._boots\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m19700\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m100.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K100.00% 100.00%   106.7s    106.7s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    5.95s     5.95s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    3.99s     4.01s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00% 100.00%    2.37s    94.44s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00% 100.00%    2.22s    131.2s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.85s     2.14s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K  0.00%   0.00%    1.09s     1.09s   _compile_bytecode (<frozen importlib._boots\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m19800\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m100.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K 73.00%  73.00%   107.4s    107.4s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    5.95s     5.95s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    3.99s     4.01s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K 27.00% 100.00%    2.64s    95.44s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00% 100.00%    2.22s    132.2s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.85s     2.14s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K  0.00%   0.00%    1.09s     1.09s   _compile_bytecode (<frozen importlib._boots\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m19900\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m100.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   107.4s    107.4s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    5.95s     5.95s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    3.99s     4.01s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K 42.00%  42.00%    3.06s    95.86s   _forward (guided_diffusion/unet.py)\n","\u001b[2K 57.00% 100.00%    2.79s    133.2s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.85s     2.14s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K  0.00%   0.00%    1.09s     1.09s   _compile_bytecode (<frozen importlib._boots\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m20000\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m100.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   107.4s    107.4s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    5.95s     5.95s   silu (torch/nn/functional.py)\n","\u001b[2K 92.00%  92.00%    4.91s     4.93s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%  92.00%    3.06s    96.78s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  8.00% 100.00%    2.87s    134.2s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.85s     2.14s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K  0.00%   0.00%    1.09s     1.09s   _compile_bytecode (<frozen importlib._boots\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m20100\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m100.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   107.4s    107.4s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K 63.00%  63.00%    6.58s     6.58s   silu (torch/nn/functional.py)\n","\u001b[2K 37.00%  37.00%    5.28s     5.30s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00% 100.00%    3.06s    97.78s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00% 100.00%    2.87s    135.2s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.85s     2.14s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K  0.00%   0.00%    1.09s     1.09s   _compile_bytecode (<frozen importlib._boots\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m20200\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m100.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   107.4s    107.4s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K100.00% 100.00%    7.58s     7.58s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    5.28s     5.30s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00% 100.00%    3.06s    98.78s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00% 100.00%    2.87s    136.2s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.85s     2.14s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K  0.00%   0.00%    1.09s     1.09s   _compile_bytecode (<frozen importlib._boots\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m20300\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m100.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K 13.00%  13.00%   107.6s    107.6s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K 87.00%  87.00%    8.45s     8.45s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    5.28s     5.30s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00% 100.00%    3.06s    99.78s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00% 100.00%    2.87s    137.2s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.85s     2.14s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K  0.00%   0.00%    1.09s     1.09s   _compile_bytecode (<frozen importlib._boots\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m20400\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m103.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K100.00% 100.00%   108.6s    108.6s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%    8.45s     8.45s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    5.28s     5.30s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00% 100.00%    3.06s    100.8s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00% 100.00%    2.87s    138.2s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  3.00%   3.00%    1.88s     2.17s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K  0.00%   0.00%    1.09s     1.09s   _compile_bytecode (<frozen importlib._boots\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m20500\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m100.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K100.00% 100.00%   109.6s    109.6s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%    8.45s     8.45s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    5.28s     5.30s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00% 100.00%    3.06s    101.8s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00% 100.00%    2.87s    139.2s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.88s     2.17s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K  0.00%   0.00%    1.09s     1.09s   _compile_bytecode (<frozen importlib._boots\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m20600\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m100.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K100.00% 100.00%   110.6s    110.6s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%    8.45s     8.45s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    5.28s     5.30s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00% 100.00%    3.06s    102.8s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00% 100.00%    2.87s    140.2s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.88s     2.17s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K  0.00%   0.00%    1.09s     1.09s   _compile_bytecode (<frozen importlib._boots\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m20700\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m100.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K100.00% 100.00%   111.6s    111.6s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%    8.45s     8.45s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    5.28s     5.30s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00% 100.00%    3.06s    103.8s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00% 100.00%    2.87s    141.2s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.88s     2.17s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K  0.00%   0.00%    1.09s     1.09s   _compile_bytecode (<frozen importlib._boots\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m20800\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m102.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K100.00% 100.00%   112.6s    112.6s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%    8.45s     8.45s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    5.28s     5.30s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00% 100.00%    3.06s    104.8s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00% 100.00%    2.87s    142.2s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  2.00%   2.00%    1.90s     2.19s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K  0.00%   0.00%    1.09s     1.09s   _compile_bytecode (<frozen importlib._boots\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m20900\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m100.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K100.00% 100.00%   113.6s    113.6s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%    8.45s     8.45s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    5.28s     5.30s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00% 100.00%    3.06s    105.8s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00% 100.00%    2.87s    143.2s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.90s     2.19s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K  0.00%   0.00%    1.09s     1.09s   _compile_bytecode (<frozen importlib._boots\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m21000\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m102.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K100.00% 100.00%   114.6s    114.6s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%    8.45s     8.45s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    5.28s     5.30s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00% 100.00%    3.06s    106.8s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00% 100.00%    2.87s    144.2s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  2.00%   2.00%    1.92s     2.21s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K  0.00%   0.00%    1.09s     1.09s   _compile_bytecode (<frozen importlib._boots\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m21100\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m100.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K100.00% 100.00%   115.6s    115.6s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%    8.45s     8.45s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    5.28s     5.30s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00% 100.00%    3.06s    107.8s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00% 100.00%    2.87s    145.2s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.92s     2.21s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K  0.00%   0.00%    1.09s     1.09s   _compile_bytecode (<frozen importlib._boots\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m21200\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m100.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K100.00% 100.00%   116.6s    116.6s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%    8.45s     8.45s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    5.28s     5.30s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00% 100.00%    3.06s    108.8s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00% 100.00%    2.87s    146.2s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.92s     2.21s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K  0.00%   0.00%    1.09s     1.09s   _compile_bytecode (<frozen importlib._boots\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m21300\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m104.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K 13.00%  13.00%   116.7s    116.7s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K 32.00%  32.00%    8.77s     8.77s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K 28.00%  28.00%    5.56s     5.58s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K 27.00% 100.00%    3.33s    109.8s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00% 100.00%    2.87s    147.2s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  4.00%   4.00%    1.96s     2.25s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K  0.00%   0.00%    1.09s     1.09s   _compile_bytecode (<frozen importlib._boots\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m21400\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m100.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K100.00% 100.00%   117.7s    117.7s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%    8.77s     8.77s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    5.56s     5.58s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00% 100.00%    3.33s    110.8s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00% 100.00%    2.87s    148.2s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.96s     2.25s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K  0.00%   0.00%    1.09s     1.09s   _compile_bytecode (<frozen importlib._boots\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m21500\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m105.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K100.00% 100.00%   118.7s    118.7s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%    8.77s     8.77s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    5.56s     5.58s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00% 100.00%    3.33s    111.8s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00% 100.00%    2.87s    149.2s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  5.00%   5.00%    2.01s     2.30s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K  0.00%   0.00%    1.09s     1.09s   _compile_bytecode (<frozen importlib._boots\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m21600\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m104.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K100.00% 100.00%   119.7s    119.7s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%    8.77s     8.77s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    5.56s     5.58s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00% 100.00%    3.33s    112.8s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00% 100.00%    2.87s    150.2s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  4.00%   4.00%    2.05s     2.34s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K  0.00%   0.00%    1.09s     1.09s   _compile_bytecode (<frozen importlib._boots\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m21700\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m105.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K100.00% 100.00%   120.7s    120.7s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%    8.77s     8.77s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    5.56s     5.58s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00% 100.00%    3.33s    113.8s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00% 100.00%    2.87s    151.2s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  3.00%   5.00%    2.08s     2.39s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K  0.00%   0.00%    1.09s     1.09s   _compile_bytecode (<frozen importlib._boots\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m21800\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m105.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K100.00% 100.00%   121.7s    121.7s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%    8.77s     8.77s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    5.56s     5.58s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00% 100.00%    3.33s    114.8s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00% 100.00%    2.87s    152.2s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  5.00%   5.00%    2.13s     2.44s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K  0.00%   0.00%    1.09s     1.09s   _compile_bytecode (<frozen importlib._boots\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m21900\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m100.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K100.00% 100.00%   122.7s    122.7s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%    8.77s     8.77s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    5.56s     5.58s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00% 100.00%    3.33s    115.8s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00% 100.00%    2.87s    153.2s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.13s     2.44s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K  0.00%   0.00%    1.09s     1.09s   _compile_bytecode (<frozen importlib._boots\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2K\u001b[31m1.46s behind in sampling, results may be inaccurate. Try reducing the sampling rate.\u001b[0m\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m22000\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m100.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K 26.00%  26.00%   123.0s    123.0s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%    8.77s     8.77s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    5.56s     5.58s   group_norm (torch/nn/functional.py)\n","\u001b[2K 42.00%  68.00%    3.75s    116.5s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K 31.00% 100.00%    3.18s    154.2s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.13s     2.44s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2K\u001b[31m1.94s behind in sampling, results may be inaccurate. Try reducing the sampling rate.\u001b[0m\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m22100\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m100.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   123.0s    123.0s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K 29.00%  29.00%    9.06s     9.06s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K 62.00%  62.00%    6.18s     6.20s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%  91.00%    3.75s    117.4s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  9.00% 100.00%    3.27s    155.2s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.13s     2.44s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2K\u001b[31m2.18s behind in sampling, results may be inaccurate. Try reducing the sampling rate.\u001b[0m\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m22200\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m101.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   123.0s    123.0s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K100.00% 100.00%   10.06s    10.06s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    6.18s     6.20s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00% 100.00%    3.75s    118.4s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00% 100.00%    3.27s    156.2s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  1.00%   1.00%    2.14s     2.45s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2K\u001b[31m2.62s behind in sampling, results may be inaccurate. Try reducing the sampling rate.\u001b[0m\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m22300\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m100.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K 94.00%  94.00%   123.9s    123.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  6.00%   6.00%   10.12s    10.12s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    6.18s     6.20s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00% 100.00%    3.75s    119.4s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00% 100.00%    3.27s    157.2s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.14s     2.45s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2K\u001b[31m2.42s behind in sampling, results may be inaccurate. Try reducing the sampling rate.\u001b[0m\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m22400\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m100.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K100.00% 100.00%   124.9s    124.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.12s    10.12s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    6.18s     6.20s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00% 100.00%    3.75s    120.4s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00% 100.00%    3.27s    158.2s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.14s     2.45s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2K\u001b[31m2.61s behind in sampling, results may be inaccurate. Try reducing the sampling rate.\u001b[0m\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m22500\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m100.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K100.00% 100.00%   125.9s    125.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.12s    10.12s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    6.18s     6.20s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00% 100.00%    3.75s    121.4s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00% 100.00%    3.27s    159.2s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.14s     2.45s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2K\u001b[31m2.39s behind in sampling, results may be inaccurate. Try reducing the sampling rate.\u001b[0m\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m22600\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m100.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K100.00% 100.00%   126.9s    126.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.12s    10.12s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    6.18s     6.20s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00% 100.00%    3.75s    122.4s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00% 100.00%    3.27s    160.2s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.14s     2.45s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2K\u001b[31m2.39s behind in sampling, results may be inaccurate. Try reducing the sampling rate.\u001b[0m\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m22700\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m103.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K100.00% 100.00%   127.9s    127.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.12s    10.12s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    6.18s     6.20s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00% 100.00%    3.75s    123.4s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00% 100.00%    3.27s    161.2s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  1.00%   3.00%    2.15s     2.48s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2K\u001b[31m2.27s behind in sampling, results may be inaccurate. Try reducing the sampling rate.\u001b[0m\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m22800\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m100.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K100.00% 100.00%   128.9s    128.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.12s    10.12s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    6.18s     6.20s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00% 100.00%    3.75s    124.4s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00% 100.00%    3.27s    162.2s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.15s     2.48s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2K\u001b[31m1.77s behind in sampling, results may be inaccurate. Try reducing the sampling rate.\u001b[0m\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m22900\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m100.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K100.00% 100.00%   129.9s    129.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.12s    10.12s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    6.18s     6.20s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00% 100.00%    3.75s    125.4s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00% 100.00%    3.27s    163.2s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.15s     2.48s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2K\u001b[31m1.40s behind in sampling, results may be inaccurate. Try reducing the sampling rate.\u001b[0m\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m23000\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m100.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K100.00% 100.00%   130.9s    130.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.12s    10.12s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    6.18s     6.20s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00% 100.00%    3.75s    126.4s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00% 100.00%    3.27s    164.2s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.15s     2.48s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2K\u001b[31m1.22s behind in sampling, results may be inaccurate. Try reducing the sampling rate.\u001b[0m\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m23100\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m100.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K100.00% 100.00%   131.9s    131.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.12s    10.12s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    6.18s     6.20s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00% 100.00%    3.75s    127.4s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00% 100.00%    3.27s    165.2s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.15s     2.48s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m23200\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m100.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K100.00% 100.00%   132.9s    132.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.12s    10.12s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    6.18s     6.20s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00% 100.00%    3.75s    128.4s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00% 100.00%    3.27s    166.2s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.15s     2.48s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K  0.00%   0.00%    1.09s     1.09s   _compile_bytecode (<frozen importlib._boots\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m23300\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m106.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K100.00% 100.00%   133.9s    133.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.12s    10.12s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    6.18s     6.20s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00% 100.00%    3.75s    129.4s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00% 100.00%    3.27s    167.2s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  6.00%   6.00%    2.21s     2.54s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K  0.00%   0.00%    1.09s     1.09s   _compile_bytecode (<frozen importlib._boots\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m23400\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m100.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K100.00% 100.00%   134.9s    134.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.12s    10.12s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    6.18s     6.20s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00% 100.00%    3.75s    130.4s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00% 100.00%    3.27s    168.2s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.21s     2.54s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K  0.00%   0.00%    1.09s     1.09s   _compile_bytecode (<frozen importlib._boots\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m23500\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m104.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K 12.00%  12.00%   135.0s    135.0s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K 12.00%  12.00%   10.24s    10.24s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K 52.00%  52.00%    6.70s     6.72s   group_norm (torch/nn/functional.py)\n","\u001b[2K 24.00% 100.00%    3.99s    131.4s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00% 100.00%    3.27s    169.2s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  4.00%   4.00%    2.25s     2.58s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K  0.00%   0.00%    1.09s     1.09s   _compile_bytecode (<frozen importlib._boots\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m23600\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m107.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K 74.00%  74.00%   135.8s    135.8s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K 26.00%  26.00%   10.50s    10.50s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    6.70s     6.72s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00% 100.00%    3.99s    132.4s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00% 100.00%    3.27s    170.2s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  6.00%   7.00%    2.31s     2.65s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K  0.00%   0.00%    1.09s     1.09s   _compile_bytecode (<frozen importlib._boots\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m23700\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m102.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K100.00% 100.00%   136.8s    136.8s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.50s    10.50s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    6.70s     6.72s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00% 100.00%    3.99s    133.4s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00% 100.00%    3.27s    171.2s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  2.00%   2.00%    2.33s     2.67s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K  0.00%   0.00%    1.09s     1.09s   _compile_bytecode (<frozen importlib._boots\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m23800\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m100.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K100.00% 100.00%   137.8s    137.8s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.50s    10.50s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    6.70s     6.72s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00% 100.00%    3.99s    134.4s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00% 100.00%    3.27s    172.2s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.33s     2.67s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K  0.00%   0.00%    1.09s     1.09s   _compile_bytecode (<frozen importlib._boots\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m23900\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m102.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K100.00% 100.00%   138.8s    138.8s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.50s    10.50s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    6.70s     6.72s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00% 100.00%    3.99s    135.4s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00% 100.00%    3.27s    173.2s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  2.00%   2.00%    2.35s     2.69s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K  0.00%   0.00%    1.09s     1.09s   _compile_bytecode (<frozen importlib._boots\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m24000\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m100.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K100.00% 100.00%   139.8s    139.8s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.50s    10.50s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    6.70s     6.72s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00% 100.00%    3.99s    136.4s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00% 100.00%    3.27s    174.2s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.35s     2.69s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K  0.00%   0.00%    1.09s     1.09s   _compile_bytecode (<frozen importlib._boots\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m24100\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m101.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K100.00% 100.00%   140.8s    140.8s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.50s    10.50s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    6.70s     6.72s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00% 100.00%    3.99s    137.4s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00% 100.00%    3.27s    175.2s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  1.00%   1.00%    2.36s     2.70s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K  0.00%   0.00%    1.09s     1.09s   _compile_bytecode (<frozen importlib._boots\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m24200\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m101.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K 90.00%  90.00%   141.7s    141.7s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.50s    10.50s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    6.70s     6.72s   group_norm (torch/nn/functional.py)\n","\u001b[2K 10.00% 100.00%    4.09s    138.4s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00% 100.00%    3.27s    176.2s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   1.00%    2.36s     2.71s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K  0.00%   0.00%    1.09s     1.09s   _compile_bytecode (<frozen importlib._boots\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m24300\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m100.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   141.7s    141.7s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.50s    10.50s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K 64.00%  64.00%    7.34s     7.36s   group_norm (torch/nn/functional.py)\n","\u001b[2K 35.00%  35.00%    4.44s    138.7s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00% 100.00%    3.27s    177.2s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.36s     2.71s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K  0.00%   0.00%    1.09s     1.09s   _compile_bytecode (<frozen importlib._boots\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m24400\u001b[0m\n","\u001b[2KGIL: \u001b[1m1.00\u001b[0m%, Active: \u001b[1m101.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K 63.00%  63.00%   142.3s    142.3s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K 32.00%  32.00%   10.82s    10.82s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  3.00%   3.00%    7.37s     7.39s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    4.44s    138.7s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%  99.00%    3.27s    178.2s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  2.00%   2.00%    2.38s     2.73s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K  0.00%   0.00%    1.09s     1.09s   _compile_bytecode (<frozen importlib._boots\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m24500\u001b[0m\n","\u001b[2KGIL: \u001b[1m1.00\u001b[0m%, Active: \u001b[1m76.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K 62.00%  62.00%   142.9s    142.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.82s    10.82s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    7.37s     7.39s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    4.44s    138.7s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%  62.00%    3.27s    178.8s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  2.00%   2.00%    2.40s     2.75s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K  0.00%   0.00%    1.09s     1.09s   _compile_bytecode (<frozen importlib._boots\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m24600\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m100.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   142.9s    142.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.82s    10.82s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    7.37s     7.39s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    4.44s    138.7s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    3.27s    178.8s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.40s     2.75s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K  0.00%   0.00%    1.09s     1.09s   _compile_bytecode (<frozen importlib._boots\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m24700\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m101.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   142.9s    142.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.82s    10.82s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    7.37s     7.39s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    4.44s    138.7s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    3.27s    178.8s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  6.00%   6.00%    2.46s     2.81s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K 95.00%  95.00%    2.00s     2.00s   _engine_run_backward (torch/autograd/graph.\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m24800\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m110.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   142.9s    142.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.82s    10.82s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    7.37s     7.39s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    4.44s    138.7s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    3.27s    178.8s   forward (guided_diffusion/unet.py)\n","\u001b[2K 99.00%  99.00%    2.99s     2.99s   _engine_run_backward (torch/autograd/graph.\n","\u001b[2K  9.00%  11.00%    2.55s     2.92s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m24900\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m103.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   142.9s    142.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.82s    10.82s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    7.37s     7.39s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    4.44s    138.7s   _forward (guided_diffusion/unet.py)\n","\u001b[2K100.00% 100.00%    3.99s     3.99s   _engine_run_backward (torch/autograd/graph.\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    3.27s    178.8s   forward (guided_diffusion/unet.py)\n","\u001b[2K  3.00%   3.00%    2.58s     2.95s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2K\u001b[31m1.14s behind in sampling, results may be inaccurate. Try reducing the sampling rate.\u001b[0m\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m25000\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m102.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   142.9s    142.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.82s    10.82s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    7.37s     7.39s   group_norm (torch/nn/functional.py)\n","\u001b[2K100.00% 100.00%    4.99s     4.99s   _engine_run_backward (torch/autograd/graph.\n","\u001b[2K  0.00%   0.00%    4.44s    138.7s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    3.27s    178.8s   forward (guided_diffusion/unet.py)\n","\u001b[2K  2.00%   2.00%    2.60s     2.97s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2K\u001b[31m1.32s behind in sampling, results may be inaccurate. Try reducing the sampling rate.\u001b[0m\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m25100\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m100.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   142.9s    142.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.82s    10.82s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    7.37s     7.39s   group_norm (torch/nn/functional.py)\n","\u001b[2K100.00% 100.00%    5.99s     5.99s   _engine_run_backward (torch/autograd/graph.\n","\u001b[2K  0.00%   0.00%    4.44s    138.7s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    3.27s    178.8s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.60s     2.97s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2K\u001b[31m1.42s behind in sampling, results may be inaccurate. Try reducing the sampling rate.\u001b[0m\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m25200\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m100.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   142.9s    142.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.82s    10.82s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    7.37s     7.39s   group_norm (torch/nn/functional.py)\n","\u001b[2K100.00% 100.00%    6.99s     6.99s   _engine_run_backward (torch/autograd/graph.\n","\u001b[2K  0.00%   0.00%    4.44s    138.7s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    3.27s    178.8s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.60s     2.97s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2K\u001b[31m1.20s behind in sampling, results may be inaccurate. Try reducing the sampling rate.\u001b[0m\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m25300\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m100.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   142.9s    142.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.82s    10.82s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K100.00% 100.00%    7.99s     7.99s   _engine_run_backward (torch/autograd/graph.\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    7.37s     7.39s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    4.44s    138.7s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    3.27s    178.8s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.60s     2.97s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m25400\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m103.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   142.9s    142.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.82s    10.82s   silu (torch/nn/functional.py)\n","\u001b[2K100.00% 100.00%    8.99s     8.99s   _engine_run_backward (torch/autograd/graph.\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    7.37s     7.39s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    4.44s    138.7s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    3.27s    178.8s   forward (guided_diffusion/unet.py)\n","\u001b[2K  1.00%   3.00%    2.61s     3.00s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m25500\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m100.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   142.9s    142.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.82s    10.82s   silu (torch/nn/functional.py)\n","\u001b[2K100.00% 100.00%    9.99s     9.99s   _engine_run_backward (torch/autograd/graph.\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    7.37s     7.39s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    4.44s    138.7s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    3.27s    178.8s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.61s     3.00s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m25600\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m100.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   142.9s    142.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K100.00% 100.00%   10.99s    10.99s   _engine_run_backward (torch/autograd/graph.\n","\u001b[2K  0.00%   0.00%   10.82s    10.82s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    7.37s     7.39s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    4.44s    138.7s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    3.27s    178.8s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.61s     3.00s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m25700\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m100.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   142.9s    142.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K100.00% 100.00%   11.99s    11.99s   _engine_run_backward (torch/autograd/graph.\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.82s    10.82s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    7.37s     7.39s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    4.44s    138.7s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    3.27s    178.8s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.61s     3.00s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m25800\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m100.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   142.9s    142.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K100.00% 100.00%   12.99s    12.99s   _engine_run_backward (torch/autograd/graph.\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.82s    10.82s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    7.37s     7.39s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    4.44s    138.7s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    3.27s    178.8s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.61s     3.00s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m25900\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m100.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   142.9s    142.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K100.00% 100.00%   13.99s    13.99s   _engine_run_backward (torch/autograd/graph.\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.82s    10.82s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    7.37s     7.39s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    4.44s    138.7s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    3.27s    178.8s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.61s     3.00s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m26000\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m100.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   142.9s    142.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K100.00% 100.00%   14.99s    14.99s   _engine_run_backward (torch/autograd/graph.\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.82s    10.82s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    7.37s     7.39s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    4.44s    138.7s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    3.27s    178.8s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.61s     3.00s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m26100\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m100.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   142.9s    142.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K100.00% 100.00%   15.99s    15.99s   _engine_run_backward (torch/autograd/graph.\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.82s    10.82s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    7.37s     7.39s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    4.44s    138.7s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    3.27s    178.8s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.61s     3.00s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m26200\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m100.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   142.9s    142.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K100.00% 100.00%   16.99s    16.99s   _engine_run_backward (torch/autograd/graph.\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.82s    10.82s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    7.37s     7.39s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    4.44s    138.7s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    3.27s    178.8s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.61s     3.00s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m26300\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m99.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   142.9s    142.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K 99.00%  99.00%   17.98s    17.98s   _engine_run_backward (torch/autograd/graph.\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.82s    10.82s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    7.37s     7.39s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    4.44s    138.7s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    3.27s    178.8s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.61s     3.00s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m26400\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m100.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   142.9s    142.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K100.00% 100.00%   18.98s    18.98s   _engine_run_backward (torch/autograd/graph.\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.82s    10.82s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    7.37s     7.39s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    4.44s    138.7s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    3.27s    178.8s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.61s     3.00s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m26500\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m101.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   142.9s    142.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K100.00% 100.00%   19.98s    19.98s   _engine_run_backward (torch/autograd/graph.\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.82s    10.82s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    7.37s     7.39s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    4.44s    138.7s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    3.27s    178.8s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   1.00%    2.61s     3.01s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m26600\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m107.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   142.9s    142.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K100.00% 100.00%   20.98s    20.98s   _engine_run_backward (torch/autograd/graph.\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.82s    10.82s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    7.37s     7.39s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    4.44s    138.7s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    3.27s    178.8s   forward (guided_diffusion/unet.py)\n","\u001b[2K  7.00%   7.00%    2.68s     3.08s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m26700\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m102.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   142.9s    142.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K100.00% 100.00%   21.98s    21.98s   _engine_run_backward (torch/autograd/graph.\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.82s    10.82s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    7.37s     7.39s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    4.44s    138.7s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    3.27s    178.8s   forward (guided_diffusion/unet.py)\n","\u001b[2K  2.00%   2.00%    2.70s     3.10s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m26800\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m106.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   142.9s    142.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K100.00% 100.00%   22.98s    22.98s   _engine_run_backward (torch/autograd/graph.\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.82s    10.82s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    7.37s     7.39s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    4.44s    138.7s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    3.27s    178.8s   forward (guided_diffusion/unet.py)\n","\u001b[2K  6.00%   6.00%    2.76s     3.16s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m26900\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m100.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   142.9s    142.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K100.00% 100.00%   23.98s    23.98s   _engine_run_backward (torch/autograd/graph.\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.82s    10.82s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    7.37s     7.39s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    4.44s    138.7s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    3.27s    178.8s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.76s     3.16s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m27000\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m100.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   142.9s    142.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K100.00% 100.00%   24.98s    24.98s   _engine_run_backward (torch/autograd/graph.\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.82s    10.82s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    7.37s     7.39s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    4.44s    138.7s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    3.27s    178.8s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.76s     3.16s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m27100\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m104.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   142.9s    142.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K100.00% 100.00%   25.98s    25.98s   _engine_run_backward (torch/autograd/graph.\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.82s    10.82s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    7.37s     7.39s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    4.44s    138.7s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    3.27s    178.8s   forward (guided_diffusion/unet.py)\n","\u001b[2K  2.00%   4.00%    2.78s     3.20s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m27200\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m102.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   142.9s    142.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K100.00% 100.00%   26.98s    26.98s   _engine_run_backward (torch/autograd/graph.\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.82s    10.82s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    7.37s     7.39s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    4.44s    138.7s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    3.27s    178.8s   forward (guided_diffusion/unet.py)\n","\u001b[2K  2.00%   2.00%    2.80s     3.22s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2K\u001b[31m1.19s behind in sampling, results may be inaccurate. Try reducing the sampling rate.\u001b[0m\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m27300\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m102.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   142.9s    142.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K100.00% 100.00%   27.98s    27.98s   _engine_run_backward (torch/autograd/graph.\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.82s    10.82s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    7.37s     7.39s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    4.44s    138.7s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    3.27s    178.8s   forward (guided_diffusion/unet.py)\n","\u001b[2K  2.00%   2.00%    2.82s     3.24s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2K\u001b[31m1.12s behind in sampling, results may be inaccurate. Try reducing the sampling rate.\u001b[0m\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m27400\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m100.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   142.9s    142.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K100.00% 100.00%   28.98s    28.98s   _engine_run_backward (torch/autograd/graph.\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.82s    10.82s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    7.37s     7.39s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    4.44s    138.7s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    3.27s    178.8s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.82s     3.24s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2K\u001b[31m1.32s behind in sampling, results may be inaccurate. Try reducing the sampling rate.\u001b[0m\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m27500\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m100.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   142.9s    142.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K100.00% 100.00%   29.98s    29.98s   _engine_run_backward (torch/autograd/graph.\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.82s    10.82s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    7.37s     7.39s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    4.44s    138.7s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    3.27s    178.8s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.82s     3.24s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2K\u001b[31m1.26s behind in sampling, results may be inaccurate. Try reducing the sampling rate.\u001b[0m\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m27600\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m100.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   142.9s    142.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K100.00% 100.00%   30.98s    30.98s   _engine_run_backward (torch/autograd/graph.\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.82s    10.82s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    7.37s     7.39s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    4.44s    138.7s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    3.27s    178.8s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.82s     3.24s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2K\u001b[31m1.18s behind in sampling, results may be inaccurate. Try reducing the sampling rate.\u001b[0m\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m27700\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m100.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   142.9s    142.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K100.00% 100.00%   31.98s    31.98s   _engine_run_backward (torch/autograd/graph.\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.82s    10.82s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    7.37s     7.39s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    4.44s    138.7s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    3.27s    178.8s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.82s     3.24s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2K\u001b[31m1.05s behind in sampling, results may be inaccurate. Try reducing the sampling rate.\u001b[0m\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m27800\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m100.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   142.9s    142.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K100.00% 100.00%   32.98s    32.98s   _engine_run_backward (torch/autograd/graph.\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.82s    10.82s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    7.37s     7.39s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    4.44s    138.7s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    3.27s    178.8s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.82s     3.24s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m27900\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m100.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   142.9s    142.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K100.00% 100.00%   33.98s    33.98s   _engine_run_backward (torch/autograd/graph.\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.82s    10.82s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    7.37s     7.39s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    4.44s    138.7s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    3.27s    178.8s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.82s     3.24s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m28000\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m100.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   142.9s    142.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K100.00% 100.00%   34.98s    34.98s   _engine_run_backward (torch/autograd/graph.\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.82s    10.82s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    7.37s     7.39s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    4.44s    138.7s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    3.27s    178.8s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.82s     3.24s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m28100\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m104.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   142.9s    142.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K100.00% 100.00%   35.98s    35.98s   _engine_run_backward (torch/autograd/graph.\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.82s    10.82s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    7.37s     7.39s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    4.44s    138.7s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    3.27s    178.8s   forward (guided_diffusion/unet.py)\n","\u001b[2K  4.00%   4.00%    2.86s     3.28s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m28200\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m100.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   142.9s    142.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K100.00% 100.00%   36.98s    36.98s   _engine_run_backward (torch/autograd/graph.\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.82s    10.82s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    7.37s     7.39s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    4.44s    138.7s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    3.27s    178.8s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.86s     3.28s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m28300\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m100.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   142.9s    142.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K100.00% 100.00%   37.98s    37.98s   _engine_run_backward (torch/autograd/graph.\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.82s    10.82s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    7.37s     7.39s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    4.44s    138.7s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    3.27s    178.8s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.86s     3.28s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m28400\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m103.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   142.9s    142.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K100.00% 100.00%   38.98s    38.98s   _engine_run_backward (torch/autograd/graph.\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.82s    10.82s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    7.37s     7.39s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    4.44s    138.7s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    3.27s    178.8s   forward (guided_diffusion/unet.py)\n","\u001b[2K  3.00%   3.00%    2.89s     3.31s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m28500\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m101.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   142.9s    142.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K100.00% 100.00%   39.98s    39.98s   _engine_run_backward (torch/autograd/graph.\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.82s    10.82s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    7.37s     7.39s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    4.44s    138.7s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    3.27s    178.8s   forward (guided_diffusion/unet.py)\n","\u001b[2K  1.00%   1.00%    2.90s     3.32s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m28600\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m102.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   142.9s    142.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K100.00% 100.00%   40.98s    40.98s   _engine_run_backward (torch/autograd/graph.\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.82s    10.82s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    7.37s     7.39s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    4.44s    138.7s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    3.27s    178.8s   forward (guided_diffusion/unet.py)\n","\u001b[2K  2.00%   2.00%    2.92s     3.34s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m28700\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m100.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   142.9s    142.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K100.00% 100.00%   41.98s    41.98s   _engine_run_backward (torch/autograd/graph.\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.82s    10.82s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    7.37s     7.39s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    4.44s    138.7s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    3.27s    178.8s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.92s     3.34s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m28800\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m100.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   142.9s    142.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K100.00% 100.00%   42.98s    42.98s   _engine_run_backward (torch/autograd/graph.\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.82s    10.82s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    7.37s     7.39s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    4.44s    138.7s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    3.27s    178.8s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.92s     3.34s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m28900\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m101.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   142.9s    142.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K100.00% 100.00%   43.98s    43.98s   _engine_run_backward (torch/autograd/graph.\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.82s    10.82s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    7.37s     7.39s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    4.44s    138.7s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    3.27s    178.8s   forward (guided_diffusion/unet.py)\n","\u001b[2K  1.00%   1.00%    2.93s     3.35s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m29000\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m100.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   142.9s    142.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K100.00% 100.00%   44.98s    44.98s   _engine_run_backward (torch/autograd/graph.\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.82s    10.82s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    7.37s     7.39s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    4.44s    138.7s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    3.27s    178.8s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.93s     3.35s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m29100\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m100.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   142.9s    142.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K100.00% 100.00%   45.98s    45.98s   _engine_run_backward (torch/autograd/graph.\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.82s    10.82s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    7.37s     7.39s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    4.44s    138.7s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    3.27s    178.8s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.93s     3.35s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m29200\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m100.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   142.9s    142.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K100.00% 100.00%   46.98s    46.98s   _engine_run_backward (torch/autograd/graph.\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.82s    10.82s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    7.37s     7.39s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    4.44s    138.7s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    3.27s    178.8s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.93s     3.35s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m29300\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m103.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   142.9s    142.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K100.00% 100.00%   47.98s    47.98s   _engine_run_backward (torch/autograd/graph.\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.82s    10.82s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    7.37s     7.39s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    4.44s    138.7s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    3.27s    178.8s   forward (guided_diffusion/unet.py)\n","\u001b[2K  3.00%   3.00%    2.96s     3.38s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m29400\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m106.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   142.9s    142.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K100.00% 100.00%   48.98s    48.98s   _engine_run_backward (torch/autograd/graph.\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.82s    10.82s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    7.37s     7.39s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    4.44s    138.7s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    3.27s    178.8s   forward (guided_diffusion/unet.py)\n","\u001b[2K  6.00%   6.00%    3.02s     3.44s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m29500\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m110.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   142.9s    142.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K100.00% 100.00%   49.98s    49.98s   _engine_run_backward (torch/autograd/graph.\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.82s    10.82s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    7.37s     7.39s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    4.44s    138.7s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    3.27s    178.8s   forward (guided_diffusion/unet.py)\n","\u001b[2K 10.00%  10.00%    3.12s     3.54s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m29600\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m102.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   142.9s    142.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K100.00% 100.00%   50.98s    50.98s   _engine_run_backward (torch/autograd/graph.\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.82s    10.82s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    7.37s     7.39s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    4.44s    138.7s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    3.27s    178.8s   forward (guided_diffusion/unet.py)\n","\u001b[2K  2.00%   2.00%    3.14s     3.56s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m29700\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m101.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   142.9s    142.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K100.00% 100.00%   51.98s    51.98s   _engine_run_backward (torch/autograd/graph.\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.82s    10.82s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    7.37s     7.39s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    4.44s    138.7s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    3.27s    178.8s   forward (guided_diffusion/unet.py)\n","\u001b[2K  1.00%   1.00%    3.15s     3.57s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m29800\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m100.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   142.9s    142.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K100.00% 100.00%   52.98s    52.98s   _engine_run_backward (torch/autograd/graph.\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.82s    10.82s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    7.37s     7.39s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    4.44s    138.7s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    3.27s    178.8s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.15s     3.57s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m29900\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m101.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   142.9s    142.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K100.00% 100.00%   53.98s    53.98s   _engine_run_backward (torch/autograd/graph.\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.82s    10.82s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    7.37s     7.39s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    4.44s    138.7s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    3.27s    178.8s   forward (guided_diffusion/unet.py)\n","\u001b[2K  1.00%   1.00%    3.16s     3.58s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m30000\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m100.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   142.9s    142.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K100.00% 100.00%   54.98s    54.98s   _engine_run_backward (torch/autograd/graph.\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.82s    10.82s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    7.37s     7.39s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    4.44s    138.7s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    3.27s    178.8s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.16s     3.58s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m30100\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m100.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   142.9s    142.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K100.00% 100.00%   55.98s    55.98s   _engine_run_backward (torch/autograd/graph.\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.82s    10.82s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    7.37s     7.39s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    4.44s    138.7s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    3.27s    178.8s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.16s     3.58s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m30200\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m101.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   142.9s    142.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K100.00% 100.00%   56.98s    56.98s   _engine_run_backward (torch/autograd/graph.\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.82s    10.82s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    7.37s     7.39s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    4.44s    138.7s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    3.27s    178.8s   forward (guided_diffusion/unet.py)\n","\u001b[2K  1.00%   1.00%    3.17s     3.59s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m30300\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m100.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   142.9s    142.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K100.00% 100.00%   57.98s    57.98s   _engine_run_backward (torch/autograd/graph.\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.82s    10.82s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    7.37s     7.39s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    4.44s    138.7s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    3.27s    178.8s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.17s     3.59s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m30400\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m102.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   142.9s    142.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K100.00% 100.00%   58.98s    58.98s   _engine_run_backward (torch/autograd/graph.\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.82s    10.82s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    7.37s     7.39s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    4.44s    138.7s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    3.27s    178.8s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   2.00%    3.17s     3.61s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m30500\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m101.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   142.9s    142.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K100.00% 100.00%   59.98s    59.98s   _engine_run_backward (torch/autograd/graph.\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.82s    10.82s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    7.37s     7.39s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    4.44s    138.7s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    3.27s    178.8s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   1.00%    3.17s     3.62s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m30600\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m100.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   142.9s    142.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K100.00% 100.00%   60.98s    60.98s   _engine_run_backward (torch/autograd/graph.\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.82s    10.82s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    7.37s     7.39s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    4.44s    138.7s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    3.27s    178.8s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.17s     3.62s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m30700\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m100.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   142.9s    142.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K100.00% 100.00%   61.98s    61.98s   _engine_run_backward (torch/autograd/graph.\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.82s    10.82s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    7.37s     7.39s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    4.44s    138.7s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    3.27s    178.8s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.17s     3.62s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m30800\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m105.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   142.9s    142.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K100.00% 100.00%   62.98s    62.98s   _engine_run_backward (torch/autograd/graph.\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.82s    10.82s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    7.37s     7.39s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    4.44s    138.7s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    3.27s    178.8s   forward (guided_diffusion/unet.py)\n","\u001b[2K  5.00%   5.00%    3.22s     3.67s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m30900\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m100.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   142.9s    142.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K100.00% 100.00%   63.98s    63.98s   _engine_run_backward (torch/autograd/graph.\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.82s    10.82s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    7.37s     7.39s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    4.44s    138.7s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    3.27s    178.8s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.22s     3.67s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m31000\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m100.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   142.9s    142.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K100.00% 100.00%   64.98s    64.98s   _engine_run_backward (torch/autograd/graph.\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.82s    10.82s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    7.37s     7.39s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    4.44s    138.7s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    3.27s    178.8s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.22s     3.67s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m31100\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m100.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   142.9s    142.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K100.00% 100.00%   65.98s    65.98s   _engine_run_backward (torch/autograd/graph.\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.82s    10.82s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    7.37s     7.39s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    4.44s    138.7s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    3.27s    178.8s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.22s     3.67s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m31200\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m100.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   142.9s    142.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K100.00% 100.00%   66.98s    66.98s   _engine_run_backward (torch/autograd/graph.\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.82s    10.82s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    7.37s     7.39s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    4.44s    138.7s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    3.27s    178.8s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.22s     3.67s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m31300\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m100.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   142.9s    142.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K100.00% 100.00%   67.98s    67.98s   _engine_run_backward (torch/autograd/graph.\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.82s    10.82s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    7.37s     7.39s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    4.44s    138.7s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    3.27s    178.8s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.22s     3.67s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m31400\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m100.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   142.9s    142.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K100.00% 100.00%   68.98s    68.98s   _engine_run_backward (torch/autograd/graph.\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.82s    10.82s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    7.37s     7.39s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    4.44s    138.7s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    3.27s    178.8s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.22s     3.67s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m31500\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m104.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   142.9s    142.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K100.00% 100.00%   69.98s    69.98s   _engine_run_backward (torch/autograd/graph.\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.82s    10.82s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    7.37s     7.39s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    4.44s    138.7s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    3.27s    178.8s   forward (guided_diffusion/unet.py)\n","\u001b[2K  4.00%   4.00%    3.26s     3.71s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m31600\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m100.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   142.9s    142.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K100.00% 100.00%   70.98s    70.98s   _engine_run_backward (torch/autograd/graph.\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.82s    10.82s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    7.37s     7.39s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    4.44s    138.7s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    3.27s    178.8s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.26s     3.71s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m31700\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m100.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   142.9s    142.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K100.00% 100.00%   71.98s    71.98s   _engine_run_backward (torch/autograd/graph.\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.82s    10.82s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    7.37s     7.39s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    4.44s    138.7s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    3.27s    178.8s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.26s     3.71s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m31800\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m100.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   142.9s    142.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K100.00% 100.00%   72.98s    72.98s   _engine_run_backward (torch/autograd/graph.\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.82s    10.82s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    7.37s     7.39s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    4.44s    138.7s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    3.27s    178.8s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.26s     3.71s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m31900\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m100.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   142.9s    142.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K100.00% 100.00%   73.98s    73.98s   _engine_run_backward (torch/autograd/graph.\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.82s    10.82s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    7.37s     7.39s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    4.44s    138.7s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    3.27s    178.8s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.26s     3.71s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m32000\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m100.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   142.9s    142.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K100.00% 100.00%   74.98s    74.98s   _engine_run_backward (torch/autograd/graph.\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.82s    10.82s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    7.37s     7.39s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    4.44s    138.7s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    3.27s    178.8s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.26s     3.71s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m32100\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m100.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   142.9s    142.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K100.00% 100.00%   75.98s    75.98s   _engine_run_backward (torch/autograd/graph.\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.82s    10.82s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    7.37s     7.39s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    4.44s    138.7s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    3.27s    178.8s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.26s     3.71s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m32200\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m101.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   142.9s    142.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K100.00% 100.00%   76.98s    76.98s   _engine_run_backward (torch/autograd/graph.\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.82s    10.82s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    7.37s     7.39s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    4.44s    138.7s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    3.27s    178.8s   forward (guided_diffusion/unet.py)\n","\u001b[2K  1.00%   1.00%    3.27s     3.72s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m32300\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m100.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   142.9s    142.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K100.00% 100.00%   77.98s    77.98s   _engine_run_backward (torch/autograd/graph.\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.82s    10.82s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    7.37s     7.39s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    4.44s    138.7s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    3.27s    178.8s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.27s     3.72s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m32400\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m101.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   142.9s    142.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K100.00% 100.00%   78.98s    78.98s   _engine_run_backward (torch/autograd/graph.\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.82s    10.82s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    7.37s     7.39s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    4.44s    138.7s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  1.00%   1.00%    3.28s     3.73s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    3.27s    178.8s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m32500\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m100.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   142.9s    142.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K100.00% 100.00%   79.98s    79.98s   _engine_run_backward (torch/autograd/graph.\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.82s    10.82s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    7.37s     7.39s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    4.44s    138.7s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    3.28s     3.73s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    3.27s    178.8s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m32600\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m100.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   142.9s    142.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K100.00% 100.00%   80.98s    80.98s   _engine_run_backward (torch/autograd/graph.\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.82s    10.82s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    7.37s     7.39s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    4.44s    138.7s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    3.28s     3.73s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    3.27s    178.8s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m32700\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m103.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   142.9s    142.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K100.00% 100.00%   81.98s    81.98s   _engine_run_backward (torch/autograd/graph.\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.82s    10.82s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    7.37s     7.39s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    4.44s    138.7s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  2.00%   3.00%    3.30s     3.76s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    3.27s    178.8s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m32800\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m100.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   142.9s    142.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K100.00% 100.00%   82.98s    82.98s   _engine_run_backward (torch/autograd/graph.\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.82s    10.82s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    7.37s     7.39s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    4.44s    138.7s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    3.30s     3.76s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    3.27s    178.8s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m32900\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m100.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   142.9s    142.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K100.00% 100.00%   83.98s    83.98s   _engine_run_backward (torch/autograd/graph.\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.82s    10.82s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    7.37s     7.39s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    4.44s    138.7s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    3.30s     3.76s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    3.27s    178.8s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m33000\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m100.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   142.9s    142.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K100.00% 100.00%   84.98s    84.98s   _engine_run_backward (torch/autograd/graph.\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.82s    10.82s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    7.37s     7.39s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    4.44s    138.7s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    3.30s     3.76s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    3.27s    178.8s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m33100\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m100.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   142.9s    142.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K100.00% 100.00%   85.98s    85.98s   _engine_run_backward (torch/autograd/graph.\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.82s    10.82s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    7.37s     7.39s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    4.44s    138.7s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    3.30s     3.76s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    3.27s    178.8s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m33200\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m100.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   142.9s    142.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K100.00% 100.00%   86.98s    86.98s   _engine_run_backward (torch/autograd/graph.\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.82s    10.82s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    7.37s     7.39s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    4.44s    138.7s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    3.30s     3.76s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    3.27s    178.8s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m33300\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m100.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   142.9s    142.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K100.00% 100.00%   87.98s    87.98s   _engine_run_backward (torch/autograd/graph.\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.82s    10.82s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    7.37s     7.39s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    4.44s    138.7s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    3.30s     3.76s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    3.27s    178.8s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m33400\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m100.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   142.9s    142.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K100.00% 100.00%   88.98s    88.98s   _engine_run_backward (torch/autograd/graph.\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.82s    10.82s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    7.37s     7.39s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    4.44s    138.7s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    3.30s     3.76s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    3.27s    178.8s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m33500\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m100.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   142.9s    142.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K100.00% 100.00%   89.98s    89.98s   _engine_run_backward (torch/autograd/graph.\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.82s    10.82s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    7.37s     7.39s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    4.44s    138.7s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    3.30s     3.76s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    3.27s    178.8s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m33600\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m100.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   142.9s    142.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K100.00% 100.00%   90.98s    90.98s   _engine_run_backward (torch/autograd/graph.\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.82s    10.82s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    7.37s     7.39s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    4.44s    138.7s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    3.30s     3.76s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    3.27s    178.8s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m33700\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m101.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   142.9s    142.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K100.00% 100.00%   91.98s    91.98s   _engine_run_backward (torch/autograd/graph.\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.82s    10.82s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    7.37s     7.39s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    4.44s    138.7s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  1.00%   1.00%    3.31s     3.77s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    3.27s    178.8s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m33800\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m100.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   142.9s    142.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K100.00% 100.00%   92.98s    92.98s   _engine_run_backward (torch/autograd/graph.\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.82s    10.82s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    7.37s     7.39s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    4.44s    138.7s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    3.31s     3.77s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    3.27s    178.8s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m33900\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m101.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   142.9s    142.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K100.00% 100.00%   93.98s    93.98s   _engine_run_backward (torch/autograd/graph.\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.82s    10.82s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    7.37s     7.39s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    4.44s    138.7s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  1.00%   1.00%    3.32s     3.78s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    3.27s    178.8s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m34000\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m100.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   142.9s    142.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K100.00% 100.00%   94.98s    94.98s   _engine_run_backward (torch/autograd/graph.\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.82s    10.82s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    7.37s     7.39s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    4.44s    138.7s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    3.32s     3.78s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    3.27s    178.8s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m34100\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m101.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   142.9s    142.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K100.00% 100.00%   95.98s    95.98s   _engine_run_backward (torch/autograd/graph.\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.82s    10.82s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    7.37s     7.39s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    4.44s    138.7s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  1.00%   1.00%    3.33s     3.79s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    3.27s    178.8s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m34200\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m107.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   142.9s    142.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K100.00% 100.00%   96.98s    96.98s   _engine_run_backward (torch/autograd/graph.\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.82s    10.82s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    7.37s     7.39s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    4.44s    138.7s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  7.00%   7.00%    3.40s     3.86s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    3.27s    178.8s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m34300\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m103.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   142.9s    142.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K100.00% 100.00%   97.98s    97.98s   _engine_run_backward (torch/autograd/graph.\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.82s    10.82s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    7.37s     7.39s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    4.44s    138.7s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  3.00%   3.00%    3.43s     3.89s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    3.27s    178.8s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m34400\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m102.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   142.9s    142.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K100.00% 100.00%   98.98s    98.98s   _engine_run_backward (torch/autograd/graph.\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.82s    10.82s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    7.37s     7.39s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    4.44s    138.7s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  2.00%   2.00%    3.45s     3.91s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    3.27s    178.8s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m34500\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m100.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   142.9s    142.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K100.00% 100.00%   99.98s    99.98s   _engine_run_backward (torch/autograd/graph.\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.82s    10.82s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    7.37s     7.39s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    4.44s    138.7s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    3.45s     3.91s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    3.27s    178.8s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m34600\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m100.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   142.9s    142.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K100.00% 100.00%   101.0s    101.0s   _engine_run_backward (torch/autograd/graph.\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.82s    10.82s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    7.37s     7.39s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    4.44s    138.7s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    3.45s     3.91s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    3.27s    178.8s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m34700\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m100.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   142.9s    142.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K100.00% 100.00%   102.0s    102.0s   _engine_run_backward (torch/autograd/graph.\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.82s    10.82s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    7.37s     7.39s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    4.44s    138.7s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    3.45s     3.91s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    3.27s    178.8s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m34800\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m100.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   142.9s    142.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K100.00% 100.00%   103.0s    103.0s   _engine_run_backward (torch/autograd/graph.\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.82s    10.82s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    7.37s     7.39s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    4.44s    138.7s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    3.45s     3.91s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    3.27s    178.8s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m34900\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m101.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   142.9s    142.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K100.00% 100.00%   104.0s    104.0s   _engine_run_backward (torch/autograd/graph.\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.82s    10.82s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    7.37s     7.39s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    4.44s    138.7s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  1.00%   1.00%    3.46s     3.92s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    3.27s    178.8s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m35000\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m102.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   142.9s    142.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K100.00% 100.00%   105.0s    105.0s   _engine_run_backward (torch/autograd/graph.\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.82s    10.82s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    7.37s     7.39s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    4.44s    138.7s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  2.00%   2.00%    3.48s     3.94s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    3.27s    178.8s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m35100\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m102.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   142.9s    142.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K100.00% 100.00%   106.0s    106.0s   _engine_run_backward (torch/autograd/graph.\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.82s    10.82s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    7.37s     7.39s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    4.44s    138.7s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  2.00%   2.00%    3.50s     3.96s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    3.27s    178.8s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m35200\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m100.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   142.9s    142.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K100.00% 100.00%   107.0s    107.0s   _engine_run_backward (torch/autograd/graph.\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.82s    10.82s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    7.37s     7.39s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    4.44s    138.7s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    3.50s     3.96s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    3.27s    178.8s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m35300\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m104.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   142.9s    142.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K100.00% 100.00%   108.0s    108.0s   _engine_run_backward (torch/autograd/graph.\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.82s    10.82s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    7.37s     7.39s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    4.44s    138.7s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  4.00%   4.00%    3.54s     4.00s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    3.27s    178.8s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m35400\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m100.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   142.9s    142.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K100.00% 100.00%   109.0s    109.0s   _engine_run_backward (torch/autograd/graph.\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.82s    10.82s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    7.37s     7.39s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    4.44s    138.7s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    3.54s     4.00s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    3.27s    178.8s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m35500\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m102.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   142.9s    142.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K100.00% 100.00%   110.0s    110.0s   _engine_run_backward (torch/autograd/graph.\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.82s    10.82s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    7.37s     7.39s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    4.44s    138.7s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  2.00%   2.00%    3.56s     4.02s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    3.27s    178.8s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m35600\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m100.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   142.9s    142.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K100.00% 100.00%   111.0s    111.0s   _engine_run_backward (torch/autograd/graph.\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.82s    10.82s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    7.37s     7.39s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    4.44s    138.7s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    3.56s     4.02s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    3.27s    178.8s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m35700\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m103.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   142.9s    142.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K100.00% 100.00%   112.0s    112.0s   _engine_run_backward (torch/autograd/graph.\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.82s    10.82s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    7.37s     7.39s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    4.44s    138.7s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  3.00%   3.00%    3.59s     4.05s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    3.27s    178.8s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m35800\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m100.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   142.9s    142.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K100.00% 100.00%   113.0s    113.0s   _engine_run_backward (torch/autograd/graph.\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.82s    10.82s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    7.37s     7.39s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    4.44s    138.7s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    3.59s     4.05s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    3.27s    178.8s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m35900\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m100.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   142.9s    142.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K100.00% 100.00%   114.0s    114.0s   _engine_run_backward (torch/autograd/graph.\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.82s    10.82s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    7.37s     7.39s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    4.44s    138.7s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    3.59s     4.05s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    3.27s    178.8s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m36000\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m100.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   142.9s    142.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K100.00% 100.00%   115.0s    115.0s   _engine_run_backward (torch/autograd/graph.\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.82s    10.82s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    7.37s     7.39s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    4.44s    138.7s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    3.59s     4.05s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    3.27s    178.8s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m36100\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m102.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   142.9s    142.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K100.00% 100.00%   116.0s    116.0s   _engine_run_backward (torch/autograd/graph.\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.82s    10.82s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    7.37s     7.39s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    4.44s    138.7s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  2.00%   2.00%    3.61s     4.07s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    3.27s    178.8s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2K\u001b[31m1.18s behind in sampling, results may be inaccurate. Try reducing the sampling rate.\u001b[0m\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m36200\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m110.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   142.9s    142.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K100.00% 100.00%   117.0s    117.0s   _engine_run_backward (torch/autograd/graph.\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.82s    10.82s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    7.37s     7.39s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    4.44s    138.7s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K 10.00%  10.00%    3.71s     4.17s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    3.27s    178.8s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2K\u001b[31m1.22s behind in sampling, results may be inaccurate. Try reducing the sampling rate.\u001b[0m\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m36300\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m100.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   142.9s    142.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K100.00% 100.00%   118.0s    118.0s   _engine_run_backward (torch/autograd/graph.\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.82s    10.82s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    7.37s     7.39s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    4.44s    138.7s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.17s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    3.27s    178.8s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2K\u001b[31m1.53s behind in sampling, results may be inaccurate. Try reducing the sampling rate.\u001b[0m\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m36400\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m106.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   142.9s    142.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K100.00% 100.00%   119.0s    119.0s   _engine_run_backward (torch/autograd/graph.\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.82s    10.82s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    7.37s     7.39s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    4.44s    138.7s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  5.00%   6.00%    3.76s     4.23s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    3.27s    178.8s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2K\u001b[31m1.60s behind in sampling, results may be inaccurate. Try reducing the sampling rate.\u001b[0m\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m36500\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m101.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   142.9s    142.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K100.00% 100.00%   120.0s    120.0s   _engine_run_backward (torch/autograd/graph.\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.82s    10.82s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    7.37s     7.39s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    4.44s    138.7s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  1.00%   1.00%    3.77s     4.24s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    3.27s    178.8s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2K\u001b[31m1.52s behind in sampling, results may be inaccurate. Try reducing the sampling rate.\u001b[0m\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m36600\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m102.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   142.9s    142.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K100.00% 100.00%   121.0s    121.0s   _engine_run_backward (torch/autograd/graph.\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.82s    10.82s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    7.37s     7.39s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    4.44s    138.7s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  2.00%   2.00%    3.79s     4.26s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    3.27s    178.8s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2K\u001b[31m1.56s behind in sampling, results may be inaccurate. Try reducing the sampling rate.\u001b[0m\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m36700\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m100.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   142.9s    142.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K100.00% 100.00%   122.0s    122.0s   _engine_run_backward (torch/autograd/graph.\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.82s    10.82s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    7.37s     7.39s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    4.44s    138.7s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.79s     4.26s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    3.27s    178.8s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2K\u001b[31m1.20s behind in sampling, results may be inaccurate. Try reducing the sampling rate.\u001b[0m\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m36800\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m100.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   142.9s    142.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K100.00% 100.00%   123.0s    123.0s   _engine_run_backward (torch/autograd/graph.\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.82s    10.82s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    7.37s     7.39s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    4.44s    138.7s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.79s     4.26s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    3.27s    178.8s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2K\u001b[31m1.14s behind in sampling, results may be inaccurate. Try reducing the sampling rate.\u001b[0m\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m36900\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m101.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   142.9s    142.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K100.00% 100.00%   124.0s    124.0s   _engine_run_backward (torch/autograd/graph.\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.82s    10.82s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    7.37s     7.39s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    4.44s    138.7s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  1.00%   1.00%    3.80s     4.27s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    3.27s    178.8s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m37000\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m100.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   142.9s    142.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K100.00% 100.00%   125.0s    125.0s   _engine_run_backward (torch/autograd/graph.\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.82s    10.82s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    7.37s     7.39s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    4.44s    138.7s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.80s     4.27s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    3.27s    178.8s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m37100\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m100.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   142.9s    142.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K100.00% 100.00%   126.0s    126.0s   _engine_run_backward (torch/autograd/graph.\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.82s    10.82s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    7.37s     7.39s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    4.44s    138.7s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.80s     4.27s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    3.27s    178.8s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m37200\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m100.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   142.9s    142.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K100.00% 100.00%   127.0s    127.0s   _engine_run_backward (torch/autograd/graph.\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.82s    10.82s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    7.37s     7.39s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    4.44s    138.7s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.80s     4.27s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    3.27s    178.8s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m37300\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m101.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   142.9s    142.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K100.00% 100.00%   128.0s    128.0s   _engine_run_backward (torch/autograd/graph.\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.82s    10.82s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    7.37s     7.39s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    4.44s    138.7s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  1.00%   1.00%    3.81s     4.28s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    3.27s    178.8s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m37400\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m100.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   142.9s    142.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K100.00% 100.00%   129.0s    129.0s   _engine_run_backward (torch/autograd/graph.\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.82s    10.82s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    7.37s     7.39s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    4.44s    138.7s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.81s     4.28s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    3.27s    178.8s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m37500\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m100.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   142.9s    142.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K100.00% 100.00%   130.0s    130.0s   _engine_run_backward (torch/autograd/graph.\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.82s    10.82s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    7.37s     7.39s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    4.44s    138.7s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.81s     4.28s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    3.27s    178.8s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m37600\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m102.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   142.9s    142.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K100.00% 100.00%   131.0s    131.0s   _engine_run_backward (torch/autograd/graph.\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.82s    10.82s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    7.37s     7.39s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    4.44s    138.7s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  2.00%   2.00%    3.83s     4.30s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    3.27s    178.8s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m37700\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m100.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   142.9s    142.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K100.00% 100.00%   132.0s    132.0s   _engine_run_backward (torch/autograd/graph.\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.82s    10.82s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    7.37s     7.39s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    4.44s    138.7s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.83s     4.30s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    3.27s    178.8s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m37800\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m102.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   142.9s    142.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K100.00% 100.00%   133.0s    133.0s   _engine_run_backward (torch/autograd/graph.\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.82s    10.82s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    7.37s     7.39s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    4.44s    138.7s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  2.00%   2.00%    3.85s     4.32s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    3.27s    178.8s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m37900\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m101.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   142.9s    142.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K100.00% 100.00%   134.0s    134.0s   _engine_run_backward (torch/autograd/graph.\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.82s    10.82s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    7.37s     7.39s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    4.44s    138.7s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  1.00%   1.00%    3.86s     4.33s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    3.27s    178.8s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m38000\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m101.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   142.9s    142.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K100.00% 100.00%   135.0s    135.0s   _engine_run_backward (torch/autograd/graph.\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.82s    10.82s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    7.37s     7.39s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    4.44s    138.7s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  1.00%   1.00%    3.87s     4.34s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    3.27s    178.8s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m38100\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m100.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   142.9s    142.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K100.00% 100.00%   136.0s    136.0s   _engine_run_backward (torch/autograd/graph.\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.82s    10.82s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    7.37s     7.39s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    4.44s    138.7s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.87s     4.34s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    3.27s    178.8s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m38200\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m100.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   142.9s    142.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K100.00% 100.00%   137.0s    137.0s   _engine_run_backward (torch/autograd/graph.\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.82s    10.82s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    7.37s     7.39s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    4.44s    138.7s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.87s     4.34s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    3.27s    178.8s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m38300\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m100.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   142.9s    142.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K100.00% 100.00%   138.0s    138.0s   _engine_run_backward (torch/autograd/graph.\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.82s    10.82s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    7.37s     7.39s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    4.44s    138.7s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.87s     4.34s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    3.27s    178.8s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m38400\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m100.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   142.9s    142.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K100.00% 100.00%   139.0s    139.0s   _engine_run_backward (torch/autograd/graph.\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.82s    10.82s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    7.37s     7.39s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    4.44s    138.7s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.87s     4.34s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    3.27s    178.8s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m38500\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m100.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   142.9s    142.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K100.00% 100.00%   140.0s    140.0s   _engine_run_backward (torch/autograd/graph.\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.82s    10.82s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    7.37s     7.39s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    4.44s    138.7s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.87s     4.34s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    3.27s    178.8s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","\u001b[2KCollecting samples from '\u001b[32mpython3 /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1\u001b[0m' (python v3.10.12)\n","\u001b[2KTotal Samples \u001b[1m38600\u001b[0m\n","\u001b[2KGIL: \u001b[1m0.00\u001b[0m%, Active: \u001b[1m100.00\u001b[0m%, Threads: \u001b[1m3\u001b[0m\n","\u001b[2K\n","\u001b[2K\u001b[7m  %Own \u001b[0m\u001b[7m  %Total\u001b[0m\u001b[1m\u001b[7m  OwnTime\u001b[0m\u001b[7m  TotalTime\u001b[0m\u001b[7m  Function (filename)                        \u001b[0m\n","\u001b[2K  0.00%   0.00%   142.9s    142.9s   _conv_forward (torch/nn/modules/conv.py)\n","\u001b[2K100.00% 100.00%   141.0s    141.0s   _engine_run_backward (torch/autograd/graph.\n","\u001b[2K  0.00%   0.00%   11.66s    22.89s   _call_with_frames_removed (<frozen importli\n","\u001b[2K  0.00%   0.00%   10.82s    10.82s   silu (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    8.14s     8.14s   interpolate (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    7.76s     7.79s   kaiming_uniform_ (torch/nn/init.py)\n","\u001b[2K  0.00%   0.00%    7.37s     7.39s   group_norm (torch/nn/functional.py)\n","\u001b[2K  0.00%   0.00%    4.44s    138.7s   _forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    3.87s     4.34s   run_socket (visdom/__init__.py)\n","\u001b[2K  0.00%   0.00%    3.71s     4.12s   load (PIL/ImageFile.py)\n","\u001b[2K  0.00%   0.00%    3.27s    178.8s   forward (guided_diffusion/unet.py)\n","\u001b[2K  0.00%   0.00%    2.51s     2.56s   __deepcopy__ (torch/nn/parameter.py)\n","\u001b[2K  0.00%   0.00%    2.02s     6.15s   convert (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.67s     1.67s   resize (PIL/Image.py)\n","\u001b[2K  0.00%   0.00%    1.24s     1.24s   get_data (<frozen importlib._bootstrap_exte\n","\u001b[2K  0.00%   0.00%    1.14s     1.14s   __init__ (torch/nn/modules/module.py)\n","\u001b[2K\n","\u001b[2KPress \u001b[1m\u001b[7mControl-C\u001b[0m to quit, or \u001b[1m\u001b[7m?\u001b[0m for help.\n","^C\n"]}],"source":["!py-spy top -- python /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1"]},{"cell_type":"markdown","metadata":{"id":"ioPPz6VpOV5I"},"source":["segmentation"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":72832,"status":"ok","timestamp":1729390587937,"user":{"displayName":"ALOK THOTTAKATHU PRASANNAKUMAR","userId":"10483471326433970042"},"user_tz":-660},"id":"XiHBHP5dOXAi","outputId":"bcd65be0-62e0-4755-9ad2-657624356f49"},"outputs":[{"name":"stdout","output_type":"stream","text":["Logging to /content/drive/MyDrive/ML/output/Test/segmentation\n","⠋ Processing... creating model and diffusion...\n","⠼ Processing... /content/drive/MyDrive/ML/guided_diffusion/dist_util.py:68: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  return th.load(io.BytesIO(data), **kwargs)\n","⠴ Processing... Number of samples: 1\n","Number of iterations (for loop runs): 1\n","⠧ Processing... sampling...\n","no dpm-solver\n","⠦ Processing... time for 1 sample 12762.5478515625\n","/content/drive/MyDrive/ML/scripts/segmentation_sample.py:146: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  co = th.tensor(cal_out)\n","no dpm-solver\n","⠋ Processing... time for 1 sample 12354.7548828125\n","no dpm-solver\n","⠧ Processing... time for 1 sample 12587.8720703125\n","no dpm-solver\n","⠧ Processing... time for 1 sample 12814.400390625\n","no dpm-solver\n","⠇ Processing... time for 1 sample 13023.0478515625\n","⠋ Processing... Iteration 5/1 completed. Estimated remaining time: -1 day, 23:59:08\n","✔ Process Completed!\n"]}],"source":["!python /content/drive/MyDrive/ML/scripts/segmentation_sample.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test/segmentation --model_path /content/drive/MyDrive/ML/output/Test/emasavedmodel_0.9999_000002.pt --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 100 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --num_ensemble 5"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5404,"status":"ok","timestamp":1729390646630,"user":{"displayName":"ALOK THOTTAKATHU PRASANNAKUMAR","userId":"10483471326433970042"},"user_tz":-660},"id":"khPxUystzWv0","outputId":"d0e188b1-97e6-4e83-92b1-6f9ce43e7632"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/drive/MyDrive/ML/scripts/segmentation_env.py:84: DeprecationWarning: <class '__main__.DiceCoeff'> should not be instantiated. Methods on autograd functionsare all static, so you should invoke them on the class itself. Instantiating an autograd function will raise an error in a future version of PyTorch.\n","  s = s + DiceCoeff().forward(c[0], c[1])\n","iou is 0.04009847535379756\n","dice is 0.06932689230889082\n"]}],"source":["!python /content/drive/MyDrive/ML/scripts/segmentation_env.py --inp_pth /content/drive/MyDrive/ML/output/Test/segmentation --out_pth /content/drive/MyDrive/ML/dataset/ISIC2018_Task1_Validation_GroundTruth\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":383,"status":"ok","timestamp":1730120451675,"user":{"displayName":"ALOK THOTTAKATHU PRASANNAKUMAR","userId":"10483471326433970042"},"user_tz":-660},"id":"WaC-dKUFIClx","outputId":"0a76c472-3142-4aae-b512-93e01a5c9dd4"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/torch/functional.py:534: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3595.)\n","  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"]}],"source":["# ============================== Imports and Dependencies ==============================\n","\n","import os\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from PIL import Image\n","from zipfile import ZipFile\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import precision_recall_curve, confusion_matrix\n","\n","# ================================ Separable Convolution =================================\n","\n","class SeparableConv2d(nn.Module):\n","    \"\"\"\n","    Implements a separable convolution layer using depthwise and pointwise convolutions.\n","    \"\"\"\n","    def __init__(self, in_channels, out_channels, kernel_size=3, padding=1, bias=True):\n","        super(SeparableConv2d, self).__init__()\n","        # Depthwise convolution (groups=in_channels)\n","        self.depthwise = nn.Conv2d(in_channels, in_channels, kernel_size=kernel_size,\n","                                   padding=padding, groups=in_channels, bias=bias)\n","        # Pointwise convolution\n","        self.pointwise = nn.Conv2d(in_channels, out_channels, kernel_size=1,\n","                                   padding=0, bias=bias)\n","\n","    def forward(self, x):\n","        x = self.depthwise(x)\n","        x = self.pointwise(x)\n","        return x\n","\n","# ================================== ConvLSTM2D ========================================\n","\n","class ConvLSTMCell(nn.Module):\n","    \"\"\"\n","    Implements a ConvLSTM cell.\n","    \"\"\"\n","    def __init__(self, input_channels, hidden_channels, kernel_size, bias=True):\n","        super(ConvLSTMCell, self).__init__()\n","\n","        padding = kernel_size // 2  # To maintain spatial dimensions\n","        self.input_channels = input_channels\n","        self.hidden_channels = hidden_channels\n","\n","        self.conv = nn.Conv2d(in_channels=input_channels + hidden_channels,\n","                              out_channels=4 * hidden_channels,\n","                              kernel_size=kernel_size,\n","                              padding=padding,\n","                              bias=bias)\n","\n","    def forward(self, input_tensor, cur_state):\n","        h_cur, c_cur = cur_state\n","\n","        # Concatenate input and hidden state\n","        combined = torch.cat([input_tensor, h_cur], dim=1)  # along channel axis\n","\n","        # Compute all gates at once\n","        conv_output = self.conv(combined)\n","        cc_i, cc_f, cc_o, cc_g = torch.split(conv_output, self.hidden_channels, dim=1)\n","\n","        i = torch.sigmoid(cc_i)   # input gate\n","        f = torch.sigmoid(cc_f)   # forget gate\n","        o = torch.sigmoid(cc_o)   # output gate\n","        g = torch.tanh(cc_g)      # gate gate\n","\n","        c_next = f * c_cur + i * g\n","        h_next = o * torch.tanh(c_next)\n","\n","        return h_next, c_next\n","\n","    def init_hidden(self, batch_size, spatial_size, device):\n","        height, width = spatial_size\n","        return (torch.zeros(batch_size, self.hidden_channels, height, width, device=device),\n","                torch.zeros(batch_size, self.hidden_channels, height, width, device=device))\n","\n","class ConvLSTM2D(nn.Module):\n","    \"\"\"\n","    Implements a ConvLSTM2D layer that processes a sequence of inputs.\n","    \"\"\"\n","    def __init__(self, input_channels, hidden_channels, kernel_size=3, bias=True, num_layers=1):\n","        super(ConvLSTM2D, self).__init__()\n","        self.num_layers = num_layers\n","        self.hidden_channels = hidden_channels\n","\n","        layers = []\n","        for i in range(num_layers):\n","            input_c = input_channels if i == 0 else hidden_channels\n","            layers.append(ConvLSTMCell(input_c, hidden_channels, kernel_size, bias))\n","        self.layers = nn.ModuleList(layers)\n","\n","    def forward(self, input_tensor):\n","        # input_tensor shape: (batch, seq_len, channels, height, width)\n","        batch_size, seq_len, channels, height, width = input_tensor.size()\n","        device = input_tensor.device\n","\n","        # Initialize hidden and cell states for all layers\n","        hidden_state = []\n","        cell_state = []\n","        for i in range(self.num_layers):\n","            h, c = self.layers[i].init_hidden(batch_size, (height, width), device)\n","            hidden_state.append(h)\n","            cell_state.append(c)\n","\n","        # Iterate over time steps\n","        for t in range(seq_len):\n","            x = input_tensor[:, t, :, :, :]  # (batch, channels, height, width)\n","            for i, layer in enumerate(self.layers):\n","                h, c = layer(x, (hidden_state[i], cell_state[i]))\n","                hidden_state[i] = h\n","                cell_state[i] = c\n","                x = h  # input to next layer\n","        return x  # Return the hidden state of the last layer\n","\n","# ============================== Swin Transformer Blocks ================================\n","\n","class WindowAttention(nn.Module):\n","    \"\"\"\n","    Window based multi-head self attention (W-MSA) module with relative position bias.\n","    \"\"\"\n","    def __init__(self, dim, window_size, num_heads, qkv_bias=True, attn_drop=0., proj_drop=0.):\n","        \"\"\"\n","        Args:\n","            dim (int): Number of input channels.\n","            window_size (tuple): Height and width of the window.\n","            num_heads (int): Number of attention heads.\n","            qkv_bias (bool): If True, add a learnable bias to query, key, value.\n","            attn_drop (float): Dropout ratio of attention weights.\n","            proj_drop (float): Dropout ratio of output.\n","        \"\"\"\n","        super(WindowAttention, self).__init__()\n","        self.dim = dim\n","        self.window_size = window_size  # Wh, Ww\n","        self.num_heads = num_heads\n","        head_dim = dim // num_heads\n","        self.scale = head_dim ** -0.5\n","\n","        # Define a parameter table of relative position bias\n","        self.relative_position_bias_table = nn.Parameter(\n","            torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads)\n","        )  # 2*Wh-1 * 2*Ww-1, nH\n","\n","        # Get pair-wise relative position index for each token inside the window\n","        coords_h = torch.arange(self.window_size[0])\n","        coords_w = torch.arange(self.window_size[1])\n","        coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n","        coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n","        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n","        relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n","        relative_coords[:, :, 0] += self.window_size[0] - 1  # shift to start from 0\n","        relative_coords[:, :, 1] += self.window_size[1] - 1\n","        relative_coords[:, :, 0] *= (2 * self.window_size[1] - 1)\n","        relative_position_index = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n","        self.register_buffer(\"relative_position_index\", relative_position_index)\n","\n","        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)  # Query, Key, Value\n","        self.attn_drop = nn.Dropout(attn_drop)\n","        self.proj = nn.Linear(dim, dim)\n","        self.proj_drop = nn.Dropout(proj_drop)\n","\n","        # Initialize relative position bias table\n","        nn.init.trunc_normal_(self.relative_position_bias_table, std=.02)\n","\n","    def forward(self, x, mask=None):\n","        \"\"\"\n","        Args:\n","            x: input features with shape of (num_windows*B, Wh*Ww, C)\n","            mask: (num_windows, Wh*Ww, Wh*Ww) or None\n","        \"\"\"\n","        B_, N, C = x.shape\n","        qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)  # 3, B_, nH, N, C//nH\n","        q, k, v = qkv[0], qkv[1], qkv[2]  # each has shape (B_, nH, N, C//nH)\n","\n","        q = q * self.scale\n","        attn = (q @ k.transpose(-2, -1))  # (B_, nH, N, N)\n","\n","        # Add relative position bias\n","        relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(\n","            self.window_size[0] * self.window_size[1], self.window_size[0] * self.window_size[1], -1\n","        )  # Wh*Ww, Wh*Ww, nH\n","        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n","        attn = attn + relative_position_bias.unsqueeze(0)  # (B_, nH, N, N)\n","\n","        if mask is not None:\n","            nW = mask.shape[0]\n","            attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)\n","            attn = attn.view(-1, self.num_heads, N, N)\n","            attn = F.softmax(attn, dim=-1)\n","        else:\n","            attn = F.softmax(attn, dim=-1)\n","\n","        attn = self.attn_drop(attn)\n","\n","        out = (attn @ v).transpose(1, 2).reshape(B_, N, C)  # (B_, N, C)\n","        out = self.proj(out)\n","        out = self.proj_drop(out)\n","        return out\n","\n","class SwinTransformerBlock(nn.Module):\n","    \"\"\"\n","    Swin Transformer Block with W-MSA and SW-MSA.\n","    \"\"\"\n","    def __init__(self, dim, num_heads, window_size=7, shift_size=0, mlp_ratio=4., qkv_bias=True,\n","                 attn_drop=0., proj_drop=0.):\n","        super(SwinTransformerBlock, self).__init__()\n","        self.dim = dim\n","        self.num_heads = num_heads\n","        self.window_size = window_size  # W\n","        self.shift_size = shift_size    # S\n","        self.mlp_ratio = mlp_ratio\n","\n","        assert 0 <= self.shift_size < self.window_size, \"shift_size must be in [0, window_size)\"\n","\n","        self.norm1 = nn.LayerNorm(dim)\n","        self.attn = WindowAttention(dim, (window_size, window_size), num_heads, qkv_bias, attn_drop, proj_drop)\n","\n","        self.drop_path = nn.Identity()  # Can implement stochastic depth if desired\n","        self.norm2 = nn.LayerNorm(dim)\n","        mlp_hidden_dim = int(dim * mlp_ratio)\n","        self.mlp = nn.Sequential(\n","            nn.Linear(dim, mlp_hidden_dim),\n","            nn.GELU(),\n","            nn.Linear(mlp_hidden_dim, dim),\n","            nn.Dropout(proj_drop)\n","        )\n","\n","        if self.shift_size > 0:\n","            # Shift the window by shift_size\n","            self.shift_partition = True\n","        else:\n","            self.shift_partition = False\n","\n","    def forward(self, x):\n","        \"\"\"\n","        Args:\n","            x: input features with shape (B, H*W, C)\n","        \"\"\"\n","        H = W = int(np.sqrt(x.shape[1]))\n","        B, L, C = x.shape\n","        assert L == H * W, \"Input feature has wrong size\"\n","\n","        shortcut = x\n","        x = self.norm1(x)\n","        x = x.view(B, H, W, C)\n","\n","        # Cyclic shift\n","        if self.shift_size > 0:\n","            shifted_x = torch.roll(x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2))\n","        else:\n","            shifted_x = x\n","\n","        # Partition windows\n","        window_size = self.window_size\n","        # Pad H and W to be multiples of window_size\n","        pad_b = (window_size - H % window_size) % window_size\n","        pad_r = (window_size - W % window_size) % window_size\n","        shifted_x = F.pad(shifted_x, (0, 0, 0, pad_r, 0, pad_b))  # pad H and W\n","        _, Hp, Wp, _ = shifted_x.shape\n","\n","        # Window partition\n","        x_windows = shifted_x.view(B, Hp // window_size, window_size, Wp // window_size, window_size, C)\n","        x_windows = x_windows.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size * window_size, C)  # (num_windows*B, window_size*window_size, C)\n","\n","        # Attention\n","        attn_windows = self.attn(x_windows)  # (num_windows*B, window_size*window_size, C)\n","\n","        # Merge windows\n","        shifted_x = attn_windows.view(-1, window_size, window_size, C)\n","        shifted_x = shifted_x.view(B, Hp // window_size, Wp // window_size, window_size, window_size, C)\n","        shifted_x = shifted_x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, Hp, Wp, C)\n","\n","        # Reverse cyclic shift\n","        if self.shift_size > 0:\n","            x = torch.roll(shifted_x, shifts=(self.shift_size, self.shift_size), dims=(1, 2))\n","        else:\n","            x = shifted_x\n","\n","        # Remove padding\n","        x = x[:, :H, :W, :].contiguous().view(B, H * W, C)\n","\n","        # FFN\n","        x = shortcut + self.drop_path(attn_windows.view(B, H * W, C))\n","        x = x + self.drop_path(self.mlp(self.norm2(x)))\n","\n","        return x\n","\n","# =============================== Dice Loss Function ====================================\n","\n","class DiceLoss(nn.Module):\n","    \"\"\"\n","    Dice Loss function to maximize the Dice coefficient.\n","    Suitable for binary segmentation tasks.\n","    \"\"\"\n","    def __init__(self, smooth=1.0):\n","        super(DiceLoss, self).__init__()\n","        self.smooth = smooth\n","\n","    def forward(self, y_pred, y_true):\n","        \"\"\"\n","        Args:\n","            y_pred (torch.Tensor): Predicted mask probabilities with shape (B, 1, H, W)\n","            y_true (torch.Tensor): Ground truth masks with shape (B, 1, H, W)\n","        Returns:\n","            torch.Tensor: Dice loss\n","        \"\"\"\n","        y_pred = y_pred.view(-1)\n","        y_true = y_true.view(-1)\n","\n","        intersection = (y_pred * y_true).sum()\n","        dice = (2. * intersection + self.smooth) / (y_pred.sum() + y_true.sum() + self.smooth)\n","\n","        return 1 - dice\n","\n","# ================================ Main Model ============================================\n","\n","import torch\n","import torch.nn as nn\n","\n","import torch\n","import torch.nn as nn\n","\n","class SwinUNet(nn.Module):\n","    \"\"\"\n","    Swin U-Net architecture for image segmentation.\n","    \"\"\"\n","    def __init__(self, input_channels=3, output_channels=1,\n","                 embed_dim=32, num_heads=[4, 8], window_size=4,\n","                 mlp_ratio=4., depth=2):\n","        super(SwinUNet, self).__init__()\n","        self.input_channels = input_channels\n","        self.output_channels = output_channels\n","\n","        # Initial convolutional layers\n","        self.conv1 = SeparableConv2d(input_channels, 24, kernel_size=3, padding=1)\n","        self.bn1 = nn.BatchNorm2d(24)\n","        self.conv2 = SeparableConv2d(24, 24, kernel_size=3, padding=1)\n","        self.bn2 = nn.BatchNorm2d(24)\n","        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)  # 256x256 -> 128x128\n","\n","        # First Swin Transformer Block\n","        self.swin_unet_E1 = SwinTransformerBlock(\n","            dim=24,  # Changed from embed_dim=32 to 24\n","            num_heads=num_heads[0],\n","            window_size=window_size,\n","            shift_size=window_size//2 if True else 0,\n","            mlp_ratio=mlp_ratio\n","        )\n","\n","        # Second convolutional block\n","        self.conv3 = SeparableConv2d(24, 48, kernel_size=3, padding=1)  # Changed input from embed_dim=32 to 24\n","        self.bn3 = nn.BatchNorm2d(48)\n","        self.conv4 = SeparableConv2d(48, 48, kernel_size=3, padding=1)\n","        self.bn4 = nn.BatchNorm2d(48)\n","        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)  # 128x128 -> 64x64\n","\n","        # Second Swin Transformer Block\n","        self.swin_unet_E2 = SwinTransformerBlock(\n","            dim=48,  # Changed from embed_dim=32 to 48\n","            num_heads=num_heads[1],\n","            window_size=window_size,\n","            shift_size=window_size//2 if True else 0,\n","            mlp_ratio=mlp_ratio\n","        )\n","\n","        # Third convolutional block (Bottleneck)\n","        self.conv5 = SeparableConv2d(48, 96, kernel_size=3, padding=1)  # Changed input from embed_dim=32 to 48\n","        self.bn5 = nn.BatchNorm2d(96)\n","        self.conv6 = SeparableConv2d(96, 96, kernel_size=3, padding=1)\n","        self.bn6 = nn.BatchNorm2d(96)\n","        self.drop5 = nn.Dropout(0.5)\n","        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)  # 64x64 -> 32x32\n","\n","        # Bottleneck convolutions with dense connections\n","        self.conv7 = SeparableConv2d(96, 192, kernel_size=3, padding=1)\n","        self.bn7 = nn.BatchNorm2d(192)\n","        self.conv8 = SeparableConv2d(192, 192, kernel_size=3, padding=1)\n","        self.bn8 = nn.BatchNorm2d(192)\n","        self.drop6_1 = nn.Dropout(0.5)\n","\n","        self.conv9 = SeparableConv2d(192, 192, kernel_size=3, padding=1)\n","        self.bn9 = nn.BatchNorm2d(192)\n","        self.conv10 = SeparableConv2d(192, 192, kernel_size=3, padding=1)\n","        self.bn10 = nn.BatchNorm2d(192)\n","        self.drop6_2 = nn.Dropout(0.5)\n","\n","        self.concat1 = nn.Sequential(\n","            SeparableConv2d(384, 192, kernel_size=3, padding=1),\n","            SeparableConv2d(192, 192, kernel_size=3, padding=1)\n","        )\n","        self.drop6_3 = nn.Dropout(0.5)\n","\n","        # First Upsampling Block\n","        self.up1 = nn.ConvTranspose2d(192, 96, kernel_size=2, stride=2)  # 32x32 -> 64x64\n","        self.bn_up1 = nn.BatchNorm2d(96)\n","        self.relu_up1 = nn.ReLU(inplace=True)\n","        self.convLSTM1 = ConvLSTM2D(input_channels=96, hidden_channels=384, kernel_size=3, num_layers=1)\n","        self.swin_unet_D1 = SwinTransformerBlock(\n","            dim=384,  # Changed from embed_dim=32 to 384\n","            num_heads=num_heads[0],\n","            window_size=window_size,\n","            shift_size=window_size//2 if True else 0,\n","            mlp_ratio=mlp_ratio\n","        )\n","        self.conv11 = SeparableConv2d(384, 48, kernel_size=3, padding=1)\n","        self.conv12 = SeparableConv2d(48, 48, kernel_size=3, padding=1)\n","\n","        # Second Upsampling Block\n","        self.up2 = nn.ConvTranspose2d(48, 48, kernel_size=2, stride=2)  # 64x64 -> 128x128\n","        self.bn_up2 = nn.BatchNorm2d(48)\n","        self.relu_up2 = nn.ReLU(inplace=True)\n","        self.convLSTM2 = ConvLSTM2D(input_channels=48, hidden_channels=96, kernel_size=3, num_layers=1)\n","        self.swin_unet_D2 = SwinTransformerBlock(\n","            dim=96,  # Changed from embed_dim=32 to 96\n","            num_heads=num_heads[1],\n","            window_size=window_size,\n","            shift_size=window_size//2 if True else 0,\n","            mlp_ratio=mlp_ratio\n","        )\n","        self.conv13 = SeparableConv2d(96, 24, kernel_size=3, padding=1)\n","        self.conv14 = SeparableConv2d(24, 24, kernel_size=3, padding=1)\n","\n","        # Third Upsampling Block\n","        self.up3 = nn.ConvTranspose2d(24, 24, kernel_size=2, stride=2)  # 128x128 -> 256x256\n","        self.bn_up3 = nn.BatchNorm2d(24)\n","        self.relu_up3 = nn.ReLU(inplace=True)\n","        self.convLSTM3 = ConvLSTM2D(input_channels=24, hidden_channels=48, kernel_size=3, num_layers=1)\n","        self.swin_unet_D3 = SwinTransformerBlock(\n","            dim=48,  # Changed from embed_dim=32 to 48\n","            num_heads=num_heads[1],\n","            window_size=window_size,\n","            shift_size=window_size//2 if True else 0,\n","            mlp_ratio=mlp_ratio\n","        )\n","        self.conv15 = SeparableConv2d(48, 24, kernel_size=3, padding=1)\n","        self.conv16 = SeparableConv2d(24, 24, kernel_size=3, padding=1)\n","\n","        # Output Layer\n","        self.final_conv1 = nn.Conv2d(24, 2, kernel_size=3, padding=1)\n","        self.final_relu = nn.ReLU(inplace=True)\n","        self.final_conv2 = nn.Conv2d(2, 1, kernel_size=1, padding=0)\n","        self.final_sigmoid = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        \"\"\"\n","        Forward pass of the Swin U-Net model.\n","        Args:\n","            x: Input tensor with shape (B, 3, 256, 256)\n","        Returns:\n","            torch.Tensor: Output segmentation mask with shape (B, 1, 256, 256)\n","        \"\"\"\n","        # Initial Convolutions\n","        x1 = self.conv1(x)          # (B, 24, 256, 256)\n","        x1 = self.bn1(x1)\n","        x1 = self.conv2(x1)         # (B, 24, 256, 256)\n","        x1 = self.bn2(x1)\n","        p1 = self.pool1(x1)         # (B, 24, 128, 128)\n","\n","        # First Swin Transformer Block\n","        p1_flat = p1.flatten(2).transpose(1, 2)  # (B, 128*128, 24)\n","        swin_E1 = self.swin_unet_E1(p1_flat)     # (B, 128*128, 24)\n","        swin_E1 = swin_E1.transpose(1, 2).view(-1, 24, 128, 128)  # Reshape for Conv2d\n","\n","        # Second Convolutional Block\n","        x2 = self.conv3(swin_E1)    # (B, 48, 128, 128)\n","        x2 = self.bn3(x2)\n","        x2 = self.conv4(x2)          # (B, 48, 128, 128)\n","        x2 = self.bn4(x2)\n","        p2 = self.pool2(x2)          # (B, 48, 64, 64)\n","\n","        # Second Swin Transformer Block\n","        p2_flat = p2.flatten(2).transpose(1, 2)  # (B, 64*64, 48)\n","        swin_E2 = self.swin_unet_E2(p2_flat)     # (B, 64*64, 48)\n","        swin_E2 = swin_E2.transpose(1, 2).view(-1, 48, 64, 64)  # Reshape for Conv2d\n","\n","        # Third Convolutional Block (Bottleneck)\n","        x3 = self.conv5(swin_E2)    # (B, 96, 64, 64)\n","        x3 = self.bn5(x3)\n","        x3 = self.conv6(x3)          # (B, 96, 64, 64)\n","        x3 = self.bn6(x3)\n","        x3 = self.drop5(x3)\n","        p3 = self.pool3(x3)          # (B, 96, 32, 32)\n","\n","        # Bottleneck Convolutions with Dense Connections\n","        x4 = self.conv7(p3)          # (B, 192, 32, 32)\n","        x4 = self.bn7(x4)\n","        x4 = self.conv8(x4)          # (B, 192, 32, 32)\n","        x4 = self.bn8(x4)\n","        x4 = self.drop6_1(x4)\n","\n","        x5 = self.conv9(x4)          # (B, 192, 32, 32)\n","        x5 = self.bn9(x5)\n","        x5 = self.conv10(x5)         # (B, 192, 32, 32)\n","        x5 = self.bn10(x5)\n","        x5 = self.drop6_2(x5)\n","\n","        concat = torch.cat([x5, x4], dim=1)  # (B, 384, 32, 32)\n","        concat = self.concat1(concat)         # (B, 192, 32, 32)\n","        concat = self.drop6_3(concat)         # (B, 192, 32, 32)\n","\n","        # First Upsampling Block\n","        up1 = self.up1(concat)                 # (B, 96, 64, 64)\n","        up1 = self.bn_up1(up1)\n","        up1 = self.relu_up1(up1)\n","\n","        # Prepare for ConvLSTM2D\n","        # ConvLSTM2D expects input of shape (B, seq_len, C, H, W)\n","        up1_seq = up1.unsqueeze(1)             # (B, 1, 96, 64, 64)\n","        x3_seq = x3.unsqueeze(1)               # (B, 1, 96, 64, 64)\n","        merge1 = torch.cat([x3_seq, up1_seq], dim=1)  # (B, 2, 96, 64, 64)\n","\n","        # Apply ConvLSTM2D\n","        convLSTM1_out = self.convLSTM1(merge1)       # (B, 384, 64, 64)\n","\n","        # Swin Transformer Block in Decoder\n","        convLSTM1_flat = convLSTM1_out.flatten(2).transpose(1, 2)  # (B, 64*64, 384)\n","        swin_D1 = self.swin_unet_D1(convLSTM1_flat)               # (B, 64*64, 384)\n","        swin_D1 = swin_D1.transpose(1, 2).view(-1, 384, 64, 64)    # Reshape for Conv2d\n","\n","        # Further Convolutions\n","        conv6 = self.conv11(swin_D1)        # (B, 48, 64, 64)\n","        conv6 = self.conv12(conv6)          # (B, 48, 64, 64)\n","\n","        # Second Upsampling Block\n","        up2 = self.up2(conv6)               # (B, 48, 128, 128)\n","        up2 = self.bn_up2(up2)\n","        up2 = self.relu_up2(up2)\n","\n","        # Prepare for ConvLSTM2D\n","        up2_seq = up2.unsqueeze(1)           # (B, 1, 48, 128, 128)\n","        x2_seq = x2.unsqueeze(1)             # (B, 1, 48, 128, 128)\n","        merge2 = torch.cat([x2_seq, up2_seq], dim=1)  # (B, 2, 48, 128, 128)\n","\n","        # Apply ConvLSTM2D\n","        convLSTM2_out = self.convLSTM2(merge2)       # (B, 96, 128, 128)\n","\n","        # Swin Transformer Block in Decoder\n","        convLSTM2_flat = convLSTM2_out.flatten(2).transpose(1, 2)  # (B, 128*128, 96)\n","        swin_D2 = self.swin_unet_D2(convLSTM2_flat)               # (B, 128*128, 96)\n","        swin_D2 = swin_D2.transpose(1, 2).view(-1, 96, 128, 128)    # Reshape for Conv2d\n","\n","        # Further Convolutions\n","        conv7 = self.conv13(swin_D2)        # (B, 24, 128, 128)\n","        conv7 = self.conv14(conv7)          # (B, 24, 128, 128)\n","\n","        # Third Upsampling Block\n","        up3 = self.up3(conv7)               # (B, 24, 256, 256)\n","        up3 = self.bn_up3(up3)\n","        up3 = self.relu_up3(up3)\n","\n","        # Prepare for ConvLSTM2D\n","        up3_seq = up3.unsqueeze(1)           # (B, 1, 24, 256, 256)\n","        x1_seq = x1.unsqueeze(1)             # (B, 1, 24, 256, 256)\n","        merge3 = torch.cat([x1_seq, up3_seq], dim=1)  # (B, 2, 24, 256, 256)\n","\n","        # Apply ConvLSTM2D\n","        convLSTM3_out = self.convLSTM3(merge3)       # (B, 48, 256, 256)\n","\n","        # Swin Transformer Block in Decoder\n","        convLSTM3_flat = convLSTM3_out.flatten(2).transpose(1, 2)  # (B, 256*256, 48)\n","        swin_D3 = self.swin_unet_D3(convLSTM3_flat)               # (B, 256*256, 48)\n","        swin_D3 = swin_D3.transpose(1, 2).view(-1, 48, 256, 256)    # Reshape for Conv2d\n","\n","        # Further Convolutions\n","        conv8 = self.conv15(swin_D3)        # (B, 24, 256, 256)\n","        conv8 = self.conv16(conv8)          # (B, 24, 256, 256)\n","\n","        # Final Output Convolutions\n","        final = self.final_conv1(conv8)      # (B, 2, 256, 256)\n","        final = self.final_relu(final)\n","        final = self.final_conv2(final)      # (B, 1, 256, 256)\n","        final = self.final_sigmoid(final)    # (B, 1, 256, 256)\n","\n","        return final\n","\n","# ================================== Dataset Class ======================================\n","\n","class SegmentationDataset(Dataset):\n","    \"\"\"\n","    Custom Dataset for image segmentation tasks.\n","    Expects images in 'x' folder and masks in 'y' folder.\n","    \"\"\"\n","    def __init__(self, images_dir, masks_dir, transform=None):\n","        super(SegmentationDataset, self).__init__()\n","        self.images_dir = images_dir\n","        self.masks_dir = masks_dir\n","        self.transform = transform\n","\n","        self.images = sorted(os.listdir(images_dir))\n","        self.masks = sorted(os.listdir(masks_dir))\n","\n","    def __len__(self):\n","        return len(self.images)\n","\n","    def __getitem__(self, idx):\n","        # Load image\n","        img_path = os.path.join(self.images_dir, self.images[idx])\n","        image = Image.open(img_path).convert('RGB')  # Ensure RGB\n","\n","        # Load mask\n","        mask_path = os.path.join(self.masks_dir, self.masks[idx])\n","        mask = Image.open(mask_path).convert('L')    # Grayscale\n","\n","        # Apply transformations\n","        if self.transform:\n","            image = self.transform(image)\n","            mask = self.transform(mask)\n","\n","        return image, mask\n","\n","# =============================== Data Loading and Preprocessing ========================\n","\n","# Define image dimensions\n","im_height = 256\n","im_width = 256\n","\n","# Define transformations\n","transform = transforms.Compose([\n","    transforms.Resize((im_height, im_width)),\n","    transforms.ToTensor(),  # Converts to [0,1]\n","])\n","\n","# Paths to the dataset (update these paths as per your directory structure)\n","train_images_dir = r'/content/drive/MyDrive/ML/dataset/ISIC2018_Task1-2_Training_Input'\n","train_masks_dir = r'/content/drive/MyDrive/ML/dataset/ISIC2018_Task1_Training_GroundTruth'\n","test_images_dir = r'/content/drive/MyDrive/ML/dataset/ISIC2018_Task1-2_Test_Input'\n","test_masks_dir = r'/content/drive/MyDrive/ML/dataset/ISIC2018_Task1_Test_GroundTruth'\n","\n","# Create datasets\n","train_dataset = SegmentationDataset(train_images_dir, train_masks_dir, transform=transform)\n","test_dataset = SegmentationDataset(test_images_dir, test_masks_dir, transform=transform)\n","\n","\n","# Split training data into training and validation sets (80-20 split)\n","train_size = int(0.8 * len(train_dataset))\n","valid_size = len(train_dataset) - train_size\n","train_subset, valid_subset = torch.utils.data.random_split(train_dataset, [train_size, valid_size])\n","\n","# Create DataLoaders\n","batch_size = 5\n","\n","train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True, num_workers=4)\n","valid_loader = DataLoader(valid_subset, batch_size=batch_size, shuffle=False, num_workers=4)\n","test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n","\n","# =============================== Training Setup ==========================================\n","\n","# Instantiate the model\n","model = SwinUNet(input_channels=3, output_channels=1, embed_dim=32, num_heads=[4, 8], window_size=4, mlp_ratio=4., depth=2)\n","model = model.to('cuda' if torch.cuda.is_available() else 'cpu')  # Move to GPU if available\n","\n","# Define loss function and optimizer\n","criterion = DiceLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n","\n","# Define learning rate scheduler and early stopping parameters\n","scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.25, patience=5, verbose=True, min_lr=1e-9)\n","early_stopping_patience = 9\n","best_val_loss = np.inf\n","epochs_no_improve = 0\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9082,"status":"ok","timestamp":1730049875220,"user":{"displayName":"ALOK THOTTAKATHU PRASANNAKUMAR","userId":"10483471326433970042"},"user_tz":-660},"id":"9e5t3DhAAAUG","outputId":"3f7e24ca-4f56-442a-edde-53e10793a240"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting torchinfo\n","  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n","Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n","Installing collected packages: torchinfo\n","Successfully installed torchinfo-1.8.0\n"]}],"source":["!pip install torchinfo"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19259,"status":"ok","timestamp":1730049896819,"user":{"displayName":"ALOK THOTTAKATHU PRASANNAKUMAR","userId":"10483471326433970042"},"user_tz":-660},"id":"7rRAMJmIAgXR","outputId":"afd05387-fa79-449f-c2b5-3863b0e6ae59"},"outputs":[{"data":{"text/plain":["==========================================================================================\n","Layer (type:depth-idx)                   Output Shape              Param #\n","==========================================================================================\n","SwinUNet                                 [1, 1, 256, 256]          --\n","├─SeparableConv2d: 1-1                   [1, 24, 256, 256]         --\n","│    └─Conv2d: 2-1                       [1, 3, 256, 256]          30\n","│    └─Conv2d: 2-2                       [1, 24, 256, 256]         96\n","├─BatchNorm2d: 1-2                       [1, 24, 256, 256]         48\n","├─SeparableConv2d: 1-3                   [1, 24, 256, 256]         --\n","│    └─Conv2d: 2-3                       [1, 24, 256, 256]         240\n","│    └─Conv2d: 2-4                       [1, 24, 256, 256]         600\n","├─BatchNorm2d: 1-4                       [1, 24, 256, 256]         48\n","├─MaxPool2d: 1-5                         [1, 24, 128, 128]         --\n","├─SwinTransformerBlock: 1-6              [1, 16384, 24]            --\n","│    └─LayerNorm: 2-5                    [1, 16384, 24]            48\n","│    └─WindowAttention: 2-6              [1024, 16, 24]            196\n","│    │    └─Linear: 3-1                  [1024, 16, 72]            1,800\n","│    │    └─Dropout: 3-2                 [1024, 4, 16, 16]         --\n","│    │    └─Linear: 3-3                  [1024, 16, 24]            600\n","│    │    └─Dropout: 3-4                 [1024, 16, 24]            --\n","│    └─Identity: 2-7                     [1, 16384, 24]            --\n","│    └─LayerNorm: 2-8                    [1, 16384, 24]            48\n","│    └─Sequential: 2-9                   [1, 16384, 24]            --\n","│    │    └─Linear: 3-5                  [1, 16384, 96]            2,400\n","│    │    └─GELU: 3-6                    [1, 16384, 96]            --\n","│    │    └─Linear: 3-7                  [1, 16384, 24]            2,328\n","│    │    └─Dropout: 3-8                 [1, 16384, 24]            --\n","│    └─Identity: 2-10                    [1, 16384, 24]            --\n","├─SeparableConv2d: 1-7                   [1, 48, 128, 128]         --\n","│    └─Conv2d: 2-11                      [1, 24, 128, 128]         240\n","│    └─Conv2d: 2-12                      [1, 48, 128, 128]         1,200\n","├─BatchNorm2d: 1-8                       [1, 48, 128, 128]         96\n","├─SeparableConv2d: 1-9                   [1, 48, 128, 128]         --\n","│    └─Conv2d: 2-13                      [1, 48, 128, 128]         480\n","│    └─Conv2d: 2-14                      [1, 48, 128, 128]         2,352\n","├─BatchNorm2d: 1-10                      [1, 48, 128, 128]         96\n","├─MaxPool2d: 1-11                        [1, 48, 64, 64]           --\n","├─SwinTransformerBlock: 1-12             [1, 4096, 48]             --\n","│    └─LayerNorm: 2-15                   [1, 4096, 48]             96\n","│    └─WindowAttention: 2-16             [256, 16, 48]             392\n","│    │    └─Linear: 3-9                  [256, 16, 144]            7,056\n","│    │    └─Dropout: 3-10                [256, 8, 16, 16]          --\n","│    │    └─Linear: 3-11                 [256, 16, 48]             2,352\n","│    │    └─Dropout: 3-12                [256, 16, 48]             --\n","│    └─Identity: 2-17                    [1, 4096, 48]             --\n","│    └─LayerNorm: 2-18                   [1, 4096, 48]             96\n","│    └─Sequential: 2-19                  [1, 4096, 48]             --\n","│    │    └─Linear: 3-13                 [1, 4096, 192]            9,408\n","│    │    └─GELU: 3-14                   [1, 4096, 192]            --\n","│    │    └─Linear: 3-15                 [1, 4096, 48]             9,264\n","│    │    └─Dropout: 3-16                [1, 4096, 48]             --\n","│    └─Identity: 2-20                    [1, 4096, 48]             --\n","├─SeparableConv2d: 1-13                  [1, 96, 64, 64]           --\n","│    └─Conv2d: 2-21                      [1, 48, 64, 64]           480\n","│    └─Conv2d: 2-22                      [1, 96, 64, 64]           4,704\n","├─BatchNorm2d: 1-14                      [1, 96, 64, 64]           192\n","├─SeparableConv2d: 1-15                  [1, 96, 64, 64]           --\n","│    └─Conv2d: 2-23                      [1, 96, 64, 64]           960\n","│    └─Conv2d: 2-24                      [1, 96, 64, 64]           9,312\n","├─BatchNorm2d: 1-16                      [1, 96, 64, 64]           192\n","├─Dropout: 1-17                          [1, 96, 64, 64]           --\n","├─MaxPool2d: 1-18                        [1, 96, 32, 32]           --\n","├─SeparableConv2d: 1-19                  [1, 192, 32, 32]          --\n","│    └─Conv2d: 2-25                      [1, 96, 32, 32]           960\n","│    └─Conv2d: 2-26                      [1, 192, 32, 32]          18,624\n","├─BatchNorm2d: 1-20                      [1, 192, 32, 32]          384\n","├─SeparableConv2d: 1-21                  [1, 192, 32, 32]          --\n","│    └─Conv2d: 2-27                      [1, 192, 32, 32]          1,920\n","│    └─Conv2d: 2-28                      [1, 192, 32, 32]          37,056\n","├─BatchNorm2d: 1-22                      [1, 192, 32, 32]          384\n","├─Dropout: 1-23                          [1, 192, 32, 32]          --\n","├─SeparableConv2d: 1-24                  [1, 192, 32, 32]          --\n","│    └─Conv2d: 2-29                      [1, 192, 32, 32]          1,920\n","│    └─Conv2d: 2-30                      [1, 192, 32, 32]          37,056\n","├─BatchNorm2d: 1-25                      [1, 192, 32, 32]          384\n","├─SeparableConv2d: 1-26                  [1, 192, 32, 32]          --\n","│    └─Conv2d: 2-31                      [1, 192, 32, 32]          1,920\n","│    └─Conv2d: 2-32                      [1, 192, 32, 32]          37,056\n","├─BatchNorm2d: 1-27                      [1, 192, 32, 32]          384\n","├─Dropout: 1-28                          [1, 192, 32, 32]          --\n","├─Sequential: 1-29                       [1, 192, 32, 32]          --\n","│    └─SeparableConv2d: 2-33             [1, 192, 32, 32]          --\n","│    │    └─Conv2d: 3-17                 [1, 384, 32, 32]          3,840\n","│    │    └─Conv2d: 3-18                 [1, 192, 32, 32]          73,920\n","│    └─SeparableConv2d: 2-34             [1, 192, 32, 32]          --\n","│    │    └─Conv2d: 3-19                 [1, 192, 32, 32]          1,920\n","│    │    └─Conv2d: 3-20                 [1, 192, 32, 32]          37,056\n","├─Dropout: 1-30                          [1, 192, 32, 32]          --\n","├─ConvTranspose2d: 1-31                  [1, 96, 64, 64]           73,824\n","├─BatchNorm2d: 1-32                      [1, 96, 64, 64]           192\n","├─ReLU: 1-33                             [1, 96, 64, 64]           --\n","├─ConvLSTM2D: 1-34                       [1, 384, 64, 64]          --\n","│    └─ModuleList: 2-35                  --                        --\n","│    │    └─ConvLSTMCell: 3-21           [1, 384, 64, 64]          6,637,056\n","│    │    └─ConvLSTMCell: 3-22           [1, 384, 64, 64]          (recursive)\n","├─SwinTransformerBlock: 1-35             [1, 4096, 384]            --\n","│    └─LayerNorm: 2-36                   [1, 4096, 384]            768\n","│    └─WindowAttention: 2-37             [256, 16, 384]            196\n","│    │    └─Linear: 3-23                 [256, 16, 1152]           443,520\n","│    │    └─Dropout: 3-24                [256, 4, 16, 16]          --\n","│    │    └─Linear: 3-25                 [256, 16, 384]            147,840\n","│    │    └─Dropout: 3-26                [256, 16, 384]            --\n","│    └─Identity: 2-38                    [1, 4096, 384]            --\n","│    └─LayerNorm: 2-39                   [1, 4096, 384]            768\n","│    └─Sequential: 2-40                  [1, 4096, 384]            --\n","│    │    └─Linear: 3-27                 [1, 4096, 1536]           591,360\n","│    │    └─GELU: 3-28                   [1, 4096, 1536]           --\n","│    │    └─Linear: 3-29                 [1, 4096, 384]            590,208\n","│    │    └─Dropout: 3-30                [1, 4096, 384]            --\n","│    └─Identity: 2-41                    [1, 4096, 384]            --\n","├─SeparableConv2d: 1-36                  [1, 48, 64, 64]           --\n","│    └─Conv2d: 2-42                      [1, 384, 64, 64]          3,840\n","│    └─Conv2d: 2-43                      [1, 48, 64, 64]           18,480\n","├─SeparableConv2d: 1-37                  [1, 48, 64, 64]           --\n","│    └─Conv2d: 2-44                      [1, 48, 64, 64]           480\n","│    └─Conv2d: 2-45                      [1, 48, 64, 64]           2,352\n","├─ConvTranspose2d: 1-38                  [1, 48, 128, 128]         9,264\n","├─BatchNorm2d: 1-39                      [1, 48, 128, 128]         96\n","├─ReLU: 1-40                             [1, 48, 128, 128]         --\n","├─ConvLSTM2D: 1-41                       [1, 96, 128, 128]         --\n","│    └─ModuleList: 2-46                  --                        --\n","│    │    └─ConvLSTMCell: 3-31           [1, 96, 128, 128]         498,048\n","│    │    └─ConvLSTMCell: 3-32           [1, 96, 128, 128]         (recursive)\n","├─SwinTransformerBlock: 1-42             [1, 16384, 96]            --\n","│    └─LayerNorm: 2-47                   [1, 16384, 96]            192\n","│    └─WindowAttention: 2-48             [1024, 16, 96]            392\n","│    │    └─Linear: 3-33                 [1024, 16, 288]           27,936\n","│    │    └─Dropout: 3-34                [1024, 8, 16, 16]         --\n","│    │    └─Linear: 3-35                 [1024, 16, 96]            9,312\n","│    │    └─Dropout: 3-36                [1024, 16, 96]            --\n","│    └─Identity: 2-49                    [1, 16384, 96]            --\n","│    └─LayerNorm: 2-50                   [1, 16384, 96]            192\n","│    └─Sequential: 2-51                  [1, 16384, 96]            --\n","│    │    └─Linear: 3-37                 [1, 16384, 384]           37,248\n","│    │    └─GELU: 3-38                   [1, 16384, 384]           --\n","│    │    └─Linear: 3-39                 [1, 16384, 96]            36,960\n","│    │    └─Dropout: 3-40                [1, 16384, 96]            --\n","│    └─Identity: 2-52                    [1, 16384, 96]            --\n","├─SeparableConv2d: 1-43                  [1, 24, 128, 128]         --\n","│    └─Conv2d: 2-53                      [1, 96, 128, 128]         960\n","│    └─Conv2d: 2-54                      [1, 24, 128, 128]         2,328\n","├─SeparableConv2d: 1-44                  [1, 24, 128, 128]         --\n","│    └─Conv2d: 2-55                      [1, 24, 128, 128]         240\n","│    └─Conv2d: 2-56                      [1, 24, 128, 128]         600\n","├─ConvTranspose2d: 1-45                  [1, 24, 256, 256]         2,328\n","├─BatchNorm2d: 1-46                      [1, 24, 256, 256]         48\n","├─ReLU: 1-47                             [1, 24, 256, 256]         --\n","├─ConvLSTM2D: 1-48                       [1, 48, 256, 256]         --\n","│    └─ModuleList: 2-57                  --                        --\n","│    │    └─ConvLSTMCell: 3-41           [1, 48, 256, 256]         124,608\n","│    │    └─ConvLSTMCell: 3-42           [1, 48, 256, 256]         (recursive)\n","├─SwinTransformerBlock: 1-49             [1, 65536, 48]            --\n","│    └─LayerNorm: 2-58                   [1, 65536, 48]            96\n","│    └─WindowAttention: 2-59             [4096, 16, 48]            392\n","│    │    └─Linear: 3-43                 [4096, 16, 144]           7,056\n","│    │    └─Dropout: 3-44                [4096, 8, 16, 16]         --\n","│    │    └─Linear: 3-45                 [4096, 16, 48]            2,352\n","│    │    └─Dropout: 3-46                [4096, 16, 48]            --\n","│    └─Identity: 2-60                    [1, 65536, 48]            --\n","│    └─LayerNorm: 2-61                   [1, 65536, 48]            96\n","│    └─Sequential: 2-62                  [1, 65536, 48]            --\n","│    │    └─Linear: 3-47                 [1, 65536, 192]           9,408\n","│    │    └─GELU: 3-48                   [1, 65536, 192]           --\n","│    │    └─Linear: 3-49                 [1, 65536, 48]            9,264\n","│    │    └─Dropout: 3-50                [1, 65536, 48]            --\n","│    └─Identity: 2-63                    [1, 65536, 48]            --\n","├─SeparableConv2d: 1-50                  [1, 24, 256, 256]         --\n","│    └─Conv2d: 2-64                      [1, 48, 256, 256]         480\n","│    └─Conv2d: 2-65                      [1, 24, 256, 256]         1,176\n","├─SeparableConv2d: 1-51                  [1, 24, 256, 256]         --\n","│    └─Conv2d: 2-66                      [1, 24, 256, 256]         240\n","│    └─Conv2d: 2-67                      [1, 24, 256, 256]         600\n","├─Conv2d: 1-52                           [1, 2, 256, 256]          434\n","├─ReLU: 1-53                             [1, 2, 256, 256]          --\n","├─Conv2d: 1-54                           [1, 1, 256, 256]          3\n","├─Sigmoid: 1-55                          [1, 1, 256, 256]          --\n","==========================================================================================\n","Total params: 9,605,467\n","Trainable params: 9,605,467\n","Non-trainable params: 0\n","Total mult-adds (G): 88.68\n","==========================================================================================\n","Input size (MB): 0.79\n","Forward/backward pass size (MB): 1298.40\n","Params size (MB): 38.42\n","Estimated Total Size (MB): 1337.60\n","=========================================================================================="]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["from torchinfo import summary\n","\n","# Define the input size based on your model's expected input.\n","# For example, if your model expects images with 3 channels and 256x256 dimensions:\n","input_size = (1, 3, 256, 256)  # (batch_size, channels, height, width)\n","\n","# Generate and print the model summary\n","summary(model, input_size=input_size, device='cuda' if torch.cuda.is_available() else 'cpu')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bFpU5JlU_p4F"},"outputs":[],"source":["# =============================== Training Loop ===========================================\n","\n","num_epochs = 1\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","for epoch in range(num_epochs):\n","    print(\"Epoch\", epoch)\n","    model.train()\n","    running_loss = 0.0\n","    for images, masks in train_loader:\n","        images = images.to(device)  # (B, 3, 256, 256)\n","        masks = masks.to(device)    # (B, 1, 256, 256)\n","\n","        optimizer.zero_grad()\n","        outputs = model(images)      # (B, 1, 256, 256)\n","        loss = criterion(outputs, masks)\n","        loss.backward()\n","        optimizer.step()\n","\n","        running_loss += loss.item() * images.size(0)\n","\n","    epoch_loss = running_loss / len(train_loader.dataset)\n","\n","    # Validation\n","    model.eval()\n","    val_loss = 0.0\n","    with torch.no_grad():\n","        for images, masks in valid_loader:\n","            images = images.to(device)\n","            masks = masks.to(device)\n","\n","            outputs = model(images)\n","            loss = criterion(outputs, masks)\n","\n","            val_loss += loss.item() * images.size(0)\n","\n","    val_loss /= len(valid_loader.dataset)\n","\n","    print(f'Epoch {epoch+1}/{num_epochs}, Training Loss: {epoch_loss:.4f}, Validation Loss: {val_loss:.4f}')\n","\n","    # Scheduler step\n","    scheduler.step(val_loss)\n","\n","    # Early stopping\n","    if val_loss < best_val_loss:\n","        best_val_loss = val_loss\n","        epochs_no_improve = 0\n","        # Save the best model\n","        torch.save(model.state_dict(), r'/content/drive/MyDrive/model/modelWeights_Swin_Trans_Weights_Swin_Trans_Leather.pth')\n","    else:\n","        epochs_no_improve += 1\n","        if epochs_no_improve >= early_stopping_patience:\n","            print('Early stopping!')\n","            break\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r0sihOUA_qyp"},"outputs":[],"source":["\n","# ================================== Prediction ==========================================\n","\n","# Load the best model weights\n","model.load_state_dict(torch.load(r'/content/drive/MyDrive/model/modelWeights_Swin_Trans_Weights_Swin_Trans_Leather.pth'))\n","model.eval()\n","\n","# Function to save predictions and ground truth\n","def save_predictions(model, dataloader, save_dir_pred, save_dir_gt, device):\n","    \"\"\"\n","    Saves the predicted masks and ground truth masks.\n","    Args:\n","        model (nn.Module): Trained model.\n","        dataloader (DataLoader): DataLoader for test data.\n","        save_dir_pred (str): Directory to save predicted masks.\n","        save_dir_gt (str): Directory to save ground truth masks.\n","        device (str): Device to run the model on.\n","    \"\"\"\n","    os.makedirs(save_dir_pred, exist_ok=True)\n","    os.makedirs(save_dir_gt, exist_ok=True)\n","\n","    with torch.no_grad():\n","        for i, (images, masks) in enumerate(dataloader):\n","            if (i % 100 == 0):\n","              print(f\"{i}th Test Image\") # There are total 1000 test images.\n","\n","            images = images.to(device)\n","            masks = masks.to(device)\n","\n","            outputs = model(images)\n","            preds = outputs.cpu().numpy()\n","            gts = masks.cpu().numpy()\n","\n","            for j in range(preds.shape[0]):\n","                pred_mask = preds[j, 0, :, :]\n","                gt_mask = gts[j, 0, :, :]\n","\n","                # Save predicted mask\n","                pred_img = Image.fromarray((pred_mask * 255).astype(np.uint8))\n","                pred_img.save(os.path.join(save_dir_pred, f\"{i * dataloader.batch_size + j + 1}.png\"))\n","\n","                # Save ground truth mask\n","                gt_img = Image.fromarray((gt_mask * 255).astype(np.uint8))\n","                gt_img.save(os.path.join(save_dir_gt, f\"{i * dataloader.batch_size + j + 1}.tiff\"))\n","\n","# Define directories to save predictions and ground truth\n","save_dir_pred = r'/content/drive/MyDrive/output/segmented predicted images'\n","save_dir_gt = r'/content/drive/MyDrive/output/segmented ground truth'\n","\n","# Save predictions\n","save_predictions(model, test_loader, save_dir_pred, save_dir_gt, device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TDKhTCBWqOQm"},"outputs":[],"source":["# # =================================== Evaluation =========================================\n","\n","# def evaluate_metrics_pytorch(model, dataloader, device):\n","#     \"\"\"\n","#     Evaluates various metrics for segmentation performance.\n","#     Args:\n","#         model (nn.Module): Trained model.\n","#         dataloader (DataLoader): DataLoader for test data.\n","#         device (str): Device to run the model on.\n","#     Returns:\n","#         dict: Dictionary containing average metrics.\n","#     \"\"\"\n","#     model.eval()\n","#     all_accuracy = []\n","#     all_dice = []\n","#     all_jaccard = []\n","#     all_sensitivity = []\n","#     all_specificity = []\n","\n","#     with torch.no_grad():\n","#         for images, masks in dataloader:\n","#             images = images.to(device)\n","#             masks = masks.to(device)\n","\n","#             outputs = model(images)\n","#             preds = outputs > 0.5  # Binary mask\n","\n","#             preds = preds.cpu().numpy().astype(np.uint8)\n","#             masks = masks.cpu().numpy().astype(np.uint8)\n","\n","#             for pred, mask in zip(preds, masks):\n","#                 pred_flat = pred.flatten()\n","#                 mask_flat = mask.flatten()\n","\n","#                 # Precision-Recall Curve to find optimal threshold\n","#                 precisions, recalls, thresholds = precision_recall_curve(mask_flat, pred.flatten())\n","#                 f1 = 2 * (precisions * recalls) / (precisions + recalls + 1e-8)\n","#                 max_idx = np.argmax(f1)\n","#                 optimal_thresh = thresholds[max_idx] if max_idx < len(thresholds) else 0.5\n","\n","#                 # Apply optimal threshold\n","#                 pred_opt = (pred_flat >= optimal_thresh).astype(np.uint8)\n","\n","#                 # Confusion matrix\n","#                 tn, fp, fn, tp = confusion_matrix(mask_flat, pred_opt).ravel()\n","\n","#                 # Calculate metrics\n","#                 accuracy = (tp + tn) / (tp + tn + fp + fn + 1e-8)\n","#                 iou = tp / (tp + fp + fn + 1e-8)\n","#                 dice = (2 * tp) / (2 * tp + fp + fn + 1e-8)\n","#                 specificity = tn / (tn + fp + 1e-8)\n","#                 sensitivity = tp / (tp + fn + 1e-8)\n","\n","#                 all_accuracy.append(accuracy)\n","#                 all_jaccard.append(iou)\n","#                 all_dice.append(dice)\n","#                 all_specificity.append(specificity)\n","#                 all_sensitivity.append(sensitivity)\n","\n","#     # Compute average metrics\n","#     metrics = {\n","#         'Accuracy': np.mean(all_accuracy),\n","#         'Dice': np.mean(all_dice),\n","#         'Jaccard': np.mean(all_jaccard),\n","#         'Sensitivity': np.mean(all_sensitivity),\n","#         'Specificity': np.mean(all_specificity)\n","#     }\n","\n","#     print(f\"Accuracy: {metrics['Accuracy']:.4f}, Dice: {metrics['Dice']:.4f}, Jaccard: {metrics['Jaccard']:.4f}, \"\n","#           f\"Sensitivity: {metrics['Sensitivity']:.4f}, Specificity: {metrics['Specificity']:.4f}\")\n","\n","#     return metrics\n","\n","# # Evaluate the model\n","# metrics = evaluate_metrics_pytorch(model, test_loader, device)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":191861,"status":"ok","timestamp":1730045247859,"user":{"displayName":"ALOK THOTTAKATHU PRASANNAKUMAR","userId":"10483471326433970042"},"user_tz":-660},"id":"rirMHwEit-lK","outputId":"353dc980-bf18-4121-bf0a-62b103beaa94"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy: 0.8472, Dice: 0.8024, Jaccard: 0.7119, Sensitivity: 0.8899, Specificity: 0.7576\n"]}],"source":["from sklearn.metrics import confusion_matrix, precision_recall_curve\n","import numpy as np\n","import torch\n","\n","def evaluate_metrics_pytorch(model, dataloader, device):\n","    \"\"\"\n","    Evaluates various metrics for segmentation performance.\n","    Args:\n","        model (nn.Module): Trained model.\n","        dataloader (DataLoader): DataLoader for test data.\n","        device (str): Device to run the model on.\n","    Returns:\n","        dict: Dictionary containing average metrics.\n","    \"\"\"\n","    model.eval()\n","    all_accuracy = []\n","    all_dice = []\n","    all_jaccard = []\n","    all_sensitivity = []\n","    all_specificity = []\n","\n","    with torch.no_grad():\n","        for images, masks in dataloader:\n","            images = images.to(device)\n","            masks = masks.to(device)\n","\n","            outputs = model(images)\n","            preds = outputs > 0.5  # Binary mask\n","\n","            preds = preds.cpu().numpy().astype(np.uint8)\n","            masks = masks.cpu().numpy().astype(np.uint8)\n","            masks = (masks > 0).astype(np.uint8)  # Convert 255 to 1\n","\n","            for pred, mask in zip(preds, masks):\n","                pred_flat = pred.flatten()\n","                mask_flat = mask.flatten()\n","\n","                # Precision-Recall Curve to find optimal threshold\n","                precisions, recalls, thresholds = precision_recall_curve(mask_flat, pred_flat)\n","                f1 = 2 * (precisions * recalls) / (precisions + recalls + 1e-8)\n","                max_idx = np.argmax(f1)\n","                optimal_thresh = thresholds[max_idx] if max_idx < len(thresholds) else 0.5\n","\n","                # Apply optimal threshold\n","                pred_opt = (pred_flat >= optimal_thresh).astype(np.uint8)\n","\n","                # Confusion matrix with specified labels\n","                cm = confusion_matrix(mask_flat, pred_opt, labels=[0,1])\n","\n","                # Unpack confusion matrix\n","                tn, fp, fn, tp = cm.ravel()\n","\n","                # Calculate metrics\n","                accuracy = (tp + tn) / (tp + tn + fp + fn + 1e-8)\n","                iou = tp / (tp + fp + fn + 1e-8)\n","                dice = (2 * tp) / (2 * tp + fp + fn + 1e-8)\n","                specificity = tn / (tn + fp + 1e-8)\n","                sensitivity = tp / (tp + fn + 1e-8)\n","\n","                all_accuracy.append(accuracy)\n","                all_jaccard.append(iou)\n","                all_dice.append(dice)\n","                all_specificity.append(specificity)\n","                all_sensitivity.append(sensitivity)\n","\n","    # Compute average metrics\n","    metrics = {\n","        'Accuracy': np.mean(all_accuracy),\n","        'Dice': np.mean(all_dice),\n","        'Jaccard': np.mean(all_jaccard),\n","        'Sensitivity': np.mean(all_sensitivity),\n","        'Specificity': np.mean(all_specificity)\n","    }\n","\n","    print(f\"Accuracy: {metrics['Accuracy']:.4f}, Dice: {metrics['Dice']:.4f}, Jaccard: {metrics['Jaccard']:.4f}, \"\n","          f\"Sensitivity: {metrics['Sensitivity']:.4f}, Specificity: {metrics['Specificity']:.4f}\")\n","\n","    return metrics\n","\n","# Evaluate the model\n","metrics = evaluate_metrics_pytorch(model, test_loader, device)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":146},"executionInfo":{"elapsed":1222,"status":"error","timestamp":1730049397396,"user":{"displayName":"ALOK THOTTAKATHU PRASANNAKUMAR","userId":"10483471326433970042"},"user_tz":-660},"id":"O_HoLxb9-ogo","outputId":"1dde0255-4799-406a-f5eb-de920de08036"},"outputs":[{"ename":"NameError","evalue":"name 'model' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-5f15418b3570>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"]}],"source":["model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ANBXWn1pQp17"},"outputs":[],"source":["1+2"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4648,"status":"ok","timestamp":1730034385132,"user":{"displayName":"ALOK THOTTAKATHU PRASANNAKUMAR","userId":"10483471326433970042"},"user_tz":-660},"id":"s6u_PM5_FX8j","outputId":"ea7881d4-4371-4865-bd9c-e73de9107520"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting torchinfo\n","  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n","Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n","Installing collected packages: torchinfo\n","Successfully installed torchinfo-1.8.0\n"]}],"source":["!pip install torchinfo"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":497},"executionInfo":{"elapsed":505,"status":"error","timestamp":1730034436272,"user":{"displayName":"ALOK THOTTAKATHU PRASANNAKUMAR","userId":"10483471326433970042"},"user_tz":-660},"id":"HnUVFWKSCuRE","outputId":"825f67d6-22aa-43b9-8c35-e9497f1db9fd"},"outputs":[{"ename":"RuntimeError","evalue":"Failed to run torchinfo. See above stack traces for more details. Executed layers up to: [SeparableConv2d: 1, Conv2d: 2, Conv2d: 2, BatchNorm2d: 1, SeparableConv2d: 1, Conv2d: 2, Conv2d: 2, BatchNorm2d: 1, MaxPool2d: 1]","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchinfo/torchinfo.py\u001b[0m in \u001b[0;36mforward_pass\u001b[0;34m(model, x, batch_dim, cache_forward_pass, device, mode, **kwargs)\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1843\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1844\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1845\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36minner\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1790\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1791\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0m_global_forward_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-2-1a508d6676cb>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    458\u001b[0m         \u001b[0mp1_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (B, 128*128, 24)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 459\u001b[0;31m         \u001b[0mswin_E1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswin_unet_E1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp1_flat\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;31m# (B, 128*128, 32)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    460\u001b[0m         \u001b[0mswin_E1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mswin_E1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Reshape for Conv2d\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1843\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1844\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1845\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36minner\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1790\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1791\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0m_global_forward_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-2-1a508d6676cb>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    247\u001b[0m         \u001b[0mshortcut\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1843\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1844\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1845\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36minner\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1790\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1791\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0m_global_forward_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/normalization.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m         return F.layer_norm(\n\u001b[0m\u001b[1;32m    218\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalized_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlayer_norm\u001b[0;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[1;32m   2899\u001b[0m         )\n\u001b[0;32m-> 2900\u001b[0;31m     return torch.layer_norm(\n\u001b[0m\u001b[1;32m   2901\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalized_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Given normalized_shape=[32], expected input with shape [*, 32], but got input of size[1, 16384, 24]","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-c7cf18e667e9>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Display the model summary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Batch size of 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchinfo/torchinfo.py\u001b[0m in \u001b[0;36msummary\u001b[0;34m(model, input_size, input_data, batch_dim, cache_forward_pass, col_names, col_width, depth, device, dtypes, mode, row_settings, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0minput_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m     )\n\u001b[0;32m--> 223\u001b[0;31m     summary_list = forward_pass(\n\u001b[0m\u001b[1;32m    224\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_forward_pass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m     )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchinfo/torchinfo.py\u001b[0m in \u001b[0;36mforward_pass\u001b[0;34m(model, x, batch_dim, cache_forward_pass, device, mode, **kwargs)\u001b[0m\n\u001b[1;32m    302\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[0mexecuted_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlayer\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msummary_list\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuted\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m         raise RuntimeError(\n\u001b[0m\u001b[1;32m    305\u001b[0m             \u001b[0;34m\"Failed to run torchinfo. See above stack traces for more details. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m             \u001b[0;34mf\"Executed layers up to: {executed_layers}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Failed to run torchinfo. See above stack traces for more details. Executed layers up to: [SeparableConv2d: 1, Conv2d: 2, Conv2d: 2, BatchNorm2d: 1, SeparableConv2d: 1, Conv2d: 2, Conv2d: 2, BatchNorm2d: 1, MaxPool2d: 1]"]}],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":460},"executionInfo":{"elapsed":553,"status":"error","timestamp":1730034453560,"user":{"displayName":"ALOK THOTTAKATHU PRASANNAKUMAR","userId":"10483471326433970042"},"user_tz":-660},"id":"7lvQREx2CvqH","outputId":"87602f70-6edb-4596-d707-5161255b2ceb"},"outputs":[{"ename":"RuntimeError","evalue":"Failed to run torchinfo. See above stack traces for more details. Executed layers up to: []","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchinfo/torchinfo.py\u001b[0m in \u001b[0;36mforward_pass\u001b[0;34m(model, x, batch_dim, cache_forward_pass, device, mode, **kwargs)\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1843\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1844\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1845\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36minner\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1790\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1791\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0m_global_forward_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: SwinUNet.forward() got an unexpected keyword argument 'batch_size'","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-c86f0703ea2c>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchinfo/torchinfo.py\u001b[0m in \u001b[0;36msummary\u001b[0;34m(model, input_size, input_data, batch_dim, cache_forward_pass, col_names, col_width, depth, device, dtypes, mode, row_settings, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0minput_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m     )\n\u001b[0;32m--> 223\u001b[0;31m     summary_list = forward_pass(\n\u001b[0m\u001b[1;32m    224\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_forward_pass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m     )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchinfo/torchinfo.py\u001b[0m in \u001b[0;36mforward_pass\u001b[0;34m(model, x, batch_dim, cache_forward_pass, device, mode, **kwargs)\u001b[0m\n\u001b[1;32m    302\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[0mexecuted_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlayer\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msummary_list\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuted\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m         raise RuntimeError(\n\u001b[0m\u001b[1;32m    305\u001b[0m             \u001b[0;34m\"Failed to run torchinfo. See above stack traces for more details. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m             \u001b[0;34mf\"Executed layers up to: {executed_layers}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Failed to run torchinfo. See above stack traces for more details. Executed layers up to: []"]}],"source":[]},{"cell_type":"markdown","metadata":{"id":"QPptVwXsXVnA"},"source":["BIDIRECTIONAL CONVLSTM"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ECZrcdEWvgVK"},"outputs":[],"source":["# ============================== Imports and Dependencies ==============================\n","\n","import os\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from PIL import Image\n","from zipfile import ZipFile\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import precision_recall_curve, confusion_matrix\n","\n","# ================================ Separable Convolution =================================\n","\n","class SeparableConv2d(nn.Module):\n","    \"\"\"\n","    Implements a separable convolution layer using depthwise and pointwise convolutions.\n","    \"\"\"\n","    def __init__(self, in_channels, out_channels, kernel_size=3, padding=1, bias=True):\n","        super(SeparableConv2d, self).__init__()\n","        # Depthwise convolution (groups=in_channels)\n","        self.depthwise = nn.Conv2d(in_channels, in_channels, kernel_size=kernel_size,\n","                                   padding=padding, groups=in_channels, bias=bias)\n","        # Pointwise convolution\n","        self.pointwise = nn.Conv2d(in_channels, out_channels, kernel_size=1,\n","                                   padding=0, bias=bias)\n","\n","    def forward(self, x):\n","        x = self.depthwise(x)\n","        x = self.pointwise(x)\n","        return x\n","\n","# ================================== ConvLSTM2D ========================================\n","\n","class ConvLSTMCell(nn.Module):\n","    \"\"\"\n","    Implements a ConvLSTM cell.\n","    \"\"\"\n","    def __init__(self, input_channels, hidden_channels, kernel_size, bias=True):\n","        super(ConvLSTMCell, self).__init__()\n","\n","        padding = kernel_size // 2  # To maintain spatial dimensions\n","        self.input_channels = input_channels\n","        self.hidden_channels = hidden_channels\n","\n","        self.conv = nn.Conv2d(in_channels=input_channels + hidden_channels,\n","                              out_channels=4 * hidden_channels,\n","                              kernel_size=kernel_size,\n","                              padding=padding,\n","                              bias=bias)\n","\n","    def forward(self, input_tensor, cur_state):\n","        h_cur, c_cur = cur_state\n","\n","        # Concatenate input and hidden state\n","        combined = torch.cat([input_tensor, h_cur], dim=1)  # along channel axis\n","\n","        # Compute all gates at once\n","        conv_output = self.conv(combined)\n","        cc_i, cc_f, cc_o, cc_g = torch.split(conv_output, self.hidden_channels, dim=1)\n","\n","        i = torch.sigmoid(cc_i)   # input gate\n","        f = torch.sigmoid(cc_f)   # forget gate\n","        o = torch.sigmoid(cc_o)   # output gate\n","        g = torch.tanh(cc_g)      # gate gate\n","\n","        c_next = f * c_cur + i * g\n","        h_next = o * torch.tanh(c_next)\n","\n","        return h_next, c_next\n","\n","    def init_hidden(self, batch_size, spatial_size, device):\n","        height, width = spatial_size\n","        return (torch.zeros(batch_size, self.hidden_channels, height, width, device=device),\n","                torch.zeros(batch_size, self.hidden_channels, height, width, device=device))\n","\n","class ConvLSTM2D(nn.Module):\n","    \"\"\"\n","    Implements a ConvLSTM2D layer that processes a sequence of inputs.\n","    \"\"\"\n","    def __init__(self, input_channels, hidden_channels, kernel_size=3, bias=True, num_layers=1):\n","        super(ConvLSTM2D, self).__init__()\n","        self.num_layers = num_layers\n","        self.hidden_channels = hidden_channels\n","\n","        layers = []\n","        for i in range(num_layers):\n","            input_c = input_channels if i == 0 else hidden_channels\n","            layers.append(ConvLSTMCell(input_c, hidden_channels, kernel_size, bias))\n","        self.layers = nn.ModuleList(layers)\n","\n","    def forward(self, input_tensor, reverse=False):\n","        # input_tensor shape: (batch, seq_len, channels, height, width)\n","        batch_size, seq_len, channels, height, width = input_tensor.size()\n","        device = input_tensor.device\n","\n","        # Initialize hidden and cell states for all layers\n","        hidden_state = []\n","        cell_state = []\n","        for i in range(self.num_layers):\n","            h, c = self.layers[i].init_hidden(batch_size, (height, width), device)\n","            hidden_state.append(h)\n","            cell_state.append(c)\n","\n","        # Iterate over time steps\n","        if reverse:\n","            time_steps = reversed(range(seq_len))\n","        else:\n","            time_steps = range(seq_len)\n","\n","        outputs = []\n","        for t in time_steps:\n","            x = input_tensor[:, t, :, :, :]  # (batch, channels, height, width)\n","            for i, layer in enumerate(self.layers):\n","                h, c = layer(x, (hidden_state[i], cell_state[i]))\n","                hidden_state[i] = h\n","                cell_state[i] = c\n","                x = h  # input to next layer\n","            outputs.append(x)\n","\n","        outputs = torch.stack(outputs, dim=1)  # (batch, seq_len, channels, height, width)\n","        if reverse:\n","            outputs = outputs.flip(dims=[1])  # Reverse back to original order\n","        return outputs  # Return the sequence of outputs\n","\n","class BidirectionalConvLSTM2D(nn.Module):\n","    \"\"\"\n","    Implements a Bidirectional ConvLSTM2D layer.\n","    Processes the input sequence in both forward and backward directions and concatenates the outputs.\n","    \"\"\"\n","    def __init__(self, input_channels, hidden_channels, kernel_size=3, num_layers=1, bias=True):\n","        super(BidirectionalConvLSTM2D, self).__init__()\n","        self.forward_conv_lstm = ConvLSTM2D(input_channels, hidden_channels, kernel_size, bias=bias, num_layers=num_layers)\n","        self.backward_conv_lstm = ConvLSTM2D(input_channels, hidden_channels, kernel_size, bias=bias, num_layers=num_layers)\n","\n","    def forward(self, input_tensor):\n","        # input_tensor shape: (batch, seq_len, channels, height, width)\n","        # Forward direction\n","        forward_output = self.forward_conv_lstm(input_tensor, reverse=False)  # (batch, seq_len, hidden_channels, H, W)\n","        # Backward direction\n","        backward_output = self.backward_conv_lstm(input_tensor, reverse=True)  # (batch, seq_len, hidden_channels, H, W)\n","        # Concatenate outputs along the channel dimension\n","        output = torch.cat([forward_output, backward_output], dim=2)  # (batch, seq_len, hidden_channels*2, H, W)\n","        # Since seq_len=1 in our case after merging, we can squeeze the seq_len dimension\n","        output = output[:, -1, :, :, :]  # Take the last output (batch, hidden_channels*2, H, W)\n","        return output\n","\n","# ============================== Swin Transformer Blocks ================================\n","\n","class WindowAttention(nn.Module):\n","    \"\"\"\n","    Window based multi-head self attention (W-MSA) module with relative position bias.\n","    \"\"\"\n","    def __init__(self, dim, window_size, num_heads, qkv_bias=True, attn_drop=0., proj_drop=0.):\n","        \"\"\"\n","        Args:\n","            dim (int): Number of input channels.\n","            window_size (tuple): Height and width of the window.\n","            num_heads (int): Number of attention heads.\n","            qkv_bias (bool): If True, add a learnable bias to query, key, value.\n","            attn_drop (float): Dropout ratio of attention weights.\n","            proj_drop (float): Dropout ratio of output.\n","        \"\"\"\n","        super(WindowAttention, self).__init__()\n","        self.dim = dim\n","        self.window_size = window_size  # Wh, Ww\n","        self.num_heads = num_heads\n","        head_dim = dim // num_heads\n","        self.scale = head_dim ** -0.5\n","\n","        # Define a parameter table of relative position bias\n","        self.relative_position_bias_table = nn.Parameter(\n","            torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads)\n","        )  # 2*Wh-1 * 2*Ww-1, nH\n","\n","        # Get pair-wise relative position index for each token inside the window\n","        coords_h = torch.arange(self.window_size[0])\n","        coords_w = torch.arange(self.window_size[1])\n","        coords = torch.stack(torch.meshgrid(coords_h, coords_w, indexing='ij'))  # 2, Wh, Ww\n","        coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n","        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n","        relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n","        relative_coords[:, :, 0] += self.window_size[0] - 1  # shift to start from 0\n","        relative_coords[:, :, 1] += self.window_size[1] - 1\n","        relative_coords[:, :, 0] *= (2 * self.window_size[1] - 1)\n","        relative_position_index = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n","        self.register_buffer(\"relative_position_index\", relative_position_index)\n","\n","        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)  # Query, Key, Value\n","        self.attn_drop = nn.Dropout(attn_drop)\n","        self.proj = nn.Linear(dim, dim)\n","        self.proj_drop = nn.Dropout(proj_drop)\n","\n","        # Initialize relative position bias table\n","        nn.init.trunc_normal_(self.relative_position_bias_table, std=.02)\n","\n","    def forward(self, x, mask=None):\n","        \"\"\"\n","        Args:\n","            x: input features with shape of (num_windows*B, Wh*Ww, C)\n","            mask: (num_windows, Wh*Ww, Wh*Ww) or None\n","        \"\"\"\n","        B_, N, C = x.shape\n","        qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)  # 3, B_, nH, N, C//nH\n","        q, k, v = qkv[0], qkv[1], qkv[2]  # each has shape (B_, nH, N, C//nH)\n","\n","        q = q * self.scale\n","        attn = (q @ k.transpose(-2, -1))  # (B_, nH, N, N)\n","\n","        # Add relative position bias\n","        relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(\n","            self.window_size[0] * self.window_size[1], self.window_size[0] * self.window_size[1], -1\n","        )  # Wh*Ww, Wh*Ww, nH\n","        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n","        attn = attn + relative_position_bias.unsqueeze(0)  # (B_, nH, N, N)\n","\n","        if mask is not None:\n","            nW = mask.shape[0]\n","            attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)\n","            attn = attn.view(-1, self.num_heads, N, N)\n","            attn = F.softmax(attn, dim=-1)\n","        else:\n","            attn = F.softmax(attn, dim=-1)\n","\n","        attn = self.attn_drop(attn)\n","\n","        out = (attn @ v).transpose(1, 2).reshape(B_, N, C)  # (B_, N, C)\n","        out = self.proj(out)\n","        out = self.proj_drop(out)\n","        return out\n","\n","class SwinTransformerBlock(nn.Module):\n","    \"\"\"\n","    Swin Transformer Block with W-MSA and SW-MSA.\n","    \"\"\"\n","    def __init__(self, dim, num_heads, window_size=7, shift_size=0, mlp_ratio=4., qkv_bias=True,\n","                 attn_drop=0., proj_drop=0.):\n","        super(SwinTransformerBlock, self).__init__()\n","        self.dim = dim\n","        self.num_heads = num_heads\n","        self.window_size = window_size  # W\n","        self.shift_size = shift_size    # S\n","        self.mlp_ratio = mlp_ratio\n","\n","        assert 0 <= self.shift_size < self.window_size, \"shift_size must be in [0, window_size)\"\n","\n","        self.norm1 = nn.LayerNorm(dim)\n","        self.attn = WindowAttention(dim, (window_size, window_size), num_heads, qkv_bias, attn_drop, proj_drop)\n","\n","        self.drop_path = nn.Identity()  # Can implement stochastic depth if desired\n","        self.norm2 = nn.LayerNorm(dim)\n","        mlp_hidden_dim = int(dim * mlp_ratio)\n","        self.mlp = nn.Sequential(\n","            nn.Linear(dim, mlp_hidden_dim),\n","            nn.GELU(),\n","            nn.Linear(mlp_hidden_dim, dim),\n","            nn.Dropout(proj_drop)\n","        )\n","\n","    def forward(self, x):\n","        \"\"\"\n","        Args:\n","            x: input features with shape (B, H*W, C)\n","        \"\"\"\n","        H = W = int(np.sqrt(x.shape[1]))\n","        B, L, C = x.shape\n","        assert L == H * W, \"Input feature has wrong size\"\n","\n","        shortcut = x\n","        x = self.norm1(x)\n","        x = x.view(B, H, W, C)\n","\n","        # Cyclic shift\n","        if self.shift_size > 0:\n","            shifted_x = torch.roll(x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2))\n","        else:\n","            shifted_x = x\n","\n","        # Partition windows\n","        window_size = self.window_size\n","        # Pad H and W to be multiples of window_size\n","        pad_b = (window_size - H % window_size) % window_size\n","        pad_r = (window_size - W % window_size) % window_size\n","        shifted_x = F.pad(shifted_x, (0, 0, 0, pad_r, 0, pad_b))  # pad H and W\n","        _, Hp, Wp, _ = shifted_x.shape\n","\n","        # Window partition\n","        x_windows = shifted_x.view(B, Hp // window_size, window_size, Wp // window_size, window_size, C)\n","        x_windows = x_windows.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size * window_size, C)  # (num_windows*B, window_size*window_size, C)\n","\n","        # Attention\n","        attn_windows = self.attn(x_windows)  # (num_windows*B, window_size*window_size, C)\n","\n","        # Merge windows\n","        shifted_x = attn_windows.view(-1, window_size, window_size, C)\n","        shifted_x = shifted_x.view(B, Hp // window_size, Wp // window_size, window_size, window_size, C)\n","        shifted_x = shifted_x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, Hp, Wp, C)\n","\n","        # Reverse cyclic shift\n","        if self.shift_size > 0:\n","            x = torch.roll(shifted_x, shifts=(self.shift_size, self.shift_size), dims=(1, 2))\n","        else:\n","            x = shifted_x\n","\n","        # Remove padding\n","        x = x[:, :H, :W, :].contiguous().view(B, H * W, C)\n","\n","        # FFN\n","        x = shortcut + self.drop_path(x)\n","        x = x + self.drop_path(self.mlp(self.norm2(x)))\n","\n","        return x\n","\n","# =============================== Dice Loss Function ====================================\n","\n","class DiceLoss(nn.Module):\n","    \"\"\"\n","    Dice Loss function to maximize the Dice coefficient.\n","    Suitable for binary segmentation tasks.\n","    \"\"\"\n","    def __init__(self, smooth=1.0):\n","        super(DiceLoss, self).__init__()\n","        self.smooth = smooth\n","\n","    def forward(self, y_pred, y_true):\n","        \"\"\"\n","        Args:\n","            y_pred (torch.Tensor): Predicted mask probabilities with shape (B, 1, H, W)\n","            y_true (torch.Tensor): Ground truth masks with shape (B, 1, H, W)\n","        Returns:\n","            torch.Tensor: Dice loss\n","        \"\"\"\n","        y_pred = y_pred.view(-1)\n","        y_true = y_true.view(-1)\n","\n","        intersection = (y_pred * y_true).sum()\n","        dice = (2. * intersection + self.smooth) / (y_pred.sum() + y_true.sum() + self.smooth)\n","\n","        return 1 - dice\n","\n","# ================================ Main Model ============================================\n","\n","class SwinUNet(nn.Module):\n","    \"\"\"\n","    Swin U-Net architecture for image segmentation with bidirectional ConvLSTM layers.\n","    \"\"\"\n","    def __init__(self, input_channels=3, output_channels=1,\n","                 embed_dim=32, num_heads=[4, 8], window_size=4,\n","                 mlp_ratio=4., depth=2):\n","        super(SwinUNet, self).__init__()\n","        self.input_channels = input_channels\n","        self.output_channels = output_channels\n","\n","        # Initial convolutional layers\n","        self.conv1 = SeparableConv2d(input_channels, 24, kernel_size=3, padding=1)\n","        self.bn1 = nn.BatchNorm2d(24)\n","        self.conv2 = SeparableConv2d(24, 24, kernel_size=3, padding=1)\n","        self.bn2 = nn.BatchNorm2d(24)\n","        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)  # 256x256 -> 128x128\n","\n","        # First Swin Transformer Block\n","        self.swin_unet_E1 = SwinTransformerBlock(\n","            dim=24,  # Changed from embed_dim=32 to 24\n","            num_heads=num_heads[0],\n","            window_size=window_size,\n","            shift_size=window_size//2 if True else 0,\n","            mlp_ratio=mlp_ratio\n","        )\n","\n","        # Second convolutional block\n","        self.conv3 = SeparableConv2d(24, 48, kernel_size=3, padding=1)\n","        self.bn3 = nn.BatchNorm2d(48)\n","        self.conv4 = SeparableConv2d(48, 48, kernel_size=3, padding=1)\n","        self.bn4 = nn.BatchNorm2d(48)\n","        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)  # 128x128 -> 64x64\n","\n","        # Second Swin Transformer Block\n","        self.swin_unet_E2 = SwinTransformerBlock(\n","            dim=48,\n","            num_heads=num_heads[1],\n","            window_size=window_size,\n","            shift_size=window_size//2 if True else 0,\n","            mlp_ratio=mlp_ratio\n","        )\n","\n","        # Third convolutional block (Bottleneck)\n","        self.conv5 = SeparableConv2d(48, 96, kernel_size=3, padding=1)\n","        self.bn5 = nn.BatchNorm2d(96)\n","        self.conv6 = SeparableConv2d(96, 96, kernel_size=3, padding=1)\n","        self.bn6 = nn.BatchNorm2d(96)\n","        self.drop5 = nn.Dropout(0.5)\n","        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)  # 64x64 -> 32x32\n","\n","        # Bottleneck convolutions with dense connections\n","        self.conv7 = SeparableConv2d(96, 192, kernel_size=3, padding=1)\n","        self.bn7 = nn.BatchNorm2d(192)\n","        self.conv8 = SeparableConv2d(192, 192, kernel_size=3, padding=1)\n","        self.bn8 = nn.BatchNorm2d(192)\n","        self.drop6_1 = nn.Dropout(0.5)\n","\n","        self.conv9 = SeparableConv2d(192, 192, kernel_size=3, padding=1)\n","        self.bn9 = nn.BatchNorm2d(192)\n","        self.conv10 = SeparableConv2d(192, 192, kernel_size=3, padding=1)\n","        self.bn10 = nn.BatchNorm2d(192)\n","        self.drop6_2 = nn.Dropout(0.5)\n","\n","        self.concat1 = nn.Sequential(\n","            SeparableConv2d(384, 192, kernel_size=3, padding=1),\n","            SeparableConv2d(192, 192, kernel_size=3, padding=1)\n","        )\n","        self.drop6_3 = nn.Dropout(0.5)\n","\n","        # First Upsampling Block\n","        self.up1 = nn.ConvTranspose2d(192, 96, kernel_size=2, stride=2)  # 32x32 -> 64x64\n","        self.bn_up1 = nn.BatchNorm2d(96)\n","        self.relu_up1 = nn.ReLU(inplace=True)\n","        self.bidirectional_convLSTM1 = BidirectionalConvLSTM2D(input_channels=96, hidden_channels=192, kernel_size=3, num_layers=1)\n","        self.swin_unet_D1 = SwinTransformerBlock(\n","            dim=192 * 2,  # Adjusted for bidirectional output\n","            num_heads=num_heads[0],\n","            window_size=window_size,\n","            shift_size=window_size//2 if True else 0,\n","            mlp_ratio=mlp_ratio\n","        )\n","        self.conv11 = SeparableConv2d(192 * 2, 48, kernel_size=3, padding=1)\n","        self.conv12 = SeparableConv2d(48, 48, kernel_size=3, padding=1)\n","\n","        # Second Upsampling Block\n","        self.up2 = nn.ConvTranspose2d(48, 48, kernel_size=2, stride=2)  # 64x64 -> 128x128\n","        self.bn_up2 = nn.BatchNorm2d(48)\n","        self.relu_up2 = nn.ReLU(inplace=True)\n","        self.bidirectional_convLSTM2 = BidirectionalConvLSTM2D(input_channels=48, hidden_channels=96, kernel_size=3, num_layers=1)\n","        self.swin_unet_D2 = SwinTransformerBlock(\n","            dim=96 * 2,\n","            num_heads=num_heads[1],\n","            window_size=window_size,\n","            shift_size=window_size//2 if True else 0,\n","            mlp_ratio=mlp_ratio\n","        )\n","        self.conv13 = SeparableConv2d(96 * 2, 24, kernel_size=3, padding=1)\n","        self.conv14 = SeparableConv2d(24, 24, kernel_size=3, padding=1)\n","\n","        # Third Upsampling Block\n","        self.up3 = nn.ConvTranspose2d(24, 24, kernel_size=2, stride=2)  # 128x128 -> 256x256\n","        self.bn_up3 = nn.BatchNorm2d(24)\n","        self.relu_up3 = nn.ReLU(inplace=True)\n","        self.bidirectional_convLSTM3 = BidirectionalConvLSTM2D(input_channels=24, hidden_channels=48, kernel_size=3, num_layers=1)\n","        self.swin_unet_D3 = SwinTransformerBlock(\n","            dim=48 * 2,\n","            num_heads=num_heads[1],\n","            window_size=window_size,\n","            shift_size=window_size//2 if True else 0,\n","            mlp_ratio=mlp_ratio\n","        )\n","        self.conv15 = SeparableConv2d(48 * 2, 24, kernel_size=3, padding=1)\n","        self.conv16 = SeparableConv2d(24, 24, kernel_size=3, padding=1)\n","\n","        # Output Layer\n","        self.final_conv1 = nn.Conv2d(24, 2, kernel_size=3, padding=1)\n","        self.final_relu = nn.ReLU(inplace=True)\n","        self.final_conv2 = nn.Conv2d(2, 1, kernel_size=1, padding=0)\n","        self.final_sigmoid = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        \"\"\"\n","        Forward pass of the Swin U-Net model.\n","        Args:\n","            x: Input tensor with shape (B, 3, 256, 256)\n","        Returns:\n","            torch.Tensor: Output segmentation mask with shape (B, 1, 256, 256)\n","        \"\"\"\n","        # Initial Convolutions\n","        x1 = self.conv1(x)          # (B, 24, 256, 256)\n","        x1 = self.bn1(x1)\n","        x1 = self.conv2(x1)         # (B, 24, 256, 256)\n","        x1 = self.bn2(x1)\n","        p1 = self.pool1(x1)         # (B, 24, 128, 128)\n","\n","        # First Swin Transformer Block\n","        p1_flat = p1.flatten(2).transpose(1, 2)  # (B, 128*128, 24)\n","        swin_E1 = self.swin_unet_E1(p1_flat)     # (B, 128*128, 24)\n","        swin_E1 = swin_E1.transpose(1, 2).view(-1, 24, 128, 128)  # Reshape for Conv2d\n","\n","        # Second Convolutional Block\n","        x2 = self.conv3(swin_E1)    # (B, 48, 128, 128)\n","        x2 = self.bn3(x2)\n","        x2 = self.conv4(x2)          # (B, 48, 128, 128)\n","        x2 = self.bn4(x2)\n","        p2 = self.pool2(x2)          # (B, 48, 64, 64)\n","\n","        # Second Swin Transformer Block\n","        p2_flat = p2.flatten(2).transpose(1, 2)  # (B, 64*64, 48)\n","        swin_E2 = self.swin_unet_E2(p2_flat)     # (B, 64*64, 48)\n","        swin_E2 = swin_E2.transpose(1, 2).view(-1, 48, 64, 64)  # Reshape for Conv2d\n","\n","        # Third Convolutional Block (Bottleneck)\n","        x3 = self.conv5(swin_E2)    # (B, 96, 64, 64)\n","        x3 = self.bn5(x3)\n","        x3 = self.conv6(x3)          # (B, 96, 64, 64)\n","        x3 = self.bn6(x3)\n","        x3 = self.drop5(x3)\n","        p3 = self.pool3(x3)          # (B, 96, 32, 32)\n","\n","        # Bottleneck Convolutions with Dense Connections\n","        x4 = self.conv7(p3)          # (B, 192, 32, 32)\n","        x4 = self.bn7(x4)\n","        x4 = self.conv8(x4)          # (B, 192, 32, 32)\n","        x4 = self.bn8(x4)\n","        x4 = self.drop6_1(x4)\n","\n","        x5 = self.conv9(x4)          # (B, 192, 32, 32)\n","        x5 = self.bn9(x5)\n","        x5 = self.conv10(x5)         # (B, 192, 32, 32)\n","        x5 = self.bn10(x5)\n","        x5 = self.drop6_2(x5)\n","\n","        concat = torch.cat([x5, x4], dim=1)  # (B, 384, 32, 32)\n","        concat = self.concat1(concat)         # (B, 192, 32, 32)\n","        concat = self.drop6_3(concat)         # (B, 192, 32, 32)\n","\n","        # First Upsampling Block\n","        up1 = self.up1(concat)                 # (B, 96, 64, 64)\n","        up1 = self.bn_up1(up1)\n","        up1 = self.relu_up1(up1)\n","\n","        # Prepare for BidirectionalConvLSTM2D\n","        up1_seq = torch.stack([x3, up1], dim=1)  # (B, 2, 96, 64, 64)\n","        bidir_convLSTM1_out = self.bidirectional_convLSTM1(up1_seq)  # (B, 192*2, 64, 64)\n","\n","        # Swin Transformer Block in Decoder\n","        bidir_convLSTM1_flat = bidir_convLSTM1_out.flatten(2).transpose(1, 2)  # (B, 64*64, 192*2)\n","        swin_D1 = self.swin_unet_D1(bidir_convLSTM1_flat)               # (B, 64*64, 192*2)\n","        swin_D1 = swin_D1.transpose(1, 2).view(-1, 192*2, 64, 64)    # Reshape for Conv2d\n","\n","        # Further Convolutions\n","        conv6 = self.conv11(swin_D1)        # (B, 48, 64, 64)\n","        conv6 = self.conv12(conv6)          # (B, 48, 64, 64)\n","\n","        # Second Upsampling Block\n","        up2 = self.up2(conv6)               # (B, 48, 128, 128)\n","        up2 = self.bn_up2(up2)\n","        up2 = self.relu_up2(up2)\n","\n","        # Prepare for BidirectionalConvLSTM2D\n","        up2_seq = torch.stack([x2, up2], dim=1)  # (B, 2, 48, 128, 128)\n","        bidir_convLSTM2_out = self.bidirectional_convLSTM2(up2_seq)  # (B, 96*2, 128, 128)\n","\n","        # Swin Transformer Block in Decoder\n","        bidir_convLSTM2_flat = bidir_convLSTM2_out.flatten(2).transpose(1, 2)  # (B, 128*128, 96*2)\n","        swin_D2 = self.swin_unet_D2(bidir_convLSTM2_flat)               # (B, 128*128, 96*2)\n","        swin_D2 = swin_D2.transpose(1, 2).view(-1, 96*2, 128, 128)    # Reshape for Conv2d\n","\n","        # Further Convolutions\n","        conv7 = self.conv13(swin_D2)        # (B, 24, 128, 128)\n","        conv7 = self.conv14(conv7)          # (B, 24, 128, 128)\n","\n","        # Third Upsampling Block\n","        up3 = self.up3(conv7)               # (B, 24, 256, 256)\n","        up3 = self.bn_up3(up3)\n","        up3 = self.relu_up3(up3)\n","\n","        # Prepare for BidirectionalConvLSTM2D\n","        up3_seq = torch.stack([x1, up3], dim=1)  # (B, 2, 24, 256, 256)\n","        bidir_convLSTM3_out = self.bidirectional_convLSTM3(up3_seq)  # (B, 48*2, 256, 256)\n","\n","        # Swin Transformer Block in Decoder\n","        bidir_convLSTM3_flat = bidir_convLSTM3_out.flatten(2).transpose(1, 2)  # (B, 256*256, 48*2)\n","        swin_D3 = self.swin_unet_D3(bidir_convLSTM3_flat)               # (B, 256*256, 48*2)\n","        swin_D3 = swin_D3.transpose(1, 2).view(-1, 48*2, 256, 256)    # Reshape for Conv2d\n","\n","        # Further Convolutions\n","        conv8 = self.conv15(swin_D3)        # (B, 24, 256, 256)\n","        conv8 = self.conv16(conv8)          # (B, 24, 256, 256)\n","\n","        # Final Output Convolutions\n","        final = self.final_conv1(conv8)      # (B, 2, 256, 256)\n","        final = self.final_relu(final)\n","        final = self.final_conv2(final)      # (B, 1, 256, 256)\n","        final = self.final_sigmoid(final)    # (B, 1, 256, 256)\n","\n","        return final\n","\n","# ================================== Dataset Class ======================================\n","\n","class SegmentationDataset(Dataset):\n","    \"\"\"\n","    Custom Dataset for image segmentation tasks.\n","    Expects images in 'x' folder and masks in 'y' folder.\n","    \"\"\"\n","    def __init__(self, images_dir, masks_dir, transform=None):\n","        super(SegmentationDataset, self).__init__()\n","        self.images_dir = images_dir\n","        self.masks_dir = masks_dir\n","        self.transform = transform\n","\n","        self.images = sorted(os.listdir(images_dir))\n","        self.masks = sorted(os.listdir(masks_dir))\n","\n","    def __len__(self):\n","        return len(self.images)\n","\n","    def __getitem__(self, idx):\n","        # Load image\n","        img_path = os.path.join(self.images_dir, self.images[idx])\n","        image = Image.open(img_path).convert('RGB')  # Ensure RGB\n","\n","        # Load mask\n","        mask_path = os.path.join(self.masks_dir, self.masks[idx])\n","        mask = Image.open(mask_path).convert('L')    # Grayscale\n","\n","        # Apply transformations\n","        if self.transform:\n","            image = self.transform(image)\n","            mask = self.transform(mask)\n","\n","        return image, mask\n","\n","# =============================== Data Loading and Preprocessing ========================\n","\n","# Define image dimensions\n","im_height = 256\n","im_width = 256\n","\n","# Define transformations\n","transform = transforms.Compose([\n","    transforms.Resize((im_height, im_width)),\n","    transforms.ToTensor(),  # Converts to [0,1]\n","])\n","\n","# Paths to the dataset (update these paths as per your directory structure)\n","train_images_dir = r'/content/drive/MyDrive/ML/dataset/ISIC2018_Task1-2_Training_Input'\n","train_masks_dir = r'/content/drive/MyDrive/ML/dataset/ISIC2018_Task1_Training_GroundTruth'\n","test_images_dir = r'/content/drive/MyDrive/ML/dataset/ISIC2018_Task1-2_Test_Input'\n","test_masks_dir = r'/content/drive/MyDrive/ML/dataset/ISIC2018_Task1_Test_GroundTruth'\n","\n","# Create datasets\n","train_dataset = SegmentationDataset(train_images_dir, train_masks_dir, transform=transform)\n","test_dataset = SegmentationDataset(test_images_dir, test_masks_dir, transform=transform)\n","\n","# Split training data into training and validation sets (80-20 split)\n","train_size = int(0.8 * len(train_dataset))\n","valid_size = len(train_dataset) - train_size\n","train_subset, valid_subset = torch.utils.data.random_split(train_dataset, [train_size, valid_size])\n","\n","# Create DataLoaders\n","batch_size = 5\n","\n","train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True, num_workers=4)\n","valid_loader = DataLoader(valid_subset, batch_size=batch_size, shuffle=False, num_workers=4)\n","test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n","\n","# =============================== Training Setup ==========================================\n","\n","# Instantiate the model\n","model = SwinUNet(input_channels=3, output_channels=1, embed_dim=32, num_heads=[4, 8], window_size=4, mlp_ratio=4., depth=2)\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","model = model.to(device)  # Move to GPU if available\n","\n","# Initialize weights using Kaiming Normal initialization\n","def initialize_weights(module):\n","    if isinstance(module, (nn.Conv2d, nn.ConvTranspose2d)):\n","        nn.init.kaiming_normal_(module.weight, mode='fan_in', nonlinearity='relu')\n","        if module.bias is not None:\n","            nn.init.zeros_(module.bias)\n","    elif isinstance(module, nn.Linear):\n","        nn.init.kaiming_normal_(module.weight, mode='fan_in', nonlinearity='relu')\n","        if module.bias is not None:\n","            nn.init.zeros_(module.bias)\n","    elif isinstance(module, nn.Conv3d):\n","        nn.init.kaiming_normal_(module.weight, mode='fan_in', nonlinearity='relu')\n","        if module.bias is not None:\n","            nn.init.zeros_(module.bias)\n","\n","model.apply(initialize_weights)\n","\n","# Define loss function and optimizer\n","criterion = DiceLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n","\n","# Define learning rate scheduler and early stopping parameters\n","scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.25, patience=5, verbose=True, min_lr=1e-9)\n","early_stopping_patience = 9\n","best_val_loss = np.inf\n","epochs_no_improve = 0\n","\n","# =============================== Training Loop ===========================================\n","\n","num_epochs = 50  # You can adjust the number of epochs\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","for epoch in range(num_epochs):\n","    print(f\"Epoch {epoch+1}/{num_epochs}\")\n","    model.train()\n","    running_loss = 0.0\n","    for images, masks in train_loader:\n","        images = images.to(device)  # (B, 3, 256, 256)\n","        masks = masks.to(device)    # (B, 1, 256, 256)\n","\n","        optimizer.zero_grad()\n","        outputs = model(images)      # (B, 1, 256, 256)\n","        loss = criterion(outputs, masks)\n","        loss.backward()\n","        optimizer.step()\n","\n","        running_loss += loss.item() * images.size(0)\n","\n","    epoch_loss = running_loss / len(train_loader.dataset)\n","\n","    # Validation\n","    model.eval()\n","    val_loss = 0.0\n","    with torch.no_grad():\n","        for images, masks in valid_loader:\n","            images = images.to(device)\n","            masks = masks.to(device)\n","\n","            outputs = model(images)\n","            loss = criterion(outputs, masks)\n","\n","            val_loss += loss.item() * images.size(0)\n","\n","    val_loss /= len(valid_loader.dataset)\n","\n","    print(f'Epoch {epoch+1}/{num_epochs}, Training Loss: {epoch_loss:.4f}, Validation Loss: {val_loss:.4f}')\n","\n","    # Scheduler step\n","    scheduler.step(val_loss)\n","\n","    # Early stopping\n","    if val_loss < best_val_loss:\n","        best_val_loss = val_loss\n","        epochs_no_improve = 0\n","        # Save the best model\n","        torch.save(model.state_dict(), r'/content/drive/MyDrive/model/modelWeights_Swin_Trans_Weights_Swin_Trans_Leather.pth')\n","    else:\n","        epochs_no_improve += 1\n","        if epochs_no_improve >= early_stopping_patience:\n","            print('Early stopping!')\n","            break\n","\n","# ================================== Prediction ==========================================\n","\n","# Load the best model weights\n","model.load_state_dict(torch.load(r'/content/drive/MyDrive/model/modelWeights_Swin_Trans_Weights_Swin_Trans_Leather.pth'))\n","model.eval()\n","\n","# Function to save predictions and ground truth\n","def save_predictions(model, dataloader, save_dir_pred, save_dir_gt, device):\n","    \"\"\"\n","    Saves the predicted masks and ground truth masks.\n","    Args:\n","        model (nn.Module): Trained model.\n","        dataloader (DataLoader): DataLoader for test data.\n","        save_dir_pred (str): Directory to save predicted masks.\n","        save_dir_gt (str): Directory to save ground truth masks.\n","        device (str): Device to run the model on.\n","    \"\"\"\n","    os.makedirs(save_dir_pred, exist_ok=True)\n","    os.makedirs(save_dir_gt, exist_ok=True)\n","\n","    with torch.no_grad():\n","        for i, (images, masks) in enumerate(dataloader):\n","            if (i % 100 == 0):\n","                print(f\"{i}th Test Image\")  # There are total 1000 test images.\n","\n","            images = images.to(device)\n","            masks = masks.to(device)\n","\n","            outputs = model(images)\n","            preds = outputs.cpu().numpy()\n","            gts = masks.cpu().numpy()\n","\n","            for j in range(preds.shape[0]):\n","                pred_mask = preds[j, 0, :, :]\n","                gt_mask = gts[j, 0, :, :]\n","\n","                # Save predicted mask\n","                pred_img = Image.fromarray((pred_mask * 255).astype(np.uint8))\n","                pred_img.save(os.path.join(save_dir_pred, f\"{i * dataloader.batch_size + j + 1}.png\"))\n","\n","                # Save ground truth mask\n","                gt_img = Image.fromarray((gt_mask * 255).astype(np.uint8))\n","                gt_img.save(os.path.join(save_dir_gt, f\"{i * dataloader.batch_size + j + 1}.tiff\"))\n","\n","# Define directories to save predictions and ground truth\n","save_dir_pred = r'/content/drive/MyDrive/output/segmented predicted images'\n","save_dir_gt = r'/content/drive/MyDrive/output/segmented ground truth'\n","\n","# Save predictions\n","save_predictions(model, test_loader, save_dir_pred, save_dir_gt, device)\n","\n","# =================================== Evaluation =========================================\n","\n","from sklearn.metrics import confusion_matrix, precision_recall_curve\n","import numpy as np\n","import torch\n","\n","def evaluate_metrics_pytorch(model, dataloader, device):\n","    \"\"\"\n","    Evaluates various metrics for segmentation performance.\n","    Args:\n","        model (nn.Module): Trained model.\n","        dataloader (DataLoader): DataLoader for test data.\n","        device (str): Device to run the model on.\n","    Returns:\n","        dict: Dictionary containing average metrics.\n","    \"\"\"\n","    model.eval()\n","    all_accuracy = []\n","    all_dice = []\n","    all_jaccard = []\n","    all_sensitivity = []\n","    all_specificity = []\n","\n","    with torch.no_grad():\n","        for images, masks in dataloader:\n","            images = images.to(device)\n","            masks = masks.to(device)\n","\n","            outputs = model(images)\n","            preds = outputs > 0.5  # Binary mask\n","\n","            preds = preds.cpu().numpy().astype(np.uint8)\n","            masks = masks.cpu().numpy().astype(np.uint8)\n","            masks = (masks > 0).astype(np.uint8)  # Convert 255 to 1\n","\n","            for pred, mask in zip(preds, masks):\n","                pred_flat = pred.flatten()\n","                mask_flat = mask.flatten()\n","\n","                # Precision-Recall Curve to find optimal threshold\n","                precisions, recalls, thresholds = precision_recall_curve(mask_flat, pred_flat)\n","                f1 = 2 * (precisions * recalls) / (precisions + recalls + 1e-8)\n","                max_idx = np.argmax(f1)\n","                optimal_thresh = thresholds[max_idx] if max_idx < len(thresholds) else 0.5\n","\n","                # Apply optimal threshold\n","                pred_opt = (pred_flat >= optimal_thresh).astype(np.uint8)\n","\n","                # Confusion matrix with specified labels\n","                cm = confusion_matrix(mask_flat, pred_opt, labels=[0,1])\n","\n","                # Unpack confusion matrix\n","                tn, fp, fn, tp = cm.ravel()\n","\n","                # Calculate metrics\n","                accuracy = (tp + tn) / (tp + tn + fp + fn + 1e-8)\n","                iou = tp / (tp + fp + fn + 1e-8)\n","                dice = (2 * tp) / (2 * tp + fp + fn + 1e-8)\n","                specificity = tn / (tn + fp + 1e-8)\n","                sensitivity = tp / (tp + fn + 1e-8)\n","\n","                all_accuracy.append(accuracy)\n","                all_jaccard.append(iou)\n","                all_dice.append(dice)\n","                all_specificity.append(specificity)\n","                all_sensitivity.append(sensitivity)\n","\n","    # Compute average metrics\n","    metrics = {\n","        'Accuracy': np.mean(all_accuracy),\n","        'Dice': np.mean(all_dice),\n","        'Jaccard': np.mean(all_jaccard),\n","        'Sensitivity': np.mean(all_sensitivity),\n","        'Specificity': np.mean(all_specificity)\n","    }\n","\n","    print(f\"Accuracy: {metrics['Accuracy']:.4f}, Dice: {metrics['Dice']:.4f}, Jaccard: {metrics['Jaccard']:.4f}, \"\n","          f\"Sensitivity: {metrics['Sensitivity']:.4f}, Specificity: {metrics['Specificity']:.4f}\")\n","\n","    return metrics\n","\n","# Evaluate the model\n","metrics = evaluate_metrics_pytorch(model, test_loader, device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jl-MfBPqF2gz"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t6bGr9s6F2eQ"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K1U6hinKF2cY"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GH6UQBRlF2XI"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r9jAocYfF2UM"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P3YPZryQF2Ra"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qk3C6Xi1F2OR"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q4wPms0cF2LI"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yaBzNHvSF2Hb"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"ugAHfwbPXbfu"},"source":["BIDIRECTIONAL CONVLSTM WITH MORE EVALUATION METRICS"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gKXumEbnDQfp","executionInfo":{"status":"ok","timestamp":1730368089553,"user_tz":-660,"elapsed":257177,"user":{"displayName":"ALOK THOTTAKATHU PRASANNAKUMAR","userId":"10483471326433970042"}},"outputId":"1178548e-bcfd-4d8f-c1e4-853eabb54648"},"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch 1/1\n","Train loader image count: 1\n","Train loader image count: 2\n","Train loader image count: 3\n","Train loader image count: 4\n","Train loader image count: 5\n","Train loader image count: 6\n","Train loader image count: 7\n","Train loader image count: 8\n","Train loader image count: 9\n","Train loader image count: 10\n","Train loader image count: 11\n","Train loader image count: 12\n","Train loader image count: 13\n","Train loader image count: 14\n","Train loader image count: 15\n","Train loader image count: 16\n","Train loader image count: 17\n","Train loader image count: 18\n","Train loader image count: 19\n","Train loader image count: 20\n","Train loader image count: 21\n","Train loader image count: 22\n","Train loader image count: 23\n","Train loader image count: 24\n","Epoch 1/1, Training Loss: 0.5967, Validation Loss: 0.6877\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["<ipython-input-3-38d69fde433c>:753: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  model.load_state_dict(torch.load(r'/content/drive/MyDrive/model/modelWeights_Swin_Trans_Weights_Swin_Trans_Leather.pth'))\n"]},{"output_type":"stream","name":"stdout","text":["0th Test Image\n","Evaluation Metrics:\n","Accuracy: 0.4796\n","Dice: 0.4065\n","Jaccard: 0.2813\n","Sensitivity: 0.7458\n","Specificity: 0.3983\n","Precision: 0.3595\n","Recall: 0.7458\n","F1-Score: 0.4065\n"]}],"source":["# ============================== Imports and Dependencies ==============================\n","\n","import os\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from PIL import Image\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import (\n","    precision_recall_curve, confusion_matrix,\n","    jaccard_score, f1_score, precision_score, recall_score\n",")\n","\n","# ================================ Separable Convolution =================================\n","\n","class SeparableConv2d(nn.Module):\n","    \"\"\"\n","    Implements a separable convolution layer using depthwise and pointwise convolutions.\n","    \"\"\"\n","    def __init__(self, in_channels, out_channels, kernel_size=3, padding=1, bias=True):\n","        super(SeparableConv2d, self).__init__()\n","        # Depthwise convolution (groups=in_channels)\n","        self.depthwise = nn.Conv2d(in_channels, in_channels, kernel_size=kernel_size,\n","                                   padding=padding, groups=in_channels, bias=bias)\n","        # Pointwise convolution\n","        self.pointwise = nn.Conv2d(in_channels, out_channels, kernel_size=1,\n","                                   padding=0, bias=bias)\n","\n","    def forward(self, x):\n","        x = self.depthwise(x)\n","        x = self.pointwise(x)\n","        return x\n","\n","# ================================== ConvLSTM2D ========================================\n","\n","class ConvLSTMCell(nn.Module):\n","    \"\"\"\n","    Implements a ConvLSTM cell.\n","    \"\"\"\n","    def __init__(self, input_channels, hidden_channels, kernel_size, bias=True):\n","        super(ConvLSTMCell, self).__init__()\n","\n","        padding = kernel_size // 2  # To maintain spatial dimensions\n","        self.input_channels = input_channels\n","        self.hidden_channels = hidden_channels\n","\n","        self.conv = nn.Conv2d(in_channels=input_channels + hidden_channels,\n","                              out_channels=4 * hidden_channels,\n","                              kernel_size=kernel_size,\n","                              padding=padding,\n","                              bias=bias)\n","\n","    def forward(self, input_tensor, cur_state):\n","        h_cur, c_cur = cur_state\n","\n","        # Concatenate input and hidden state\n","        combined = torch.cat([input_tensor, h_cur], dim=1)  # along channel axis\n","\n","        # Compute all gates at once\n","        conv_output = self.conv(combined)\n","        cc_i, cc_f, cc_o, cc_g = torch.split(conv_output, self.hidden_channels, dim=1)\n","\n","        i = torch.sigmoid(cc_i)   # input gate\n","        f = torch.sigmoid(cc_f)   # forget gate\n","        o = torch.sigmoid(cc_o)   # output gate\n","        g = torch.tanh(cc_g)      # gate gate\n","\n","        c_next = f * c_cur + i * g\n","        h_next = o * torch.tanh(c_next)\n","\n","        return h_next, c_next\n","\n","    def init_hidden(self, batch_size, spatial_size, device):\n","        height, width = spatial_size\n","        return (torch.zeros(batch_size, self.hidden_channels, height, width, device=device),\n","                torch.zeros(batch_size, self.hidden_channels, height, width, device=device))\n","\n","class ConvLSTM2D(nn.Module):\n","    \"\"\"\n","    Implements a ConvLSTM2D layer that processes a sequence of inputs.\n","    \"\"\"\n","    def __init__(self, input_channels, hidden_channels, kernel_size=3, bias=True, num_layers=1):\n","        super(ConvLSTM2D, self).__init__()\n","        self.num_layers = num_layers\n","        self.hidden_channels = hidden_channels\n","\n","        layers = []\n","        for i in range(num_layers):\n","            input_c = input_channels if i == 0 else hidden_channels\n","            layers.append(ConvLSTMCell(input_c, hidden_channels, kernel_size, bias))\n","        self.layers = nn.ModuleList(layers)\n","\n","    def forward(self, input_tensor, reverse=False):\n","        # input_tensor shape: (batch, seq_len, channels, height, width)\n","        batch_size, seq_len, channels, height, width = input_tensor.size()\n","        device = input_tensor.device\n","\n","        # Initialize hidden and cell states for all layers\n","        hidden_state = []\n","        cell_state = []\n","        for i in range(self.num_layers):\n","            h, c = self.layers[i].init_hidden(batch_size, (height, width), device)\n","            hidden_state.append(h)\n","            cell_state.append(c)\n","\n","        # Iterate over time steps\n","        if reverse:\n","            time_steps = reversed(range(seq_len))\n","        else:\n","            time_steps = range(seq_len)\n","\n","        outputs = []\n","        for t in time_steps:\n","            x = input_tensor[:, t, :, :, :]  # (batch, channels, height, width)\n","            for i, layer in enumerate(self.layers):\n","                h, c = layer(x, (hidden_state[i], cell_state[i]))\n","                hidden_state[i] = h\n","                cell_state[i] = c\n","                x = h  # input to next layer\n","            outputs.append(x)\n","\n","        outputs = torch.stack(outputs, dim=1)  # (batch, seq_len, channels, height, width)\n","        if reverse:\n","            outputs = outputs.flip(dims=[1])  # Reverse back to original order\n","        return outputs  # Return the sequence of outputs\n","\n","class BidirectionalConvLSTM2D(nn.Module):\n","    \"\"\"\n","    Implements a Bidirectional ConvLSTM2D layer.\n","    Processes the input sequence in both forward and backward directions and concatenates the outputs.\n","    \"\"\"\n","    def __init__(self, input_channels, hidden_channels, kernel_size=3, num_layers=1, bias=True):\n","        super(BidirectionalConvLSTM2D, self).__init__()\n","        self.forward_conv_lstm = ConvLSTM2D(input_channels, hidden_channels, kernel_size, bias=bias, num_layers=num_layers)\n","        self.backward_conv_lstm = ConvLSTM2D(input_channels, hidden_channels, kernel_size, bias=bias, num_layers=num_layers)\n","\n","    def forward(self, input_tensor):\n","        # input_tensor shape: (batch, seq_len, channels, height, width)\n","        # Forward direction\n","        forward_output = self.forward_conv_lstm(input_tensor, reverse=False)  # (batch, seq_len, hidden_channels, H, W)\n","        # Backward direction\n","        backward_output = self.backward_conv_lstm(input_tensor, reverse=True)  # (batch, seq_len, hidden_channels, H, W)\n","        # Concatenate outputs along the channel dimension\n","        output = torch.cat([forward_output, backward_output], dim=2)  # (batch, seq_len, hidden_channels*2, H, W)\n","        # Since seq_len=2, we can take the last output\n","        output = output[:, -1, :, :, :]  # Take the last output (batch, hidden_channels*2, H, W)\n","        return output\n","\n","# ============================== Swin Transformer Blocks ================================\n","\n","class WindowAttention(nn.Module):\n","    \"\"\"\n","    Window based multi-head self attention (W-MSA) module with relative position bias.\n","    \"\"\"\n","    def __init__(self, dim, window_size, num_heads, qkv_bias=True, attn_drop=0., proj_drop=0.):\n","        \"\"\"\n","        Args:\n","            dim (int): Number of input channels.\n","            window_size (tuple): Height and width of the window.\n","            num_heads (int): Number of attention heads.\n","            qkv_bias (bool): If True, add a learnable bias to query, key, value.\n","            attn_drop (float): Dropout ratio of attention weights.\n","            proj_drop (float): Dropout ratio of output.\n","        \"\"\"\n","        super(WindowAttention, self).__init__()\n","        self.dim = dim\n","        self.window_size = window_size  # Wh, Ww\n","        self.num_heads = num_heads\n","        head_dim = dim // num_heads\n","        self.scale = head_dim ** -0.5\n","\n","        # Define a parameter table of relative position bias\n","        self.relative_position_bias_table = nn.Parameter(\n","            torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads)\n","        )  # 2*Wh-1 * 2*Ww-1, nH\n","\n","        # Get pair-wise relative position index for each token inside the window\n","        coords_h = torch.arange(self.window_size[0])\n","        coords_w = torch.arange(self.window_size[1])\n","        coords = torch.stack(torch.meshgrid(coords_h, coords_w, indexing='ij'))  # 2, Wh, Ww\n","        coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n","        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n","        relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n","        relative_coords[:, :, 0] += self.window_size[0] - 1  # shift to start from 0\n","        relative_coords[:, :, 1] += self.window_size[1] - 1\n","        relative_coords[:, :, 0] *= (2 * self.window_size[1] - 1)\n","        relative_position_index = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n","        self.register_buffer(\"relative_position_index\", relative_position_index)\n","\n","        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)  # Query, Key, Value\n","        self.attn_drop = nn.Dropout(attn_drop)\n","        self.proj = nn.Linear(dim, dim)\n","        self.proj_drop = nn.Dropout(proj_drop)\n","\n","        # Initialize relative position bias table\n","        nn.init.trunc_normal_(self.relative_position_bias_table, std=.02)\n","\n","    def forward(self, x, mask=None):\n","        \"\"\"\n","        Args:\n","            x: input features with shape of (num_windows*B, Wh*Ww, C)\n","            mask: (num_windows, Wh*Ww, Wh*Ww) or None\n","        \"\"\"\n","        B_, N, C = x.shape\n","        qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)  # 3, B_, nH, N, C//nH\n","        q, k, v = qkv[0], qkv[1], qkv[2]  # each has shape (B_, nH, N, C//nH)\n","\n","        q = q * self.scale\n","        attn = (q @ k.transpose(-2, -1))  # (B_, nH, N, N)\n","\n","        # Add relative position bias\n","        relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(\n","            self.window_size[0] * self.window_size[1], self.window_size[0] * self.window_size[1], -1\n","        )  # Wh*Ww, Wh*Ww, nH\n","        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n","        attn = attn + relative_position_bias.unsqueeze(0)  # (B_, nH, N, N)\n","\n","        if mask is not None:\n","            nW = mask.shape[0]\n","            attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)\n","            attn = attn.view(-1, self.num_heads, N, N)\n","            attn = F.softmax(attn, dim=-1)\n","        else:\n","            attn = F.softmax(attn, dim=-1)\n","\n","        attn = self.attn_drop(attn)\n","\n","        out = (attn @ v).transpose(1, 2).reshape(B_, N, C)  # (B_, N, C)\n","        out = self.proj(out)\n","        out = self.proj_drop(out)\n","        return out\n","\n","class SwinTransformerBlock(nn.Module):\n","    \"\"\"\n","    Swin Transformer Block with W-MSA and SW-MSA.\n","    \"\"\"\n","    def __init__(self, dim, num_heads, window_size=7, shift_size=0, mlp_ratio=4., qkv_bias=True,\n","                 attn_drop=0., proj_drop=0.):\n","        super(SwinTransformerBlock, self).__init__()\n","        self.dim = dim\n","        self.num_heads = num_heads\n","        self.window_size = window_size  # W\n","        self.shift_size = shift_size    # S\n","        self.mlp_ratio = mlp_ratio\n","\n","        assert 0 <= self.shift_size < self.window_size, \"shift_size must be in [0, window_size)\"\n","\n","        self.norm1 = nn.LayerNorm(dim)\n","        self.attn = WindowAttention(dim, (window_size, window_size), num_heads, qkv_bias, attn_drop, proj_drop)\n","\n","        self.drop_path = nn.Identity()  # Can implement stochastic depth if desired\n","        self.norm2 = nn.LayerNorm(dim)\n","        mlp_hidden_dim = int(dim * mlp_ratio)\n","        self.mlp = nn.Sequential(\n","            nn.Linear(dim, mlp_hidden_dim),\n","            nn.GELU(),\n","            nn.Linear(mlp_hidden_dim, dim),\n","            nn.Dropout(proj_drop)\n","        )\n","\n","    def forward(self, x):\n","        \"\"\"\n","        Args:\n","            x: input features with shape (B, H*W, C)\n","        \"\"\"\n","        H = W = int(np.sqrt(x.shape[1]))\n","        B, L, C = x.shape\n","        assert L == H * W, \"Input feature has wrong size\"\n","\n","        shortcut = x\n","        x = self.norm1(x)\n","        x = x.view(B, H, W, C)\n","\n","        # Cyclic shift\n","        if self.shift_size > 0:\n","            shifted_x = torch.roll(x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2))\n","        else:\n","            shifted_x = x\n","\n","        # Partition windows\n","        window_size = self.window_size\n","        # Pad H and W to be multiples of window_size\n","        pad_b = (window_size - H % window_size) % window_size\n","        pad_r = (window_size - W % window_size) % window_size\n","        shifted_x = F.pad(shifted_x, (0, 0, 0, pad_r, 0, pad_b))  # pad H and W\n","        _, Hp, Wp, _ = shifted_x.shape\n","\n","        # Window partition\n","        x_windows = shifted_x.view(B, Hp // window_size, window_size, Wp // window_size, window_size, C)\n","        x_windows = x_windows.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size * window_size, C)  # (num_windows*B, window_size*window_size, C)\n","\n","        # Attention\n","        attn_windows = self.attn(x_windows)  # (num_windows*B, window_size*window_size, C)\n","\n","        # Merge windows\n","        shifted_x = attn_windows.view(-1, window_size, window_size, C)\n","        shifted_x = shifted_x.view(B, Hp // window_size, Wp // window_size, window_size, window_size, C)\n","        shifted_x = shifted_x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, Hp, Wp, C)\n","\n","        # Reverse cyclic shift\n","        if self.shift_size > 0:\n","            x = torch.roll(shifted_x, shifts=(self.shift_size, self.shift_size), dims=(1, 2))\n","        else:\n","            x = shifted_x\n","\n","        # Remove padding\n","        x = x[:, :H, :W, :].contiguous().view(B, H * W, C)\n","\n","        # FFN\n","        x = shortcut + self.drop_path(x)\n","        x = x + self.drop_path(self.mlp(self.norm2(x)))\n","\n","        return x\n","\n","# =============================== Dice Loss Function ====================================\n","\n","class DiceLoss(nn.Module):\n","    \"\"\"\n","    Dice Loss function to maximize the Dice coefficient.\n","    Suitable for binary segmentation tasks.\n","    \"\"\"\n","    def __init__(self, smooth=1.0):\n","        super(DiceLoss, self).__init__()\n","        self.smooth = smooth\n","\n","    def forward(self, y_pred, y_true):\n","        \"\"\"\n","        Args:\n","            y_pred (torch.Tensor): Predicted mask probabilities with shape (B, 1, H, W)\n","            y_true (torch.Tensor): Ground truth masks with shape (B, 1, H, W)\n","        Returns:\n","            torch.Tensor: Dice loss\n","        \"\"\"\n","        y_pred = y_pred.view(-1)\n","        y_true = y_true.view(-1)\n","\n","        intersection = (y_pred * y_true).sum()\n","        dice = (2. * intersection + self.smooth) / (y_pred.sum() + y_true.sum() + self.smooth)\n","\n","        return 1 - dice\n","\n","# ================================ Main Model ============================================\n","\n","class SwinUNet(nn.Module):\n","    \"\"\"\n","    Swin U-Net architecture for image segmentation with bidirectional ConvLSTM layers.\n","    \"\"\"\n","    def __init__(self, input_channels=3, output_channels=1,\n","                 embed_dim=32, num_heads=[4, 8], window_size=4,\n","                 mlp_ratio=4., depth=2):\n","        super(SwinUNet, self).__init__()\n","        self.input_channels = input_channels\n","        self.output_channels = output_channels\n","\n","        # Initial convolutional layers\n","        self.conv1 = SeparableConv2d(input_channels, 24, kernel_size=3, padding=1)\n","        self.bn1 = nn.BatchNorm2d(24)\n","        self.conv2 = SeparableConv2d(24, 24, kernel_size=3, padding=1)\n","        self.bn2 = nn.BatchNorm2d(24)\n","        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)  # 256x256 -> 128x128\n","\n","        # First Swin Transformer Block\n","        self.swin_unet_E1 = SwinTransformerBlock(\n","            dim=24,  # Changed from embed_dim=32 to 24\n","            num_heads=num_heads[0],\n","            window_size=window_size,\n","            shift_size=window_size//2 if True else 0,\n","            mlp_ratio=mlp_ratio\n","        )\n","\n","        # Second convolutional block\n","        self.conv3 = SeparableConv2d(24, 48, kernel_size=3, padding=1)\n","        self.bn3 = nn.BatchNorm2d(48)\n","        self.conv4 = SeparableConv2d(48, 48, kernel_size=3, padding=1)\n","        self.bn4 = nn.BatchNorm2d(48)\n","        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)  # 128x128 -> 64x64\n","\n","        # Second Swin Transformer Block\n","        self.swin_unet_E2 = SwinTransformerBlock(\n","            dim=48,\n","            num_heads=num_heads[1],\n","            window_size=window_size,\n","            shift_size=window_size//2 if True else 0,\n","            mlp_ratio=mlp_ratio\n","        )\n","\n","        # Third convolutional block (Bottleneck)\n","        self.conv5 = SeparableConv2d(48, 96, kernel_size=3, padding=1)\n","        self.bn5 = nn.BatchNorm2d(96)\n","        self.conv6 = SeparableConv2d(96, 96, kernel_size=3, padding=1)\n","        self.bn6 = nn.BatchNorm2d(96)\n","        self.drop5 = nn.Dropout(0.5)\n","        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)  # 64x64 -> 32x32\n","\n","        # Bottleneck convolutions with dense connections\n","        self.conv7 = SeparableConv2d(96, 192, kernel_size=3, padding=1)\n","        self.bn7 = nn.BatchNorm2d(192)\n","        self.conv8 = SeparableConv2d(192, 192, kernel_size=3, padding=1)\n","        self.bn8 = nn.BatchNorm2d(192)\n","        self.drop6_1 = nn.Dropout(0.5)\n","\n","        self.conv9 = SeparableConv2d(192, 192, kernel_size=3, padding=1)\n","        self.bn9 = nn.BatchNorm2d(192)\n","        self.conv10 = SeparableConv2d(192, 192, kernel_size=3, padding=1)\n","        self.bn10 = nn.BatchNorm2d(192)\n","        self.drop6_2 = nn.Dropout(0.5)\n","\n","        self.concat1 = nn.Sequential(\n","            SeparableConv2d(384, 192, kernel_size=3, padding=1),\n","            SeparableConv2d(192, 192, kernel_size=3, padding=1)\n","        )\n","        self.drop6_3 = nn.Dropout(0.5)\n","\n","        # First Upsampling Block\n","        self.up1 = nn.ConvTranspose2d(192, 96, kernel_size=2, stride=2)  # 32x32 -> 64x64\n","        self.bn_up1 = nn.BatchNorm2d(96)\n","        self.relu_up1 = nn.ReLU(inplace=True)\n","        self.bidirectional_convLSTM1 = BidirectionalConvLSTM2D(input_channels=96, hidden_channels=192, kernel_size=3, num_layers=1)\n","        self.swin_unet_D1 = SwinTransformerBlock(\n","            dim=192 * 2,  # Adjusted for bidirectional output\n","            num_heads=num_heads[0],\n","            window_size=window_size,\n","            shift_size=window_size//2 if True else 0,\n","            mlp_ratio=mlp_ratio\n","        )\n","        self.conv11 = SeparableConv2d(192 * 2, 48, kernel_size=3, padding=1)\n","        self.conv12 = SeparableConv2d(48, 48, kernel_size=3, padding=1)\n","\n","        # Second Upsampling Block\n","        self.up2 = nn.ConvTranspose2d(48, 48, kernel_size=2, stride=2)  # 64x64 -> 128x128\n","        self.bn_up2 = nn.BatchNorm2d(48)\n","        self.relu_up2 = nn.ReLU(inplace=True)\n","        self.bidirectional_convLSTM2 = BidirectionalConvLSTM2D(input_channels=48, hidden_channels=96, kernel_size=3, num_layers=1)\n","        self.swin_unet_D2 = SwinTransformerBlock(\n","            dim=96 * 2,\n","            num_heads=num_heads[1],\n","            window_size=window_size,\n","            shift_size=window_size//2 if True else 0,\n","            mlp_ratio=mlp_ratio\n","        )\n","        self.conv13 = SeparableConv2d(96 * 2, 24, kernel_size=3, padding=1)\n","        self.conv14 = SeparableConv2d(24, 24, kernel_size=3, padding=1)\n","\n","        # Third Upsampling Block\n","        self.up3 = nn.ConvTranspose2d(24, 24, kernel_size=2, stride=2)  # 128x128 -> 256x256\n","        self.bn_up3 = nn.BatchNorm2d(24)\n","        self.relu_up3 = nn.ReLU(inplace=True)\n","        self.bidirectional_convLSTM3 = BidirectionalConvLSTM2D(input_channels=24, hidden_channels=48, kernel_size=3, num_layers=1)\n","        self.swin_unet_D3 = SwinTransformerBlock(\n","            dim=48 * 2,\n","            num_heads=num_heads[1],\n","            window_size=window_size,\n","            shift_size=window_size//2 if True else 0,\n","            mlp_ratio=mlp_ratio\n","        )\n","        self.conv15 = SeparableConv2d(48 * 2, 24, kernel_size=3, padding=1)\n","        self.conv16 = SeparableConv2d(24, 24, kernel_size=3, padding=1)\n","\n","        # Output Layer\n","        self.final_conv1 = nn.Conv2d(24, 2, kernel_size=3, padding=1)\n","        self.final_relu = nn.ReLU(inplace=True)\n","        self.final_conv2 = nn.Conv2d(2, 1, kernel_size=1, padding=0)\n","        self.final_sigmoid = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        \"\"\"\n","        Forward pass of the Swin U-Net model.\n","        Args:\n","            x: Input tensor with shape (B, 3, 256, 256)\n","        Returns:\n","            torch.Tensor: Output segmentation mask with shape (B, 1, 256, 256)\n","        \"\"\"\n","        # Initial Convolutions\n","        x1 = self.conv1(x)          # (B, 24, 256, 256)\n","        x1 = self.bn1(x1)\n","        x1 = self.conv2(x1)         # (B, 24, 256, 256)\n","        x1 = self.bn2(x1)\n","        p1 = self.pool1(x1)         # (B, 24, 128, 128)\n","\n","        # First Swin Transformer Block\n","        p1_flat = p1.flatten(2).transpose(1, 2)  # (B, 128*128, 24)\n","        swin_E1 = self.swin_unet_E1(p1_flat)     # (B, 128*128, 24)\n","        swin_E1 = swin_E1.transpose(1, 2).view(-1, 24, 128, 128)  # Reshape for Conv2d\n","\n","        # Second Convolutional Block\n","        x2 = self.conv3(swin_E1)    # (B, 48, 128, 128)\n","        x2 = self.bn3(x2)\n","        x2 = self.conv4(x2)          # (B, 48, 128, 128)\n","        x2 = self.bn4(x2)\n","        p2 = self.pool2(x2)          # (B, 48, 64, 64)\n","\n","        # Second Swin Transformer Block\n","        p2_flat = p2.flatten(2).transpose(1, 2)  # (B, 64*64, 48)\n","        swin_E2 = self.swin_unet_E2(p2_flat)     # (B, 64*64, 48)\n","        swin_E2 = swin_E2.transpose(1, 2).view(-1, 48, 64, 64)  # Reshape for Conv2d\n","\n","        # Third Convolutional Block (Bottleneck)\n","        x3 = self.conv5(swin_E2)    # (B, 96, 64, 64)\n","        x3 = self.bn5(x3)\n","        x3 = self.conv6(x3)          # (B, 96, 64, 64)\n","        x3 = self.bn6(x3)\n","        x3 = self.drop5(x3)\n","        p3 = self.pool3(x3)          # (B, 96, 32, 32)\n","\n","        # Bottleneck Convolutions with Dense Connections\n","        x4 = self.conv7(p3)          # (B, 192, 32, 32)\n","        x4 = self.bn7(x4)\n","        x4 = self.conv8(x4)          # (B, 192, 32, 32)\n","        x4 = self.bn8(x4)\n","        x4 = self.drop6_1(x4)\n","\n","        x5 = self.conv9(x4)          # (B, 192, 32, 32)\n","        x5 = self.bn9(x5)\n","        x5 = self.conv10(x5)         # (B, 192, 32, 32)\n","        x5 = self.bn10(x5)\n","        x5 = self.drop6_2(x5)\n","\n","        concat = torch.cat([x5, x4], dim=1)  # (B, 384, 32, 32)\n","        concat = self.concat1(concat)         # (B, 192, 32, 32)\n","        concat = self.drop6_3(concat)         # (B, 192, 32, 32)\n","\n","        # First Upsampling Block\n","        up1 = self.up1(concat)                 # (B, 96, 64, 64)\n","        up1 = self.bn_up1(up1)\n","        up1 = self.relu_up1(up1)\n","\n","        # Prepare for BidirectionalConvLSTM2D\n","        up1_seq = torch.stack([x3, up1], dim=1)  # (B, 2, 96, 64, 64)\n","        bidir_convLSTM1_out = self.bidirectional_convLSTM1(up1_seq)  # (B, 192*2, 64, 64)\n","\n","        # Swin Transformer Block in Decoder\n","        bidir_convLSTM1_flat = bidir_convLSTM1_out.flatten(2).transpose(1, 2)  # (B, 64*64, 192*2)\n","        swin_D1 = self.swin_unet_D1(bidir_convLSTM1_flat)               # (B, 64*64, 192*2)\n","        swin_D1 = swin_D1.transpose(1, 2).view(-1, 192*2, 64, 64)    # Reshape for Conv2d\n","\n","        # Further Convolutions\n","        conv6 = self.conv11(swin_D1)        # (B, 48, 64, 64)\n","        conv6 = self.conv12(conv6)          # (B, 48, 64, 64)\n","\n","        # Second Upsampling Block\n","        up2 = self.up2(conv6)               # (B, 48, 128, 128)\n","        up2 = self.bn_up2(up2)\n","        up2 = self.relu_up2(up2)\n","\n","        # Prepare for BidirectionalConvLSTM2D\n","        up2_seq = torch.stack([x2, up2], dim=1)  # (B, 2, 48, 128, 128)\n","        bidir_convLSTM2_out = self.bidirectional_convLSTM2(up2_seq)  # (B, 96*2, 128, 128)\n","\n","        # Swin Transformer Block in Decoder\n","        bidir_convLSTM2_flat = bidir_convLSTM2_out.flatten(2).transpose(1, 2)  # (B, 128*128, 96*2)\n","        swin_D2 = self.swin_unet_D2(bidir_convLSTM2_flat)               # (B, 128*128, 96*2)\n","        swin_D2 = swin_D2.transpose(1, 2).view(-1, 96*2, 128, 128)    # Reshape for Conv2d\n","\n","        # Further Convolutions\n","        conv7 = self.conv13(swin_D2)        # (B, 24, 128, 128)\n","        conv7 = self.conv14(conv7)          # (B, 24, 128, 128)\n","\n","        # Third Upsampling Block\n","        up3 = self.up3(conv7)               # (B, 24, 256, 256)\n","        up3 = self.bn_up3(up3)\n","        up3 = self.relu_up3(up3)\n","\n","        # Prepare for BidirectionalConvLSTM2D\n","        up3_seq = torch.stack([x1, up3], dim=1)  # (B, 2, 24, 256, 256)\n","        bidir_convLSTM3_out = self.bidirectional_convLSTM3(up3_seq)  # (B, 48*2, 256, 256)\n","\n","        # Swin Transformer Block in Decoder\n","        bidir_convLSTM3_flat = bidir_convLSTM3_out.flatten(2).transpose(1, 2)  # (B, 256*256, 48*2)\n","        swin_D3 = self.swin_unet_D3(bidir_convLSTM3_flat)               # (B, 256*256, 48*2)\n","        swin_D3 = swin_D3.transpose(1, 2).view(-1, 48*2, 256, 256)    # Reshape for Conv2d\n","\n","        # Further Convolutions\n","        conv8 = self.conv15(swin_D3)        # (B, 24, 256, 256)\n","        conv8 = self.conv16(conv8)          # (B, 24, 256, 256)\n","\n","        # Final Output Convolutions\n","        final = self.final_conv1(conv8)      # (B, 2, 256, 256)\n","        final = self.final_relu(final)\n","        final = self.final_conv2(final)      # (B, 1, 256, 256)\n","        final = self.final_sigmoid(final)    # (B, 1, 256, 256)\n","\n","        return final\n","\n","# ================================== Dataset Class ======================================\n","\n","class SegmentationDataset(Dataset):\n","    \"\"\"\n","    Custom Dataset for image segmentation tasks.\n","    Expects images in 'x' folder and masks in 'y' folder.\n","    \"\"\"\n","    def __init__(self, images_dir, masks_dir, transform=None):\n","        super(SegmentationDataset, self).__init__()\n","        self.images_dir = images_dir\n","        self.masks_dir = masks_dir\n","        self.transform = transform\n","\n","        self.images = sorted(os.listdir(images_dir))\n","        self.masks = sorted(os.listdir(masks_dir))\n","\n","    def __len__(self):\n","        return len(self.images)\n","\n","    def __getitem__(self, idx):\n","        # Load image\n","        img_path = os.path.join(self.images_dir, self.images[idx])\n","        image = Image.open(img_path).convert('RGB')  # Ensure RGB\n","\n","        # Load mask\n","        mask_path = os.path.join(self.masks_dir, self.masks[idx])\n","        mask = Image.open(mask_path).convert('L')    # Grayscale\n","\n","        # Apply transformations\n","        if self.transform:\n","            image = self.transform(image)\n","            mask = self.transform(mask)\n","\n","        return image, mask\n","\n","# =============================== Data Loading and Preprocessing ========================\n","\n","# Define image dimensions\n","im_height = 256\n","im_width = 256\n","\n","# Define transformations\n","transform = transforms.Compose([\n","    transforms.Resize((im_height, im_width)),\n","    transforms.ToTensor(),  # Converts to [0,1]\n","])\n","\n","# Paths to the dataset (update these paths as per your directory structure)\n","train_images_dir = r'/content/drive/MyDrive/ML/dataset/ISIC2018_Task1-2_Training_Input'\n","train_masks_dir = r'/content/drive/MyDrive/ML/dataset/ISIC2018_Task1_Training_GroundTruth'\n","test_images_dir = r'/content/drive/MyDrive/ML/dataset/ISIC2018_Task1-2_Test_Input'\n","test_masks_dir = r'/content/drive/MyDrive/ML/dataset/ISIC2018_Task1_Test_GroundTruth'\n","\n","# Create datasets\n","train_dataset = SegmentationDataset(train_images_dir, train_masks_dir, transform=transform)\n","test_dataset = SegmentationDataset(test_images_dir, test_masks_dir, transform=transform)\n","\n","# Split training data into training and validation sets (80-20 split)\n","train_size = int(0.8 * len(train_dataset))\n","valid_size = len(train_dataset) - train_size\n","train_subset, valid_subset = torch.utils.data.random_split(train_dataset, [train_size, valid_size])\n","\n","# Create DataLoaders\n","batch_size = 5\n","\n","train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True, num_workers=4)\n","valid_loader = DataLoader(valid_subset, batch_size=batch_size, shuffle=False, num_workers=4)\n","test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n","\n","# =============================== Training Setup ==========================================\n","\n","# Instantiate the model\n","model = SwinUNet(input_channels=3, output_channels=1, embed_dim=32, num_heads=[4, 8], window_size=4, mlp_ratio=4., depth=2)\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","model = model.to(device)  # Move to GPU if available\n","\n","# Initialize weights using Kaiming Normal initialization\n","def initialize_weights(module):\n","    if isinstance(module, (nn.Conv2d, nn.ConvTranspose2d)):\n","        nn.init.kaiming_normal_(module.weight, mode='fan_in', nonlinearity='relu')\n","        if module.bias is not None:\n","            nn.init.zeros_(module.bias)\n","    elif isinstance(module, nn.Linear):\n","        nn.init.kaiming_normal_(module.weight, mode='fan_in', nonlinearity='relu')\n","        if module.bias is not None:\n","            nn.init.zeros_(module.bias)\n","    elif isinstance(module, nn.Conv3d):\n","        nn.init.kaiming_normal_(module.weight, mode='fan_in', nonlinearity='relu')\n","        if module.bias is not None:\n","            nn.init.zeros_(module.bias)\n","\n","model.apply(initialize_weights)\n","\n","# Define loss function and optimizer\n","criterion = DiceLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n","\n","# Define learning rate scheduler and early stopping parameters\n","scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.25, patience=5, verbose=True, min_lr=1e-9)\n","early_stopping_patience = 9\n","best_val_loss = np.inf\n","epochs_no_improve = 0\n","\n","# =============================== Training Loop ===========================================\n","\n","num_epochs = 1  # You can adjust the number of epochs\n","\n","for epoch in range(num_epochs):\n","    print(f\"Epoch {epoch+1}/{num_epochs}\")\n","    model.train()\n","    running_loss = 0.0\n","    train_loader_count = 0\n","    for images, masks in train_loader:\n","        train_loader_count += 1\n","        print(f\"Train loader image count: {train_loader_count}\")\n","        images = images.to(device)  # (B, 3, 256, 256)\n","        masks = masks.to(device)    # (B, 1, 256, 256)\n","\n","        optimizer.zero_grad()\n","        outputs = model(images)      # (B, 1, 256, 256)\n","        loss = criterion(outputs, masks)\n","        loss.backward()\n","        optimizer.step()\n","\n","        running_loss += loss.item() * images.size(0)\n","\n","    epoch_loss = running_loss / len(train_loader.dataset)\n","\n","    # Validation\n","    model.eval()\n","    val_loss = 0.0\n","    with torch.no_grad():\n","        for images, masks in valid_loader:\n","            images = images.to(device)\n","            masks = masks.to(device)\n","\n","            outputs = model(images)\n","            loss = criterion(outputs, masks)\n","\n","            val_loss += loss.item() * images.size(0)\n","\n","    val_loss /= len(valid_loader.dataset)\n","\n","    print(f'Epoch {epoch+1}/{num_epochs}, Training Loss: {epoch_loss:.4f}, Validation Loss: {val_loss:.4f}')\n","\n","    # Scheduler step\n","    scheduler.step(val_loss)\n","\n","    # Early stopping\n","    if val_loss < best_val_loss:\n","        best_val_loss = val_loss\n","        epochs_no_improve = 0\n","        # Save the best model\n","        torch.save(model.state_dict(), r'/content/drive/MyDrive/model/modelWeights_Swin_Trans_Weights_Swin_Trans_Leather.pth')\n","    else:\n","        epochs_no_improve += 1\n","        if epochs_no_improve >= early_stopping_patience:\n","            print('Early stopping!')\n","            break\n","\n","# ================================== Prediction ==========================================\n","\n","# Load the best model weights\n","model.load_state_dict(torch.load(r'/content/drive/MyDrive/model/modelWeights_Swin_Trans_Weights_Swin_Trans_Leather.pth'))\n","model.eval()\n","\n","# Function to save predictions and ground truth\n","def save_predictions(model, dataloader, save_dir_pred, save_dir_gt, device):\n","    \"\"\"\n","    Saves the predicted masks and ground truth masks.\n","    Args:\n","        model (nn.Module): Trained model.\n","        dataloader (DataLoader): DataLoader for test data.\n","        save_dir_pred (str): Directory to save predicted masks.\n","        save_dir_gt (str): Directory to save ground truth masks.\n","        device (str): Device to run the model on.\n","    \"\"\"\n","    os.makedirs(save_dir_pred, exist_ok=True)\n","    os.makedirs(save_dir_gt, exist_ok=True)\n","\n","    with torch.no_grad():\n","        for i, (images, masks) in enumerate(dataloader):\n","            if (i % 100 == 0):\n","                print(f\"{i}th Test Image\")  # Adjust as per your dataset\n","\n","            images = images.to(device)\n","            masks = masks.to(device)\n","\n","            outputs = model(images)\n","            preds = outputs.cpu().numpy()\n","            gts = masks.cpu().numpy()\n","\n","            for j in range(preds.shape[0]):\n","                pred_mask = preds[j, 0, :, :]\n","                gt_mask = gts[j, 0, :, :]\n","\n","                # Save predicted mask\n","                pred_img = Image.fromarray((pred_mask * 255).astype(np.uint8))\n","                pred_img.save(os.path.join(save_dir_pred, f\"{i * dataloader.batch_size + j + 1}.png\"))\n","\n","                # Save ground truth mask\n","                gt_img = Image.fromarray((gt_mask * 255).astype(np.uint8))\n","                gt_img.save(os.path.join(save_dir_gt, f\"{i * dataloader.batch_size + j + 1}.tiff\"))\n","\n","# Define directories to save predictions and ground truth\n","save_dir_pred = r'/content/drive/MyDrive/output/segmented predicted images'\n","save_dir_gt = r'/content/drive/MyDrive/output/segmented ground truth'\n","\n","# Save predictions\n","save_predictions(model, test_loader, save_dir_pred, save_dir_gt, device)\n","\n","# =================================== Evaluation =========================================\n","\n","def evaluate_metrics_pytorch(model, dataloader, device):\n","    \"\"\"\n","    Evaluates various metrics for segmentation performance.\n","    Args:\n","        model (nn.Module): Trained model.\n","        dataloader (DataLoader): DataLoader for test data.\n","        device (str): Device to run the model on.\n","    Returns:\n","        dict: Dictionary containing average metrics.\n","    \"\"\"\n","    model.eval()\n","    all_metrics = {\n","        'Accuracy': [],\n","        'Dice': [],\n","        'Jaccard': [],\n","        'Sensitivity': [],\n","        'Specificity': [],\n","        'Precision': [],\n","        'Recall': [],\n","        'F1-Score': []\n","    }\n","\n","    with torch.no_grad():\n","        for images, masks in dataloader:\n","            images = images.to(device)\n","            masks = masks.to(device)\n","\n","            outputs = model(images)\n","            preds = outputs > 0.5  # Binary mask\n","\n","            preds = preds.cpu().numpy().astype(np.uint8)\n","            masks = masks.cpu().numpy().astype(np.uint8)\n","            masks = (masks > 0).astype(np.uint8)  # Convert to binary masks\n","\n","            for pred, mask in zip(preds, masks):\n","                pred_flat = pred.flatten()\n","                mask_flat = mask.flatten()\n","\n","                # Calculate metrics\n","                tn, fp, fn, tp = confusion_matrix(mask_flat, pred_flat, labels=[0,1]).ravel()\n","\n","                accuracy = (tp + tn) / (tp + tn + fp + fn + 1e-8)\n","                iou = jaccard_score(mask_flat, pred_flat, zero_division=0)\n","                dice = f1_score(mask_flat, pred_flat, zero_division=0)\n","                specificity = tn / (tn + fp + 1e-8)\n","                sensitivity = recall_score(mask_flat, pred_flat, zero_division=0)\n","                precision = precision_score(mask_flat, pred_flat, zero_division=0)\n","                recall = sensitivity\n","                f1 = dice  # F1-Score is the same as Dice coefficient for binary classification\n","\n","                all_metrics['Accuracy'].append(accuracy)\n","                all_metrics['Jaccard'].append(iou)\n","                all_metrics['Dice'].append(dice)\n","                all_metrics['Specificity'].append(specificity)\n","                all_metrics['Sensitivity'].append(sensitivity)\n","                all_metrics['Precision'].append(precision)\n","                all_metrics['Recall'].append(recall)\n","                all_metrics['F1-Score'].append(f1)\n","\n","    # Compute average metrics\n","    avg_metrics = {metric: np.mean(values) for metric, values in all_metrics.items()}\n","\n","    print(\"Evaluation Metrics:\")\n","    for metric, value in avg_metrics.items():\n","        print(f\"{metric}: {value:.4f}\")\n","\n","    return avg_metrics\n","\n","# Evaluate the model\n","metrics = evaluate_metrics_pytorch(model, test_loader, device)\n"]},{"cell_type":"code","source":["# ============================== Imports and Dependencies ==============================\n","\n","import os\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from PIL import Image\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import (\n","    precision_recall_curve, confusion_matrix,\n","    jaccard_score, f1_score, precision_score, recall_score\n",")\n","\n","# ================================ Separable Convolution =================================\n","\n","class SeparableConv2d(nn.Module):\n","    \"\"\"\n","    Implements a separable convolution layer using depthwise and pointwise convolutions.\n","    \"\"\"\n","    def __init__(self, in_channels, out_channels, kernel_size=3, padding=1, bias=True):\n","        super(SeparableConv2d, self).__init__()\n","        # Depthwise convolution (groups=in_channels)\n","        self.depthwise = nn.Conv2d(in_channels, in_channels, kernel_size=kernel_size,\n","                                   padding=padding, groups=in_channels, bias=bias)\n","        # Pointwise convolution\n","        self.pointwise = nn.Conv2d(in_channels, out_channels, kernel_size=1,\n","                                   padding=0, bias=bias)\n","\n","    def forward(self, x):\n","        x = self.depthwise(x)\n","        x = self.pointwise(x)\n","        return x\n","\n","# ================================== ConvLSTM2D ========================================\n","\n","class ConvLSTMCell(nn.Module):\n","    \"\"\"\n","    Implements a ConvLSTM cell.\n","    \"\"\"\n","    def __init__(self, input_channels, hidden_channels, kernel_size, bias=True):\n","        super(ConvLSTMCell, self).__init__()\n","\n","        padding = kernel_size // 2  # To maintain spatial dimensions\n","        self.input_channels = input_channels\n","        self.hidden_channels = hidden_channels\n","\n","        self.conv = nn.Conv2d(in_channels=input_channels + hidden_channels,\n","                              out_channels=4 * hidden_channels,\n","                              kernel_size=kernel_size,\n","                              padding=padding,\n","                              bias=bias)\n","\n","    def forward(self, input_tensor, cur_state):\n","        h_cur, c_cur = cur_state\n","\n","        # Concatenate input and hidden state\n","        combined = torch.cat([input_tensor, h_cur], dim=1)  # along channel axis\n","\n","        # Compute all gates at once\n","        conv_output = self.conv(combined)\n","        cc_i, cc_f, cc_o, cc_g = torch.split(conv_output, self.hidden_channels, dim=1)\n","\n","        i = torch.sigmoid(cc_i)   # input gate\n","        f = torch.sigmoid(cc_f)   # forget gate\n","        o = torch.sigmoid(cc_o)   # output gate\n","        g = torch.tanh(cc_g)      # gate gate\n","\n","        c_next = f * c_cur + i * g\n","        h_next = o * torch.tanh(c_next)\n","\n","        return h_next, c_next\n","\n","    def init_hidden(self, batch_size, spatial_size, device):\n","        height, width = spatial_size\n","        return (torch.zeros(batch_size, self.hidden_channels, height, width, device=device),\n","                torch.zeros(batch_size, self.hidden_channels, height, width, device=device))\n","\n","class ConvLSTM2D(nn.Module):\n","    \"\"\"\n","    Implements a ConvLSTM2D layer that processes a sequence of inputs.\n","    \"\"\"\n","    def __init__(self, input_channels, hidden_channels, kernel_size=3, bias=True, num_layers=1):\n","        super(ConvLSTM2D, self).__init__()\n","        self.num_layers = num_layers\n","        self.hidden_channels = hidden_channels\n","\n","        layers = []\n","        for i in range(num_layers):\n","            input_c = input_channels if i == 0 else hidden_channels\n","            layers.append(ConvLSTMCell(input_c, hidden_channels, kernel_size, bias))\n","        self.layers = nn.ModuleList(layers)\n","\n","    def forward(self, input_tensor, reverse=False):\n","        # input_tensor shape: (batch, seq_len, channels, height, width)\n","        batch_size, seq_len, channels, height, width = input_tensor.size()\n","        device = input_tensor.device\n","\n","        # Initialize hidden and cell states for all layers\n","        hidden_state = []\n","        cell_state = []\n","        for i in range(self.num_layers):\n","            h, c = self.layers[i].init_hidden(batch_size, (height, width), device)\n","            hidden_state.append(h)\n","            cell_state.append(c)\n","\n","        # Iterate over time steps\n","        if reverse:\n","            time_steps = reversed(range(seq_len))\n","        else:\n","            time_steps = range(seq_len)\n","\n","        outputs = []\n","        for t in time_steps:\n","            x = input_tensor[:, t, :, :, :]  # (batch, channels, height, width)\n","            for i, layer in enumerate(self.layers):\n","                h, c = layer(x, (hidden_state[i], cell_state[i]))\n","                hidden_state[i] = h\n","                cell_state[i] = c\n","                x = h  # input to next layer\n","            outputs.append(x)\n","\n","        outputs = torch.stack(outputs, dim=1)  # (batch, seq_len, channels, height, width)\n","        if reverse:\n","            outputs = outputs.flip(dims=[1])  # Reverse back to original order\n","        return outputs  # Return the sequence of outputs\n","\n","class BidirectionalConvLSTM2D(nn.Module):\n","    \"\"\"\n","    Implements a Bidirectional ConvLSTM2D layer.\n","    Processes the input sequence in both forward and backward directions and concatenates the outputs.\n","    \"\"\"\n","    def __init__(self, input_channels, hidden_channels, kernel_size=3, num_layers=1, bias=True):\n","        super(BidirectionalConvLSTM2D, self).__init__()\n","        self.forward_conv_lstm = ConvLSTM2D(input_channels, hidden_channels, kernel_size, bias=bias, num_layers=num_layers)\n","        self.backward_conv_lstm = ConvLSTM2D(input_channels, hidden_channels, kernel_size, bias=bias, num_layers=num_layers)\n","\n","    def forward(self, input_tensor):\n","        # input_tensor shape: (batch, seq_len, channels, height, width)\n","        # Forward direction\n","        forward_output = self.forward_conv_lstm(input_tensor, reverse=False)  # (batch, seq_len, hidden_channels, H, W)\n","        # Backward direction\n","        backward_output = self.backward_conv_lstm(input_tensor, reverse=True)  # (batch, seq_len, hidden_channels, H, W)\n","        # Concatenate outputs along the channel dimension\n","        output = torch.cat([forward_output, backward_output], dim=2)  # (batch, seq_len, hidden_channels*2, H, W)\n","        # Since seq_len=2, we can take the last output\n","        output = output[:, -1, :, :, :]  # Take the last output (batch, hidden_channels*2, H, W)\n","        return output\n","\n","# ============================== Swin Transformer Blocks ================================\n","\n","class WindowAttention(nn.Module):\n","    \"\"\"\n","    Window based multi-head self attention (W-MSA) module with relative position bias.\n","    \"\"\"\n","    def __init__(self, dim, window_size, num_heads, qkv_bias=True, attn_drop=0., proj_drop=0.):\n","        \"\"\"\n","        Args:\n","            dim (int): Number of input channels.\n","            window_size (tuple): Height and width of the window.\n","            num_heads (int): Number of attention heads.\n","            qkv_bias (bool): If True, add a learnable bias to query, key, value.\n","            attn_drop (float): Dropout ratio of attention weights.\n","            proj_drop (float): Dropout ratio of output.\n","        \"\"\"\n","        super(WindowAttention, self).__init__()\n","        self.dim = dim\n","        self.window_size = window_size  # Wh, Ww\n","        self.num_heads = num_heads\n","        head_dim = dim // num_heads\n","        self.scale = head_dim ** -0.5\n","\n","        # Define a parameter table of relative position bias\n","        self.relative_position_bias_table = nn.Parameter(\n","            torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads)\n","        )  # 2*Wh-1 * 2*Ww-1, nH\n","\n","        # Get pair-wise relative position index for each token inside the window\n","        coords_h = torch.arange(self.window_size[0])\n","        coords_w = torch.arange(self.window_size[1])\n","        coords = torch.stack(torch.meshgrid(coords_h, coords_w, indexing='ij'))  # 2, Wh, Ww\n","        coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n","        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n","        relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n","        relative_coords[:, :, 0] += self.window_size[0] - 1  # shift to start from 0\n","        relative_coords[:, :, 1] += self.window_size[1] - 1\n","        relative_coords[:, :, 0] *= (2 * self.window_size[1] - 1)\n","        relative_position_index = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n","        self.register_buffer(\"relative_position_index\", relative_position_index)\n","\n","        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)  # Query, Key, Value\n","        self.attn_drop = nn.Dropout(attn_drop)\n","        self.proj = nn.Linear(dim, dim)\n","        self.proj_drop = nn.Dropout(proj_drop)\n","\n","        # Initialize relative position bias table\n","        nn.init.trunc_normal_(self.relative_position_bias_table, std=.02)\n","\n","    def forward(self, x, mask=None):\n","        \"\"\"\n","        Args:\n","            x: input features with shape of (num_windows*B, Wh*Ww, C)\n","            mask: (num_windows, Wh*Ww, Wh*Ww) or None\n","        \"\"\"\n","        B_, N, C = x.shape\n","        qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)  # 3, B_, nH, N, C//nH\n","        q, k, v = qkv[0], qkv[1], qkv[2]  # each has shape (B_, nH, N, C//nH)\n","\n","        q = q * self.scale\n","        attn = (q @ k.transpose(-2, -1))  # (B_, nH, N, N)\n","\n","        # Add relative position bias\n","        relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(\n","            self.window_size[0] * self.window_size[1], self.window_size[0] * self.window_size[1], -1\n","        )  # Wh*Ww, Wh*Ww, nH\n","        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n","        attn = attn + relative_position_bias.unsqueeze(0)  # (B_, nH, N, N)\n","\n","        if mask is not None:\n","            nW = mask.shape[0]\n","            attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)\n","            attn = attn.view(-1, self.num_heads, N, N)\n","            attn = F.softmax(attn, dim=-1)\n","        else:\n","            attn = F.softmax(attn, dim=-1)\n","\n","        attn = self.attn_drop(attn)\n","\n","        out = (attn @ v).transpose(1, 2).reshape(B_, N, C)  # (B_, N, C)\n","        out = self.proj(out)\n","        out = self.proj_drop(out)\n","        return out\n","\n","class SwinTransformerBlock(nn.Module):\n","    \"\"\"\n","    Swin Transformer Block with W-MSA and SW-MSA.\n","    \"\"\"\n","    def __init__(self, dim, num_heads, window_size=7, shift_size=0, mlp_ratio=4., qkv_bias=True,\n","                 attn_drop=0., proj_drop=0.):\n","        super(SwinTransformerBlock, self).__init__()\n","        self.dim = dim\n","        self.num_heads = num_heads\n","        self.window_size = window_size  # W\n","        self.shift_size = shift_size    # S\n","        self.mlp_ratio = mlp_ratio\n","\n","        assert 0 <= self.shift_size < self.window_size, \"shift_size must be in [0, window_size)\"\n","\n","        self.norm1 = nn.LayerNorm(dim)\n","        self.attn = WindowAttention(dim, (window_size, window_size), num_heads, qkv_bias, attn_drop, proj_drop)\n","\n","        self.drop_path = nn.Identity()  # Can implement stochastic depth if desired\n","        self.norm2 = nn.LayerNorm(dim)\n","        mlp_hidden_dim = int(dim * mlp_ratio)\n","        self.mlp = nn.Sequential(\n","            nn.Linear(dim, mlp_hidden_dim),\n","            nn.GELU(),\n","            nn.Linear(mlp_hidden_dim, dim),\n","            nn.Dropout(proj_drop)\n","        )\n","\n","    def forward(self, x):\n","        \"\"\"\n","        Args:\n","            x: input features with shape (B, H*W, C)\n","        \"\"\"\n","        H = W = int(np.sqrt(x.shape[1]))\n","        B, L, C = x.shape\n","        assert L == H * W, \"Input feature has wrong size\"\n","\n","        shortcut = x\n","        x = self.norm1(x)\n","        x = x.view(B, H, W, C)\n","\n","        # Cyclic shift\n","        if self.shift_size > 0:\n","            shifted_x = torch.roll(x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2))\n","        else:\n","            shifted_x = x\n","\n","        # Partition windows\n","        window_size = self.window_size\n","        # Pad H and W to be multiples of window_size\n","        pad_b = (window_size - H % window_size) % window_size\n","        pad_r = (window_size - W % window_size) % window_size\n","        shifted_x = F.pad(shifted_x, (0, 0, 0, pad_r, 0, pad_b))  # pad H and W\n","        _, Hp, Wp, _ = shifted_x.shape\n","\n","        # Window partition\n","        x_windows = shifted_x.view(B, Hp // window_size, window_size, Wp // window_size, window_size, C)\n","        x_windows = x_windows.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size * window_size, C)  # (num_windows*B, window_size*window_size, C)\n","\n","        # Attention\n","        attn_windows = self.attn(x_windows)  # (num_windows*B, window_size*window_size, C)\n","\n","        # Merge windows\n","        shifted_x = attn_windows.view(-1, window_size, window_size, C)\n","        shifted_x = shifted_x.view(B, Hp // window_size, Wp // window_size, window_size, window_size, C)\n","        shifted_x = shifted_x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, Hp, Wp, C)\n","\n","        # Reverse cyclic shift\n","        if self.shift_size > 0:\n","            x = torch.roll(shifted_x, shifts=(self.shift_size, self.shift_size), dims=(1, 2))\n","        else:\n","            x = shifted_x\n","\n","        # Remove padding\n","        x = x[:, :H, :W, :].contiguous().view(B, H * W, C)\n","\n","        # FFN\n","        x = shortcut + self.drop_path(x)\n","        x = x + self.drop_path(self.mlp(self.norm2(x)))\n","\n","        return x\n","\n","# =============================== Dice Loss Function ====================================\n","\n","class DiceLoss(nn.Module):\n","    \"\"\"\n","    Dice Loss function to maximize the Dice coefficient.\n","    Suitable for binary segmentation tasks.\n","    \"\"\"\n","    def __init__(self, smooth=1.0):\n","        super(DiceLoss, self).__init__()\n","        self.smooth = smooth\n","\n","    def forward(self, y_pred, y_true):\n","        \"\"\"\n","        Args:\n","            y_pred (torch.Tensor): Predicted mask probabilities with shape (B, 1, H, W)\n","            y_true (torch.Tensor): Ground truth masks with shape (B, 1, H, W)\n","        Returns:\n","            torch.Tensor: Dice loss\n","        \"\"\"\n","        y_pred = y_pred.view(-1)\n","        y_true = y_true.view(-1)\n","\n","        intersection = (y_pred * y_true).sum()\n","        dice = (2. * intersection + self.smooth) / (y_pred.sum() + y_true.sum() + self.smooth)\n","\n","        return 1 - dice\n","\n","# ================================ Main Model ============================================\n","\n","class SwinUNet(nn.Module):\n","    \"\"\"\n","    Swin U-Net architecture for image segmentation with bidirectional ConvLSTM layers.\n","    \"\"\"\n","    def __init__(self, input_channels=3, output_channels=1,\n","                 embed_dim=32, num_heads=[4, 8], window_size=4,\n","                 mlp_ratio=4., depth=2):\n","        super(SwinUNet, self).__init__()\n","        self.input_channels = input_channels\n","        self.output_channels = output_channels\n","\n","        # Initial convolutional layers\n","        self.conv1 = SeparableConv2d(input_channels, 24, kernel_size=3, padding=1)\n","        self.bn1 = nn.BatchNorm2d(24)\n","        self.conv2 = SeparableConv2d(24, 24, kernel_size=3, padding=1)\n","        self.bn2 = nn.BatchNorm2d(24)\n","        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)  # 256x256 -> 128x128\n","\n","        # First Swin Transformer Block\n","        self.swin_unet_E1 = SwinTransformerBlock(\n","            dim=24,  # Changed from embed_dim=32 to 24\n","            num_heads=num_heads[0],\n","            window_size=window_size,\n","            shift_size=window_size//2 if True else 0,\n","            mlp_ratio=mlp_ratio\n","        )\n","\n","        # Second convolutional block\n","        self.conv3 = SeparableConv2d(24, 48, kernel_size=3, padding=1)\n","        self.bn3 = nn.BatchNorm2d(48)\n","        self.conv4 = SeparableConv2d(48, 48, kernel_size=3, padding=1)\n","        self.bn4 = nn.BatchNorm2d(48)\n","        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)  # 128x128 -> 64x64\n","\n","        # Second Swin Transformer Block\n","        self.swin_unet_E2 = SwinTransformerBlock(\n","            dim=48,\n","            num_heads=num_heads[1],\n","            window_size=window_size,\n","            shift_size=window_size//2 if True else 0,\n","            mlp_ratio=mlp_ratio\n","        )\n","\n","        # Third convolutional block (Bottleneck)\n","        self.conv5 = SeparableConv2d(48, 96, kernel_size=3, padding=1)\n","        self.bn5 = nn.BatchNorm2d(96)\n","        self.conv6 = SeparableConv2d(96, 96, kernel_size=3, padding=1)\n","        self.bn6 = nn.BatchNorm2d(96)\n","        self.drop5 = nn.Dropout(0.5)\n","        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)  # 64x64 -> 32x32\n","\n","        # Bottleneck convolutions with dense connections\n","        self.conv7 = SeparableConv2d(96, 192, kernel_size=3, padding=1)\n","        self.bn7 = nn.BatchNorm2d(192)\n","        self.conv8 = SeparableConv2d(192, 192, kernel_size=3, padding=1)\n","        self.bn8 = nn.BatchNorm2d(192)\n","        self.drop6_1 = nn.Dropout(0.5)\n","\n","        self.conv9 = SeparableConv2d(192, 192, kernel_size=3, padding=1)\n","        self.bn9 = nn.BatchNorm2d(192)\n","        self.conv10 = SeparableConv2d(192, 192, kernel_size=3, padding=1)\n","        self.bn10 = nn.BatchNorm2d(192)\n","        self.drop6_2 = nn.Dropout(0.5)\n","\n","        self.concat1 = nn.Sequential(\n","            SeparableConv2d(384, 192, kernel_size=3, padding=1),\n","            SeparableConv2d(192, 192, kernel_size=3, padding=1)\n","        )\n","        self.drop6_3 = nn.Dropout(0.5)\n","\n","        # First Upsampling Block\n","        self.up1 = nn.ConvTranspose2d(192, 96, kernel_size=2, stride=2)  # 32x32 -> 64x64\n","        self.bn_up1 = nn.BatchNorm2d(96)\n","        self.relu_up1 = nn.ReLU(inplace=True)\n","        self.bidirectional_convLSTM1 = BidirectionalConvLSTM2D(input_channels=96, hidden_channels=192, kernel_size=3, num_layers=1)\n","        self.swin_unet_D1 = SwinTransformerBlock(\n","            dim=192 * 2,  # Adjusted for bidirectional output\n","            num_heads=num_heads[0],\n","            window_size=window_size,\n","            shift_size=window_size//2 if True else 0,\n","            mlp_ratio=mlp_ratio\n","        )\n","        self.conv11 = SeparableConv2d(192 * 2, 48, kernel_size=3, padding=1)\n","        self.conv12 = SeparableConv2d(48, 48, kernel_size=3, padding=1)\n","\n","        # Second Upsampling Block\n","        self.up2 = nn.ConvTranspose2d(48, 48, kernel_size=2, stride=2)  # 64x64 -> 128x128\n","        self.bn_up2 = nn.BatchNorm2d(48)\n","        self.relu_up2 = nn.ReLU(inplace=True)\n","        self.bidirectional_convLSTM2 = BidirectionalConvLSTM2D(input_channels=48, hidden_channels=96, kernel_size=3, num_layers=1)\n","        self.swin_unet_D2 = SwinTransformerBlock(\n","            dim=96 * 2,\n","            num_heads=num_heads[1],\n","            window_size=window_size,\n","            shift_size=window_size//2 if True else 0,\n","            mlp_ratio=mlp_ratio\n","        )\n","        self.conv13 = SeparableConv2d(96 * 2, 24, kernel_size=3, padding=1)\n","        self.conv14 = SeparableConv2d(24, 24, kernel_size=3, padding=1)\n","\n","        # Third Upsampling Block\n","        self.up3 = nn.ConvTranspose2d(24, 24, kernel_size=2, stride=2)  # 128x128 -> 256x256\n","        self.bn_up3 = nn.BatchNorm2d(24)\n","        self.relu_up3 = nn.ReLU(inplace=True)\n","        self.bidirectional_convLSTM3 = BidirectionalConvLSTM2D(input_channels=24, hidden_channels=48, kernel_size=3, num_layers=1)\n","        self.swin_unet_D3 = SwinTransformerBlock(\n","            dim=48 * 2,\n","            num_heads=num_heads[1],\n","            window_size=window_size,\n","            shift_size=window_size//2 if True else 0,\n","            mlp_ratio=mlp_ratio\n","        )\n","        self.conv15 = SeparableConv2d(48 * 2, 24, kernel_size=3, padding=1)\n","        self.conv16 = SeparableConv2d(24, 24, kernel_size=3, padding=1)\n","\n","        # Output Layer\n","        self.final_conv1 = nn.Conv2d(24, 2, kernel_size=3, padding=1)\n","        self.final_relu = nn.ReLU(inplace=True)\n","        self.final_conv2 = nn.Conv2d(2, 1, kernel_size=1, padding=0)\n","        self.final_sigmoid = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        \"\"\"\n","        Forward pass of the Swin U-Net model.\n","        Args:\n","            x: Input tensor with shape (B, 3, 256, 256)\n","        Returns:\n","            torch.Tensor: Output segmentation mask with shape (B, 1, 256, 256)\n","        \"\"\"\n","        # Initial Convolutions\n","        x1 = self.conv1(x)          # (B, 24, 256, 256)\n","        x1 = self.bn1(x1)\n","        x1 = self.conv2(x1)         # (B, 24, 256, 256)\n","        x1 = self.bn2(x1)\n","        p1 = self.pool1(x1)         # (B, 24, 128, 128)\n","\n","        # First Swin Transformer Block\n","        p1_flat = p1.flatten(2).transpose(1, 2)  # (B, 128*128, 24)\n","        swin_E1 = self.swin_unet_E1(p1_flat)     # (B, 128*128, 24)\n","        swin_E1 = swin_E1.transpose(1, 2).view(-1, 24, 128, 128)  # Reshape for Conv2d\n","\n","        # Second Convolutional Block\n","        x2 = self.conv3(swin_E1)    # (B, 48, 128, 128)\n","        x2 = self.bn3(x2)\n","        x2 = self.conv4(x2)          # (B, 48, 128, 128)\n","        x2 = self.bn4(x2)\n","        p2 = self.pool2(x2)          # (B, 48, 64, 64)\n","\n","        # Second Swin Transformer Block\n","        p2_flat = p2.flatten(2).transpose(1, 2)  # (B, 64*64, 48)\n","        swin_E2 = self.swin_unet_E2(p2_flat)     # (B, 64*64, 48)\n","        swin_E2 = swin_E2.transpose(1, 2).view(-1, 48, 64, 64)  # Reshape for Conv2d\n","\n","        # Third Convolutional Block (Bottleneck)\n","        x3 = self.conv5(swin_E2)    # (B, 96, 64, 64)\n","        x3 = self.bn5(x3)\n","        x3 = self.conv6(x3)          # (B, 96, 64, 64)\n","        x3 = self.bn6(x3)\n","        x3 = self.drop5(x3)\n","        p3 = self.pool3(x3)          # (B, 96, 32, 32)\n","\n","        # Bottleneck Convolutions with Dense Connections\n","        x4 = self.conv7(p3)          # (B, 192, 32, 32)\n","        x4 = self.bn7(x4)\n","        x4 = self.conv8(x4)          # (B, 192, 32, 32)\n","        x4 = self.bn8(x4)\n","        x4 = self.drop6_1(x4)\n","\n","        x5 = self.conv9(x4)          # (B, 192, 32, 32)\n","        x5 = self.bn9(x5)\n","        x5 = self.conv10(x5)         # (B, 192, 32, 32)\n","        x5 = self.bn10(x5)\n","        x5 = self.drop6_2(x5)\n","\n","        concat = torch.cat([x5, x4], dim=1)  # (B, 384, 32, 32)\n","        concat = self.concat1(concat)         # (B, 192, 32, 32)\n","        concat = self.drop6_3(concat)         # (B, 192, 32, 32)\n","\n","        # First Upsampling Block\n","        up1 = self.up1(concat)                 # (B, 96, 64, 64)\n","        up1 = self.bn_up1(up1)\n","        up1 = self.relu_up1(up1)\n","\n","        # Prepare for BidirectionalConvLSTM2D\n","        up1_seq = torch.stack([x3, up1], dim=1)  # (B, 2, 96, 64, 64)\n","        bidir_convLSTM1_out = self.bidirectional_convLSTM1(up1_seq)  # (B, 192*2, 64, 64)\n","\n","        # Swin Transformer Block in Decoder\n","        bidir_convLSTM1_flat = bidir_convLSTM1_out.flatten(2).transpose(1, 2)  # (B, 64*64, 192*2)\n","        swin_D1 = self.swin_unet_D1(bidir_convLSTM1_flat)               # (B, 64*64, 192*2)\n","        swin_D1 = swin_D1.transpose(1, 2).view(-1, 192*2, 64, 64)    # Reshape for Conv2d\n","\n","        # Further Convolutions\n","        conv6 = self.conv11(swin_D1)        # (B, 48, 64, 64)\n","        conv6 = self.conv12(conv6)          # (B, 48, 64, 64)\n","\n","        # Second Upsampling Block\n","        up2 = self.up2(conv6)               # (B, 48, 128, 128)\n","        up2 = self.bn_up2(up2)\n","        up2 = self.relu_up2(up2)\n","\n","        # Prepare for BidirectionalConvLSTM2D\n","        up2_seq = torch.stack([x2, up2], dim=1)  # (B, 2, 48, 128, 128)\n","        bidir_convLSTM2_out = self.bidirectional_convLSTM2(up2_seq)  # (B, 96*2, 128, 128)\n","\n","        # Swin Transformer Block in Decoder\n","        bidir_convLSTM2_flat = bidir_convLSTM2_out.flatten(2).transpose(1, 2)  # (B, 128*128, 96*2)\n","        swin_D2 = self.swin_unet_D2(bidir_convLSTM2_flat)               # (B, 128*128, 96*2)\n","        swin_D2 = swin_D2.transpose(1, 2).view(-1, 96*2, 128, 128)    # Reshape for Conv2d\n","\n","        # Further Convolutions\n","        conv7 = self.conv13(swin_D2)        # (B, 24, 128, 128)\n","        conv7 = self.conv14(conv7)          # (B, 24, 128, 128)\n","\n","        # Third Upsampling Block\n","        up3 = self.up3(conv7)               # (B, 24, 256, 256)\n","        up3 = self.bn_up3(up3)\n","        up3 = self.relu_up3(up3)\n","\n","        # Prepare for BidirectionalConvLSTM2D\n","        up3_seq = torch.stack([x1, up3], dim=1)  # (B, 2, 24, 256, 256)\n","        bidir_convLSTM3_out = self.bidirectional_convLSTM3(up3_seq)  # (B, 48*2, 256, 256)\n","\n","        # Swin Transformer Block in Decoder\n","        bidir_convLSTM3_flat = bidir_convLSTM3_out.flatten(2).transpose(1, 2)  # (B, 256*256, 48*2)\n","        swin_D3 = self.swin_unet_D3(bidir_convLSTM3_flat)               # (B, 256*256, 48*2)\n","        swin_D3 = swin_D3.transpose(1, 2).view(-1, 48*2, 256, 256)    # Reshape for Conv2d\n","\n","        # Further Convolutions\n","        conv8 = self.conv15(swin_D3)        # (B, 24, 256, 256)\n","        conv8 = self.conv16(conv8)          # (B, 24, 256, 256)\n","\n","        # Final Output Convolutions\n","        final = self.final_conv1(conv8)      # (B, 2, 256, 256)\n","        final = self.final_relu(final)\n","        final = self.final_conv2(final)      # (B, 1, 256, 256)\n","        final = self.final_sigmoid(final)    # (B, 1, 256, 256)\n","\n","        return final\n","\n","# ================================== Dataset Class ======================================\n","\n","class SegmentationDataset(Dataset):\n","    \"\"\"\n","    Custom Dataset for image segmentation tasks.\n","    Expects images in 'x' folder and masks in 'y' folder.\n","    \"\"\"\n","    def __init__(self, images_dir, masks_dir, transform=None):\n","        super(SegmentationDataset, self).__init__()\n","        self.images_dir = images_dir\n","        self.masks_dir = masks_dir\n","        self.transform = transform\n","\n","        self.images = sorted(os.listdir(images_dir))\n","        self.masks = sorted(os.listdir(masks_dir))\n","\n","    def __len__(self):\n","        return len(self.images)\n","\n","    def __getitem__(self, idx):\n","        # Load image\n","        img_path = os.path.join(self.images_dir, self.images[idx])\n","        image = Image.open(img_path).convert('RGB')  # Ensure RGB\n","\n","        # Load mask\n","        mask_path = os.path.join(self.masks_dir, self.masks[idx])\n","        mask = Image.open(mask_path).convert('L')    # Grayscale\n","\n","        # Apply transformations\n","        if self.transform:\n","            image = self.transform(image)\n","            mask = self.transform(mask)\n","\n","        return image, mask\n","\n","# =============================== Data Loading and Preprocessing ========================\n","\n","# Define image dimensions\n","im_height = 256\n","im_width = 256\n","\n","# Define transformations\n","transform = transforms.Compose([\n","    transforms.Resize((im_height, im_width)),\n","    transforms.ToTensor(),  # Converts to [0,1]\n","])\n","\n","# Paths to the dataset (update these paths as per your directory structure)\n","train_images_dir = r'/content/drive/MyDrive/ML/dataset/ISIC2018_Task1-2_Training_Input'\n","train_masks_dir = r'/content/drive/MyDrive/ML/dataset/ISIC2018_Task1_Training_GroundTruth'\n","test_images_dir = r'/content/drive/MyDrive/ML/dataset/ISIC2018_Task1-2_Test_Input'\n","test_masks_dir = r'/content/drive/MyDrive/ML/dataset/ISIC2018_Task1_Test_GroundTruth'\n","\n","# Create datasets\n","train_dataset = SegmentationDataset(train_images_dir, train_masks_dir, transform=transform)\n","test_dataset = SegmentationDataset(test_images_dir, test_masks_dir, transform=transform)\n","\n","# Split training data into training and validation sets (80-20 split)\n","train_size = int(0.8 * len(train_dataset))\n","valid_size = len(train_dataset) - train_size\n","train_subset, valid_subset = torch.utils.data.random_split(train_dataset, [train_size, valid_size])\n","\n","# Create DataLoaders\n","batch_size = 5\n","\n","train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True, num_workers=4)\n","valid_loader = DataLoader(valid_subset, batch_size=batch_size, shuffle=False, num_workers=4)\n","test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n","\n","# =============================== Training Setup ==========================================\n","\n","# Instantiate the model\n","model = SwinUNet(input_channels=3, output_channels=1, embed_dim=32, num_heads=[4, 8], window_size=4, mlp_ratio=4., depth=2)\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","model = model.to(device)  # Move to GPU if available\n","\n","# Initialize weights using Kaiming Normal initialization\n","def initialize_weights(module):\n","    if isinstance(module, (nn.Conv2d, nn.ConvTranspose2d)):\n","        nn.init.kaiming_normal_(module.weight, mode='fan_in', nonlinearity='relu')\n","        if module.bias is not None:\n","            nn.init.zeros_(module.bias)\n","    elif isinstance(module, nn.Linear):\n","        nn.init.kaiming_normal_(module.weight, mode='fan_in', nonlinearity='relu')\n","        if module.bias is not None:\n","            nn.init.zeros_(module.bias)\n","    elif isinstance(module, nn.Conv3d):\n","        nn.init.kaiming_normal_(module.weight, mode='fan_in', nonlinearity='relu')\n","        if module.bias is not None:\n","            nn.init.zeros_(module.bias)\n","\n","model.apply(initialize_weights)\n","\n","# Define loss function and optimizer\n","criterion = DiceLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n","\n","# Define learning rate scheduler and early stopping parameters\n","scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.25, patience=5, verbose=True, min_lr=1e-9)\n","early_stopping_patience = 9\n","best_val_loss = np.inf\n","epochs_no_improve = 0\n","\n","# =============================== Training Loop ===========================================\n","\n","num_epochs = 1  # You can adjust the number of epochs\n","\n","for epoch in range(num_epochs):\n","    print(f\"Epoch {epoch+1}/{num_epochs}\")\n","    model.train()\n","    running_loss = 0.0\n","    train_loader_count = 0\n","    for images, masks in train_loader:\n","        train_loader_count += 1\n","        print(f\"Train loader image count: {train_loader_count}\")\n","        images = images.to(device)  # (B, 3, 256, 256)\n","        masks = masks.to(device)    # (B, 1, 256, 256)\n","\n","        optimizer.zero_grad()\n","        outputs = model(images)      # (B, 1, 256, 256)\n","        loss = criterion(outputs, masks)\n","        loss.backward()\n","        optimizer.step()\n","\n","        running_loss += loss.item() * images.size(0)\n","\n","    epoch_loss = running_loss / len(train_loader.dataset)\n","\n","    # Validation\n","    model.eval()\n","    val_loss = 0.0\n","    with torch.no_grad():\n","        for images, masks in valid_loader:\n","            images = images.to(device)\n","            masks = masks.to(device)\n","\n","            outputs = model(images)\n","            loss = criterion(outputs, masks)\n","\n","            val_loss += loss.item() * images.size(0)\n","\n","    val_loss /= len(valid_loader.dataset)\n","\n","    print(f'Epoch {epoch+1}/{num_epochs}, Training Loss: {epoch_loss:.4f}, Validation Loss: {val_loss:.4f}')\n","\n","    # Scheduler step\n","    scheduler.step(val_loss)\n","\n","    # Early stopping\n","    if val_loss < best_val_loss:\n","        best_val_loss = val_loss\n","        epochs_no_improve = 0\n","        # Save the best model\n","        torch.save(model.state_dict(), r'/content/drive/MyDrive/model/modelWeights_Swin_Trans_Weights_Swin_Trans_Leather.pth')\n","    else:\n","        epochs_no_improve += 1\n","        if epochs_no_improve >= early_stopping_patience:\n","            print('Early stopping!')\n","            break\n","\n","# ================================== Prediction ==========================================\n","\n","# Load the best model weights\n","model.load_state_dict(torch.load(r'/content/drive/MyDrive/model/modelWeights_Swin_Trans_Weights_Swin_Trans_Leather.pth'))\n","model.eval()\n","\n","# Function to save predictions and ground truth\n","def save_predictions(model, dataloader, save_dir_pred, save_dir_gt, device):\n","    \"\"\"\n","    Saves the predicted masks and ground truth masks.\n","    Args:\n","        model (nn.Module): Trained model.\n","        dataloader (DataLoader): DataLoader for test data.\n","        save_dir_pred (str): Directory to save predicted masks.\n","        save_dir_gt (str): Directory to save ground truth masks.\n","        device (str): Device to run the model on.\n","    \"\"\"\n","    os.makedirs(save_dir_pred, exist_ok=True)\n","    os.makedirs(save_dir_gt, exist_ok=True)\n","\n","    with torch.no_grad():\n","        for i, (images, masks) in enumerate(dataloader):\n","            if (i % 100 == 0):\n","                print(f\"{i}th Test Image\")  # Adjust as per your dataset\n","\n","            images = images.to(device)\n","            masks = masks.to(device)\n","\n","            outputs = model(images)\n","            preds = outputs.cpu().numpy()\n","            gts = masks.cpu().numpy()\n","\n","            for j in range(preds.shape[0]):\n","                pred_mask = preds[j, 0, :, :]\n","                gt_mask = gts[j, 0, :, :]\n","\n","                # Save predicted mask\n","                pred_img = Image.fromarray((pred_mask * 255).astype(np.uint8))\n","                pred_img.save(os.path.join(save_dir_pred, f\"{i * dataloader.batch_size + j + 1}.png\"))\n","\n","                # Save ground truth mask\n","                gt_img = Image.fromarray((gt_mask * 255).astype(np.uint8))\n","                gt_img.save(os.path.join(save_dir_gt, f\"{i * dataloader.batch_size + j + 1}.tiff\"))\n","\n","# Define directories to save predictions and ground truth\n","save_dir_pred = r'/content/drive/MyDrive/output/segmented predicted images'\n","save_dir_gt = r'/content/drive/MyDrive/output/segmented ground truth'\n","\n","# Save predictions\n","save_predictions(model, test_loader, save_dir_pred, save_dir_gt, device)\n","\n","# =================================== Evaluation =========================================\n","\n","def evaluate_metrics_pytorch(model, dataloader, device):\n","    \"\"\"\n","    Evaluates various metrics for segmentation performance.\n","    Args:\n","        model (nn.Module): Trained model.\n","        dataloader (DataLoader): DataLoader for test data.\n","        device (str): Device to run the model on.\n","    Returns:\n","        dict: Dictionary containing average metrics.\n","    \"\"\"\n","    model.eval()\n","    all_metrics = {\n","        'Accuracy': [],\n","        'Dice': [],\n","        'Jaccard': [],\n","        'Sensitivity': [],\n","        'Specificity': [],\n","        'Precision': [],\n","        'Recall': [],\n","        'F1-Score': []\n","    }\n","\n","    with torch.no_grad():\n","        for images, masks in dataloader:\n","            images = images.to(device)\n","            masks = masks.to(device)\n","\n","            outputs = model(images)\n","            preds = outputs > 0.5  # Binary mask\n","\n","            preds = preds.cpu().numpy().astype(np.uint8)\n","            masks = masks.cpu().numpy().astype(np.uint8)\n","            masks = (masks > 0).astype(np.uint8)  # Convert to binary masks\n","\n","            for pred, mask in zip(preds, masks):\n","                pred_flat = pred.flatten()\n","                mask_flat = mask.flatten()\n","\n","                # Calculate metrics\n","                tn, fp, fn, tp = confusion_matrix(mask_flat, pred_flat, labels=[0,1]).ravel()\n","\n","                accuracy = (tp + tn) / (tp + tn + fp + fn + 1e-8)\n","                iou = jaccard_score(mask_flat, pred_flat, zero_division=0)\n","                dice = f1_score(mask_flat, pred_flat, zero_division=0)\n","                specificity = tn / (tn + fp + 1e-8)\n","                sensitivity = recall_score(mask_flat, pred_flat, zero_division=0)\n","                precision = precision_score(mask_flat, pred_flat, zero_division=0)\n","                recall = sensitivity\n","                f1 = dice  # F1-Score is the same as Dice coefficient for binary classification\n","\n","                all_metrics['Accuracy'].append(accuracy)\n","                all_metrics['Jaccard'].append(iou)\n","                all_metrics['Dice'].append(dice)\n","                all_metrics['Specificity'].append(specificity)\n","                all_metrics['Sensitivity'].append(sensitivity)\n","                all_metrics['Precision'].append(precision)\n","                all_metrics['Recall'].append(recall)\n","                all_metrics['F1-Score'].append(f1)\n","\n","    # Compute average metrics\n","    avg_metrics = {metric: np.mean(values) for metric, values in all_metrics.items()}\n","\n","    print(\"Evaluation Metrics:\")\n","    for metric, value in avg_metrics.items():\n","        print(f\"{metric}: {value:.4f}\")\n","\n","    return avg_metrics\n","\n","# Evaluate the model\n","metrics = evaluate_metrics_pytorch(model, test_loader, device)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vJlUbzgnLeKk","outputId":"76d7239b-6ce9-42b6-b7ea-d10d7a762270"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/1\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Train loader image count: 1\n","Train loader image count: 2\n","Train loader image count: 3\n","Train loader image count: 4\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2037,"status":"ok","timestamp":1730119931130,"user":{"displayName":"ALOK THOTTAKATHU PRASANNAKUMAR","userId":"10483471326433970042"},"user_tz":-660},"id":"mh37fiQOLvJt","outputId":"e4b89806-b24a-4a2e-decb-85d79711288f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting torchinfo\n","  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n","Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n","Installing collected packages: torchinfo\n","Successfully installed torchinfo-1.8.0\n"]}],"source":["!pip install torchinfo"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3925,"status":"ok","timestamp":1730120316265,"user":{"displayName":"ALOK THOTTAKATHU PRASANNAKUMAR","userId":"10483471326433970042"},"user_tz":-660},"id":"GVicjKqLLSXV","outputId":"02cc02ee-3ac4-40d9-f468-89f72b86c39a"},"outputs":[{"data":{"text/plain":["==========================================================================================\n","Layer (type:depth-idx)                   Output Shape              Param #\n","==========================================================================================\n","SwinUNet                                 [1, 1, 256, 256]          --\n","├─SeparableConv2d: 1-1                   [1, 24, 256, 256]         --\n","│    └─Conv2d: 2-1                       [1, 3, 256, 256]          30\n","│    └─Conv2d: 2-2                       [1, 24, 256, 256]         96\n","├─BatchNorm2d: 1-2                       [1, 24, 256, 256]         48\n","├─SeparableConv2d: 1-3                   [1, 24, 256, 256]         --\n","│    └─Conv2d: 2-3                       [1, 24, 256, 256]         240\n","│    └─Conv2d: 2-4                       [1, 24, 256, 256]         600\n","├─BatchNorm2d: 1-4                       [1, 24, 256, 256]         48\n","├─MaxPool2d: 1-5                         [1, 24, 128, 128]         --\n","├─SwinTransformerBlock: 1-6              [1, 16384, 24]            --\n","│    └─LayerNorm: 2-5                    [1, 16384, 24]            48\n","│    └─WindowAttention: 2-6              [1024, 16, 24]            196\n","│    │    └─Linear: 3-1                  [1024, 16, 72]            1,800\n","│    │    └─Dropout: 3-2                 [1024, 4, 16, 16]         --\n","│    │    └─Linear: 3-3                  [1024, 16, 24]            600\n","│    │    └─Dropout: 3-4                 [1024, 16, 24]            --\n","│    └─Identity: 2-7                     [1, 16384, 24]            --\n","│    └─LayerNorm: 2-8                    [1, 16384, 24]            48\n","│    └─Sequential: 2-9                   [1, 16384, 24]            --\n","│    │    └─Linear: 3-5                  [1, 16384, 96]            2,400\n","│    │    └─GELU: 3-6                    [1, 16384, 96]            --\n","│    │    └─Linear: 3-7                  [1, 16384, 24]            2,328\n","│    │    └─Dropout: 3-8                 [1, 16384, 24]            --\n","│    └─Identity: 2-10                    [1, 16384, 24]            --\n","├─SeparableConv2d: 1-7                   [1, 48, 128, 128]         --\n","│    └─Conv2d: 2-11                      [1, 24, 128, 128]         240\n","│    └─Conv2d: 2-12                      [1, 48, 128, 128]         1,200\n","├─BatchNorm2d: 1-8                       [1, 48, 128, 128]         96\n","├─SeparableConv2d: 1-9                   [1, 48, 128, 128]         --\n","│    └─Conv2d: 2-13                      [1, 48, 128, 128]         480\n","│    └─Conv2d: 2-14                      [1, 48, 128, 128]         2,352\n","├─BatchNorm2d: 1-10                      [1, 48, 128, 128]         96\n","├─MaxPool2d: 1-11                        [1, 48, 64, 64]           --\n","├─SwinTransformerBlock: 1-12             [1, 4096, 48]             --\n","│    └─LayerNorm: 2-15                   [1, 4096, 48]             96\n","│    └─WindowAttention: 2-16             [256, 16, 48]             392\n","│    │    └─Linear: 3-9                  [256, 16, 144]            7,056\n","│    │    └─Dropout: 3-10                [256, 8, 16, 16]          --\n","│    │    └─Linear: 3-11                 [256, 16, 48]             2,352\n","│    │    └─Dropout: 3-12                [256, 16, 48]             --\n","│    └─Identity: 2-17                    [1, 4096, 48]             --\n","│    └─LayerNorm: 2-18                   [1, 4096, 48]             96\n","│    └─Sequential: 2-19                  [1, 4096, 48]             --\n","│    │    └─Linear: 3-13                 [1, 4096, 192]            9,408\n","│    │    └─GELU: 3-14                   [1, 4096, 192]            --\n","│    │    └─Linear: 3-15                 [1, 4096, 48]             9,264\n","│    │    └─Dropout: 3-16                [1, 4096, 48]             --\n","│    └─Identity: 2-20                    [1, 4096, 48]             --\n","├─SeparableConv2d: 1-13                  [1, 96, 64, 64]           --\n","│    └─Conv2d: 2-21                      [1, 48, 64, 64]           480\n","│    └─Conv2d: 2-22                      [1, 96, 64, 64]           4,704\n","├─BatchNorm2d: 1-14                      [1, 96, 64, 64]           192\n","├─SeparableConv2d: 1-15                  [1, 96, 64, 64]           --\n","│    └─Conv2d: 2-23                      [1, 96, 64, 64]           960\n","│    └─Conv2d: 2-24                      [1, 96, 64, 64]           9,312\n","├─BatchNorm2d: 1-16                      [1, 96, 64, 64]           192\n","├─Dropout: 1-17                          [1, 96, 64, 64]           --\n","├─MaxPool2d: 1-18                        [1, 96, 32, 32]           --\n","├─SeparableConv2d: 1-19                  [1, 192, 32, 32]          --\n","│    └─Conv2d: 2-25                      [1, 96, 32, 32]           960\n","│    └─Conv2d: 2-26                      [1, 192, 32, 32]          18,624\n","├─BatchNorm2d: 1-20                      [1, 192, 32, 32]          384\n","├─SeparableConv2d: 1-21                  [1, 192, 32, 32]          --\n","│    └─Conv2d: 2-27                      [1, 192, 32, 32]          1,920\n","│    └─Conv2d: 2-28                      [1, 192, 32, 32]          37,056\n","├─BatchNorm2d: 1-22                      [1, 192, 32, 32]          384\n","├─Dropout: 1-23                          [1, 192, 32, 32]          --\n","├─SeparableConv2d: 1-24                  [1, 192, 32, 32]          --\n","│    └─Conv2d: 2-29                      [1, 192, 32, 32]          1,920\n","│    └─Conv2d: 2-30                      [1, 192, 32, 32]          37,056\n","├─BatchNorm2d: 1-25                      [1, 192, 32, 32]          384\n","├─SeparableConv2d: 1-26                  [1, 192, 32, 32]          --\n","│    └─Conv2d: 2-31                      [1, 192, 32, 32]          1,920\n","│    └─Conv2d: 2-32                      [1, 192, 32, 32]          37,056\n","├─BatchNorm2d: 1-27                      [1, 192, 32, 32]          384\n","├─Dropout: 1-28                          [1, 192, 32, 32]          --\n","├─Sequential: 1-29                       [1, 192, 32, 32]          --\n","│    └─SeparableConv2d: 2-33             [1, 192, 32, 32]          --\n","│    │    └─Conv2d: 3-17                 [1, 384, 32, 32]          3,840\n","│    │    └─Conv2d: 3-18                 [1, 192, 32, 32]          73,920\n","│    └─SeparableConv2d: 2-34             [1, 192, 32, 32]          --\n","│    │    └─Conv2d: 3-19                 [1, 192, 32, 32]          1,920\n","│    │    └─Conv2d: 3-20                 [1, 192, 32, 32]          37,056\n","├─Dropout: 1-30                          [1, 192, 32, 32]          --\n","├─ConvTranspose2d: 1-31                  [1, 96, 64, 64]           73,824\n","├─BatchNorm2d: 1-32                      [1, 96, 64, 64]           192\n","├─ReLU: 1-33                             [1, 96, 64, 64]           --\n","├─BidirectionalConvLSTM2D: 1-34          [1, 384, 64, 64]          --\n","│    └─ConvLSTM2D: 2-35                  [1, 2, 192, 64, 64]       --\n","│    │    └─ModuleList: 3-21             --                        1,991,424\n","│    └─ConvLSTM2D: 2-36                  [1, 2, 192, 64, 64]       --\n","│    │    └─ModuleList: 3-22             --                        1,991,424\n","├─SwinTransformerBlock: 1-35             [1, 4096, 384]            --\n","│    └─LayerNorm: 2-37                   [1, 4096, 384]            768\n","│    └─WindowAttention: 2-38             [256, 16, 384]            196\n","│    │    └─Linear: 3-23                 [256, 16, 1152]           443,520\n","│    │    └─Dropout: 3-24                [256, 4, 16, 16]          --\n","│    │    └─Linear: 3-25                 [256, 16, 384]            147,840\n","│    │    └─Dropout: 3-26                [256, 16, 384]            --\n","│    └─Identity: 2-39                    [1, 4096, 384]            --\n","│    └─LayerNorm: 2-40                   [1, 4096, 384]            768\n","│    └─Sequential: 2-41                  [1, 4096, 384]            --\n","│    │    └─Linear: 3-27                 [1, 4096, 1536]           591,360\n","│    │    └─GELU: 3-28                   [1, 4096, 1536]           --\n","│    │    └─Linear: 3-29                 [1, 4096, 384]            590,208\n","│    │    └─Dropout: 3-30                [1, 4096, 384]            --\n","│    └─Identity: 2-42                    [1, 4096, 384]            --\n","├─SeparableConv2d: 1-36                  [1, 48, 64, 64]           --\n","│    └─Conv2d: 2-43                      [1, 384, 64, 64]          3,840\n","│    └─Conv2d: 2-44                      [1, 48, 64, 64]           18,480\n","├─SeparableConv2d: 1-37                  [1, 48, 64, 64]           --\n","│    └─Conv2d: 2-45                      [1, 48, 64, 64]           480\n","│    └─Conv2d: 2-46                      [1, 48, 64, 64]           2,352\n","├─ConvTranspose2d: 1-38                  [1, 48, 128, 128]         9,264\n","├─BatchNorm2d: 1-39                      [1, 48, 128, 128]         96\n","├─ReLU: 1-40                             [1, 48, 128, 128]         --\n","├─BidirectionalConvLSTM2D: 1-41          [1, 192, 128, 128]        --\n","│    └─ConvLSTM2D: 2-47                  [1, 2, 96, 128, 128]      --\n","│    │    └─ModuleList: 3-31             --                        498,048\n","│    └─ConvLSTM2D: 2-48                  [1, 2, 96, 128, 128]      --\n","│    │    └─ModuleList: 3-32             --                        498,048\n","├─SwinTransformerBlock: 1-42             [1, 16384, 192]           --\n","│    └─LayerNorm: 2-49                   [1, 16384, 192]           384\n","│    └─WindowAttention: 2-50             [1024, 16, 192]           392\n","│    │    └─Linear: 3-33                 [1024, 16, 576]           111,168\n","│    │    └─Dropout: 3-34                [1024, 8, 16, 16]         --\n","│    │    └─Linear: 3-35                 [1024, 16, 192]           37,056\n","│    │    └─Dropout: 3-36                [1024, 16, 192]           --\n","│    └─Identity: 2-51                    [1, 16384, 192]           --\n","│    └─LayerNorm: 2-52                   [1, 16384, 192]           384\n","│    └─Sequential: 2-53                  [1, 16384, 192]           --\n","│    │    └─Linear: 3-37                 [1, 16384, 768]           148,224\n","│    │    └─GELU: 3-38                   [1, 16384, 768]           --\n","│    │    └─Linear: 3-39                 [1, 16384, 192]           147,648\n","│    │    └─Dropout: 3-40                [1, 16384, 192]           --\n","│    └─Identity: 2-54                    [1, 16384, 192]           --\n","├─SeparableConv2d: 1-43                  [1, 24, 128, 128]         --\n","│    └─Conv2d: 2-55                      [1, 192, 128, 128]        1,920\n","│    └─Conv2d: 2-56                      [1, 24, 128, 128]         4,632\n","├─SeparableConv2d: 1-44                  [1, 24, 128, 128]         --\n","│    └─Conv2d: 2-57                      [1, 24, 128, 128]         240\n","│    └─Conv2d: 2-58                      [1, 24, 128, 128]         600\n","├─ConvTranspose2d: 1-45                  [1, 24, 256, 256]         2,328\n","├─BatchNorm2d: 1-46                      [1, 24, 256, 256]         48\n","├─ReLU: 1-47                             [1, 24, 256, 256]         --\n","├─BidirectionalConvLSTM2D: 1-48          [1, 96, 256, 256]         --\n","│    └─ConvLSTM2D: 2-59                  [1, 2, 48, 256, 256]      --\n","│    │    └─ModuleList: 3-41             --                        124,608\n","│    └─ConvLSTM2D: 2-60                  [1, 2, 48, 256, 256]      --\n","│    │    └─ModuleList: 3-42             --                        124,608\n","├─SwinTransformerBlock: 1-49             [1, 65536, 96]            --\n","│    └─LayerNorm: 2-61                   [1, 65536, 96]            192\n","│    └─WindowAttention: 2-62             [4096, 16, 96]            392\n","│    │    └─Linear: 3-43                 [4096, 16, 288]           27,936\n","│    │    └─Dropout: 3-44                [4096, 8, 16, 16]         --\n","│    │    └─Linear: 3-45                 [4096, 16, 96]            9,312\n","│    │    └─Dropout: 3-46                [4096, 16, 96]            --\n","│    └─Identity: 2-63                    [1, 65536, 96]            --\n","│    └─LayerNorm: 2-64                   [1, 65536, 96]            192\n","│    └─Sequential: 2-65                  [1, 65536, 96]            --\n","│    │    └─Linear: 3-47                 [1, 65536, 384]           37,248\n","│    │    └─GELU: 3-48                   [1, 65536, 384]           --\n","│    │    └─Linear: 3-49                 [1, 65536, 96]            36,960\n","│    │    └─Dropout: 3-50                [1, 65536, 96]            --\n","│    └─Identity: 2-66                    [1, 65536, 96]            --\n","├─SeparableConv2d: 1-50                  [1, 24, 256, 256]         --\n","│    └─Conv2d: 2-67                      [1, 96, 256, 256]         960\n","│    └─Conv2d: 2-68                      [1, 24, 256, 256]         2,328\n","├─SeparableConv2d: 1-51                  [1, 24, 256, 256]         --\n","│    └─Conv2d: 2-69                      [1, 24, 256, 256]         240\n","│    └─Conv2d: 2-70                      [1, 24, 256, 256]         600\n","├─Conv2d: 1-52                           [1, 2, 256, 256]          434\n","├─ReLU: 1-53                             [1, 2, 256, 256]          --\n","├─Conv2d: 1-54                           [1, 1, 256, 256]          3\n","├─Sigmoid: 1-55                          [1, 1, 256, 256]          --\n","==========================================================================================\n","Total params: 7,995,403\n","Trainable params: 7,995,403\n","Non-trainable params: 0\n","Total mult-adds (G): 99.98\n","==========================================================================================\n","Input size (MB): 0.79\n","Forward/backward pass size (MB): 2053.37\n","Params size (MB): 31.98\n","Estimated Total Size (MB): 2086.14\n","=========================================================================================="]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["from torchinfo import summary\n","\n","# Define the input size based on your model's expected input.\n","# For example, if your model expects images with 3 channels and 256x256 dimensions:\n","input_size = (1, 3, 256, 256)  # (batch_size, channels, height, width)\n","\n","# Generate and print the model summary\n","summary(model, input_size=input_size, device='cuda' if torch.cuda.is_available() else 'cpu')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6ZXUJmRPLqjP","colab":{"base_uri":"https://localhost:8080/","height":424},"executionInfo":{"status":"error","timestamp":1730630992542,"user_tz":-660,"elapsed":17486,"user":{"displayName":"ALOK THOTTAKATHU PRASANNAKUMAR","userId":"10483471326433970042"}},"outputId":"24b3ebbd-e312-4ae2-f899-c7bf54127ba4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/1\n","Train loader batch count: 1\n"]},{"output_type":"error","ename":"RuntimeError","evalue":"Given transposed=1, weight of size [128, 64, 2, 2], expected input[5, 256, 16, 16] to have 128 channels, but got 256 channels instead","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-dfab6a483e49>\u001b[0m in \u001b[0;36m<cell line: 636>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 648\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m      \u001b[0;31m# (B, 1, 256, 256)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    649\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-9-dfab6a483e49>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    508\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m                 \u001b[0mupsample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"upsample_{i}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 510\u001b[0;31m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupsample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    511\u001b[0m                 \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, output_size)\u001b[0m\n\u001b[1;32m   1160\u001b[0m         )\n\u001b[1;32m   1161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1162\u001b[0;31m         return F.conv_transpose2d(\n\u001b[0m\u001b[1;32m   1163\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1164\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Given transposed=1, weight of size [128, 64, 2, 2], expected input[5, 256, 16, 16] to have 128 channels, but got 256 channels instead"]}],"source":["# ============================== Imports and Dependencies ==============================\n","\n","import os\n","import numpy as np\n","from PIL import Image\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms\n","\n","from sklearn.metrics import (\n","    confusion_matrix,\n","    jaccard_score, f1_score, precision_score, recall_score\n",")\n","\n","# ================================ Separable Convolution =================================\n","\n","class SeparableConv2d(nn.Module):\n","    \"\"\"\n","    Implements a separable convolution layer using depthwise and pointwise convolutions.\n","    \"\"\"\n","    def __init__(self, in_channels, out_channels, kernel_size=3, padding=1, bias=True):\n","        super(SeparableConv2d, self).__init__()\n","        # Depthwise convolution (groups=in_channels)\n","        self.depthwise = nn.Conv2d(in_channels, in_channels, kernel_size=kernel_size,\n","                                   padding=padding, groups=in_channels, bias=bias)\n","        # Pointwise convolution\n","        self.pointwise = nn.Conv2d(in_channels, out_channels, kernel_size=1,\n","                                   padding=0, bias=bias)\n","\n","    def forward(self, x):\n","        x = self.depthwise(x)\n","        x = self.pointwise(x)\n","        return x\n","\n","# ================================== ConvLSTM2D ========================================\n","\n","class ConvLSTMCell(nn.Module):\n","    \"\"\"\n","    Implements a ConvLSTM cell.\n","    \"\"\"\n","    def __init__(self, input_channels, hidden_channels, kernel_size, bias=True):\n","        super(ConvLSTMCell, self).__init__()\n","\n","        padding = kernel_size // 2  # To maintain spatial dimensions\n","        self.input_channels = input_channels\n","        self.hidden_channels = hidden_channels\n","\n","        self.conv = nn.Conv2d(in_channels=input_channels + hidden_channels,\n","                              out_channels=4 * hidden_channels,\n","                              kernel_size=kernel_size,\n","                              padding=padding,\n","                              bias=bias)\n","\n","    def forward(self, input_tensor, cur_state):\n","        h_cur, c_cur = cur_state\n","\n","        # Concatenate input and hidden state\n","        combined = torch.cat([input_tensor, h_cur], dim=1)  # along channel axis\n","\n","        # Compute all gates at once\n","        conv_output = self.conv(combined)\n","        cc_i, cc_f, cc_o, cc_g = torch.split(conv_output, self.hidden_channels, dim=1)\n","\n","        i = torch.sigmoid(cc_i)   # input gate\n","        f = torch.sigmoid(cc_f)   # forget gate\n","        o = torch.sigmoid(cc_o)   # output gate\n","        g = torch.tanh(cc_g)      # gate gate\n","\n","        c_next = f * c_cur + i * g\n","        h_next = o * torch.tanh(c_next)\n","\n","        return h_next, c_next\n","\n","    def init_hidden(self, batch_size, spatial_size, device):\n","        height, width = spatial_size\n","        return (torch.zeros(batch_size, self.hidden_channels, height, width, device=device),\n","                torch.zeros(batch_size, self.hidden_channels, height, width, device=device))\n","\n","class ConvLSTM2D(nn.Module):\n","    \"\"\"\n","    Implements a ConvLSTM2D layer that processes a sequence of inputs.\n","    \"\"\"\n","    def __init__(self, input_channels, hidden_channels, kernel_size=3, bias=True, num_layers=1):\n","        super(ConvLSTM2D, self).__init__()\n","        self.num_layers = num_layers\n","        self.hidden_channels = hidden_channels\n","\n","        layers = []\n","        for i in range(num_layers):\n","            input_c = input_channels if i == 0 else hidden_channels\n","            layers.append(ConvLSTMCell(input_c, hidden_channels, kernel_size, bias))\n","        self.layers = nn.ModuleList(layers)\n","\n","    def forward(self, input_tensor, reverse=False):\n","        # input_tensor shape: (batch, seq_len, channels, height, width)\n","        batch_size, seq_len, channels, height, width = input_tensor.size()\n","        device = input_tensor.device\n","\n","        # Initialize hidden and cell states for all layers\n","        hidden_state = []\n","        cell_state = []\n","        for i in range(self.num_layers):\n","            h, c = self.layers[i].init_hidden(batch_size, (height, width), device)\n","            hidden_state.append(h)\n","            cell_state.append(c)\n","\n","        # Iterate over time steps\n","        if reverse:\n","            time_steps = reversed(range(seq_len))\n","        else:\n","            time_steps = range(seq_len)\n","\n","        outputs = []\n","        for t in time_steps:\n","            x = input_tensor[:, t, :, :, :]  # (batch, channels, height, width)\n","            for i, layer in enumerate(self.layers):\n","                h, c = layer(x, (hidden_state[i], cell_state[i]))\n","                hidden_state[i] = h\n","                cell_state[i] = c\n","                x = h  # input to next layer\n","            outputs.append(x)\n","\n","        outputs = torch.stack(outputs, dim=1)  # (batch, seq_len, channels, height, width)\n","        if reverse:\n","            outputs = outputs.flip(dims=[1])  # Reverse back to original order\n","        return outputs  # Return the sequence of outputs\n","\n","class BidirectionalConvLSTM2D(nn.Module):\n","    \"\"\"\n","    Implements a Bidirectional ConvLSTM2D layer.\n","    Processes the input sequence in both forward and backward directions and concatenates the outputs.\n","    \"\"\"\n","    def __init__(self, input_channels, hidden_channels, kernel_size=3, num_layers=1, bias=True):\n","        super(BidirectionalConvLSTM2D, self).__init__()\n","        self.forward_conv_lstm = ConvLSTM2D(input_channels, hidden_channels, kernel_size, bias=bias, num_layers=num_layers)\n","        self.backward_conv_lstm = ConvLSTM2D(input_channels, hidden_channels, kernel_size, bias=bias, num_layers=num_layers)\n","\n","    def forward(self, input_tensor):\n","        # input_tensor shape: (batch, seq_len, channels, height, width)\n","        # Forward direction\n","        forward_output = self.forward_conv_lstm(input_tensor, reverse=False)  # (batch, seq_len, hidden_channels, H, W)\n","        # Backward direction\n","        backward_output = self.backward_conv_lstm(input_tensor, reverse=True)  # (batch, seq_len, hidden_channels, H, W)\n","        # Concatenate outputs along the channel dimension\n","        output = torch.cat([forward_output, backward_output], dim=2)  # (batch, seq_len, hidden_channels*2, H, W)\n","        # Since seq_len=2, we can take the last output\n","        output = output[:, -1, :, :, :]  # Take the last output (batch, hidden_channels*2, H, W)\n","        return output\n","\n","# =============================== Patch Embedding ========================================\n","\n","class PatchEmbed(nn.Module):\n","    \"\"\"\n","    Image to Patch Embedding.\n","    \"\"\"\n","    def __init__(self, img_size=256, patch_size=4, in_chans=3, embed_dim=32):\n","        super().__init__()\n","        self.img_size = img_size\n","        self.patch_size = patch_size\n","        num_patches = (img_size // patch_size) * (img_size // patch_size)\n","        self.num_patches = num_patches\n","\n","        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n","        self.norm = nn.LayerNorm(embed_dim)\n","\n","    def forward(self, x):\n","        # x: [B, C, H, W]\n","        x = self.proj(x)  # [B, embed_dim, H/patch_size, W/patch_size]\n","        x = x.flatten(2).transpose(1, 2)  # [B, num_patches, embed_dim]\n","        x = self.norm(x)\n","        return x\n","\n","# ============================== Swin Transformer Blocks ================================\n","\n","class WindowAttention(nn.Module):\n","    \"\"\"\n","    Window based multi-head self attention (W-MSA) module with relative position bias.\n","    \"\"\"\n","    def __init__(self, dim, window_size, num_heads, qkv_bias=True, attn_drop=0., proj_drop=0.):\n","        \"\"\"\n","        Args:\n","            dim (int): Number of input channels.\n","            window_size (tuple): Height and width of the window.\n","            num_heads (int): Number of attention heads.\n","            qkv_bias (bool): If True, add a learnable bias to query, key, value.\n","            attn_drop (float): Dropout ratio of attention weights.\n","            proj_drop (float): Dropout ratio of output.\n","        \"\"\"\n","        super(WindowAttention, self).__init__()\n","        self.dim = dim\n","        self.window_size = window_size  # Wh, Ww\n","        self.num_heads = num_heads\n","        head_dim = dim // num_heads\n","        self.scale = head_dim ** -0.5\n","\n","        # Define a parameter table of relative position bias\n","        self.relative_position_bias_table = nn.Parameter(\n","            torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads)\n","        )  # 2*Wh-1 * 2*Ww-1, nH\n","\n","        # Get pair-wise relative position index for each token inside the window\n","        coords_h = torch.arange(self.window_size[0])\n","        coords_w = torch.arange(self.window_size[1])\n","        coords = torch.stack(torch.meshgrid(coords_h, coords_w, indexing='ij'))  # 2, Wh, Ww\n","        coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n","        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n","        relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n","        relative_coords[:, :, 0] += self.window_size[0] - 1  # shift to start from 0\n","        relative_coords[:, :, 1] += self.window_size[1] - 1\n","        relative_coords[:, :, 0] *= (2 * self.window_size[1] - 1)\n","        relative_position_index = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n","        self.register_buffer(\"relative_position_index\", relative_position_index)\n","\n","        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)  # Query, Key, Value\n","        self.attn_drop = nn.Dropout(attn_drop)\n","        self.proj = nn.Linear(dim, dim)\n","        self.proj_drop = nn.Dropout(proj_drop)\n","\n","        # Initialize relative position bias table\n","        nn.init.trunc_normal_(self.relative_position_bias_table, std=.02)\n","\n","    def forward(self, x, mask=None):\n","        \"\"\"\n","        Args:\n","            x: input features with shape of (num_windows*B, Wh*Ww, C)\n","            mask: (num_windows, Wh*Ww, Wh*Ww) or None\n","        \"\"\"\n","        B_, N, C = x.shape\n","        qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)  # 3, B_, nH, N, C//nH\n","        q, k, v = qkv[0], qkv[1], qkv[2]  # each has shape (B_, nH, N, C//nH)\n","\n","        q = q * self.scale\n","        attn = (q @ k.transpose(-2, -1))  # (B_, nH, N, N)\n","\n","        # Add relative position bias\n","        relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(\n","            self.window_size[0] * self.window_size[1], self.window_size[0] * self.window_size[1], -1\n","        )  # Wh*Ww, Wh*Ww, nH\n","        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n","        attn = attn + relative_position_bias.unsqueeze(0)  # (B_, nH, N, N)\n","\n","        if mask is not None:\n","            nW = mask.shape[0]\n","            attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)\n","            attn = attn.view(-1, self.num_heads, N, N)\n","            attn = F.softmax(attn, dim=-1)\n","        else:\n","            attn = F.softmax(attn, dim=-1)\n","\n","        attn = self.attn_drop(attn)\n","\n","        out = (attn @ v).transpose(1, 2).reshape(B_, N, C)  # (B_, N, C)\n","        out = self.proj(out)\n","        out = self.proj_drop(out)\n","        return out\n","\n","class SwinTransformerBlock(nn.Module):\n","    \"\"\"\n","    Swin Transformer Block with W-MSA and SW-MSA.\n","    \"\"\"\n","    def __init__(self, dim, num_heads, window_size=7, shift_size=0, mlp_ratio=4., qkv_bias=True,\n","                 attn_drop=0., proj_drop=0., mlp_hidden_dim=512):\n","        super(SwinTransformerBlock, self).__init__()\n","        self.dim = dim\n","        self.num_heads = num_heads\n","        self.window_size = window_size  # W\n","        self.shift_size = shift_size    # S\n","        self.mlp_ratio = mlp_ratio\n","\n","        assert 0 <= self.shift_size < self.window_size, \"shift_size must be in [0, window_size)\"\n","\n","        self.norm1 = nn.LayerNorm(dim)\n","        self.attn = WindowAttention(dim, (window_size, window_size), num_heads, qkv_bias, attn_drop, proj_drop)\n","\n","        self.drop_path = nn.Identity()  # Can implement stochastic depth if desired\n","        self.norm2 = nn.LayerNorm(dim)\n","        # mlp_hidden_dim = int(dim * mlp_ratio)\n","        self.mlp = nn.Sequential(\n","            nn.Linear(dim, mlp_hidden_dim),\n","            nn.GELU(),\n","            nn.Linear(mlp_hidden_dim, dim),\n","            nn.Dropout(proj_drop)\n","        )\n","\n","    def forward(self, x, H, W, mask_matrix=None):\n","        \"\"\"\n","        Args:\n","            x: input features with shape (B, H*W, C)\n","            H, W: spatial dimensions\n","            mask_matrix: attention mask\n","        \"\"\"\n","        B, L, C = x.shape\n","        assert L == H * W, \"Input feature has wrong size\"\n","\n","        shortcut = x\n","        x = self.norm1(x)\n","        x = x.view(B, H, W, C)\n","\n","        # Padding for window partition\n","        pad_r = (self.window_size - W % self.window_size) % self.window_size\n","        pad_b = (self.window_size - H % self.window_size) % self.window_size\n","        x = F.pad(x, (0, 0, 0, pad_r, 0, pad_b))\n","\n","        _, Hp, Wp, _ = x.shape\n","\n","        # Cyclic shift\n","        if self.shift_size > 0:\n","            shifted_x = torch.roll(\n","                x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2)\n","            )\n","            attn_mask = mask_matrix\n","        else:\n","            shifted_x = x\n","            attn_mask = None\n","\n","        # Partition windows\n","        x_windows = shifted_x.unfold(1, self.window_size, self.window_size).unfold(\n","            2, self.window_size, self.window_size\n","        )\n","        x_windows = x_windows.contiguous().view(-1, self.window_size * self.window_size, C)\n","\n","        # W-MSA/SW-MSA\n","        attn_windows = self.attn(x_windows, mask=attn_mask)\n","\n","        # Merge windows\n","        attn_windows = attn_windows.view(-1, self.window_size, self.window_size, C)\n","        shifted_x = attn_windows.view(\n","            B, Hp // self.window_size, Wp // self.window_size, self.window_size, self.window_size, C\n","        )\n","        shifted_x = shifted_x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, Hp, Wp, C)\n","\n","        # Reverse cyclic shift\n","        if self.shift_size > 0:\n","            x = torch.roll(\n","                shifted_x, shifts=(self.shift_size, self.shift_size), dims=(1, 2)\n","            )\n","        else:\n","            x = shifted_x\n","\n","        # Remove padding\n","        x = x[:, :H, :W, :].contiguous().view(B, H * W, C)\n","\n","        # FFN\n","        x = shortcut + self.drop_path(x)\n","        x = x + self.drop_path(self.mlp(self.norm2(x)))\n","\n","        return x\n","\n","# =============================== Dice Loss Function ====================================\n","\n","class DiceLoss(nn.Module):\n","    \"\"\"\n","    Dice Loss function to maximize the Dice coefficient.\n","    Suitable for binary segmentation tasks.\n","    \"\"\"\n","    def __init__(self, smooth=1.0):\n","        super(DiceLoss, self).__init__()\n","        self.smooth = smooth\n","\n","    def forward(self, y_pred, y_true):\n","        \"\"\"\n","        Args:\n","            y_pred (torch.Tensor): Predicted mask probabilities with shape (B, 1, H, W)\n","            y_true (torch.Tensor): Ground truth masks with shape (B, 1, H, W)\n","        Returns:\n","            torch.Tensor: Dice loss\n","        \"\"\"\n","        y_pred = y_pred.view(-1)\n","        y_true = y_true.view(-1)\n","\n","        intersection = (y_pred * y_true).sum()\n","        dice = (2. * intersection + self.smooth) / (y_pred.sum() + y_true.sum() + self.smooth)\n","\n","        return 1 - dice\n","\n","# =============================== Swin UNet Model ========================================\n","\n","class SwinUNet(nn.Module):\n","    \"\"\"\n","    Swin U-Net architecture for image segmentation.\n","    \"\"\"\n","    def __init__(self, input_channels=3, output_channels=1,\n","                 filter_num_begin=32, depth=4, stack_num_down=2, stack_num_up=2,\n","                 num_heads=[4, 8, 8, 8], window_size=[4, 2, 2, 2], num_mlp=512,\n","                 shift_window=True, **kwargs):\n","        super(SwinUNet, self).__init__()\n","\n","        self.input_channels = input_channels\n","        self.output_channels = output_channels\n","        self.filter_num_begin = filter_num_begin\n","        self.depth = depth\n","        self.stack_num_down = stack_num_down\n","        self.stack_num_up = stack_num_up\n","        self.num_heads = num_heads\n","        self.window_size = window_size\n","        self.num_mlp = num_mlp\n","        self.shift_window = shift_window\n","\n","        # Define the number of channels at each level\n","        self.filter_nums = [filter_num_begin * (2 ** i) for i in range(depth)]\n","        # Example: [32, 64, 128, 256] if depth=4\n","\n","        # Patch Embedding\n","        self.patch_embed = PatchEmbed(\n","            img_size=256, patch_size=4, in_chans=input_channels, embed_dim=self.filter_nums[0]\n","        )\n","\n","        # Encoder\n","        self.encoder_layers = nn.ModuleList()\n","        for i in range(depth):\n","            down_layers = nn.ModuleList()\n","            for _ in range(stack_num_down):\n","                down_layers.append(SwinTransformerBlock(\n","                    dim=self.filter_nums[i],\n","                    num_heads=num_heads[i],\n","                    window_size=window_size[i],\n","                    shift_size=window_size[i] // 2 if shift_window else 0,\n","                    mlp_hidden_dim=num_mlp,\n","                ))\n","            self.encoder_layers.append(down_layers)\n","\n","            if i < depth - 1:\n","                setattr(self, f\"downsample_{i}\", nn.Conv2d(\n","                    self.filter_nums[i], self.filter_nums[i + 1], kernel_size=2, stride=2)\n","                )\n","\n","        # Bottleneck\n","        self.bottleneck_layers = nn.ModuleList()\n","        for _ in range(stack_num_down):\n","            self.bottleneck_layers.append(SwinTransformerBlock(\n","                dim=self.filter_nums[-1],\n","                num_heads=num_heads[-1],\n","                window_size=window_size[-1],\n","                shift_size=window_size[-1] // 2 if shift_window else 0,\n","                mlp_hidden_dim=num_mlp,\n","            ))\n","\n","        # Decoder\n","        self.decoder_layers = nn.ModuleList()\n","        for i in range(depth):\n","            if i == 0:\n","                # The first decoder layer after the bottleneck\n","                up_in_channels = self.filter_nums[depth - 1]\n","                in_channels = self.filter_nums[depth - 1]\n","                out_channels = self.filter_nums[depth - 1]\n","            else:\n","                up_in_channels = self.filter_nums[depth - i]\n","                in_channels = self.filter_nums[depth - i - 1] * 2  # After concatenation\n","                out_channels = self.filter_nums[depth - i - 1]\n","\n","            # Upsampling layer (except for the first decoder layer)\n","            if i > 0:\n","                setattr(self, f\"upsample_{i}\", nn.ConvTranspose2d(\n","                    up_in_channels, out_channels, kernel_size=2, stride=2)\n","                )\n","\n","            # Swin Transformer Blocks\n","            up_layers = nn.ModuleList()\n","            for _ in range(stack_num_up):\n","                up_layers.append(SwinTransformerBlock(\n","                    dim=in_channels,\n","                    num_heads=num_heads[depth - i - 1],\n","                    window_size=window_size[depth - i - 1],\n","                    shift_size=window_size[depth - i - 1] // 2 if shift_window else 0,\n","                    mlp_hidden_dim=num_mlp,\n","                ))\n","            self.decoder_layers.append(up_layers)\n","\n","        # Final Convolution\n","        self.final_conv = nn.Sequential(\n","            nn.Conv2d(self.filter_nums[0], self.filter_nums[0] // 2, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(self.filter_nums[0] // 2, output_channels, kernel_size=1)\n","        )\n","\n","    def forward(self, x):\n","        # Initial Patch Embedding\n","        x = self.patch_embed(x)  # x shape: [B, num_patches, embed_dim]\n","        B, N, C = x.shape\n","        H = W = int(np.sqrt(N))\n","\n","        x = x.transpose(1, 2).view(B, C, H, W)  # [B, C, H, W]\n","\n","        # Encoder\n","        encodings = []\n","        for i, layers in enumerate(self.encoder_layers):\n","            for blk in layers:\n","                x = x.flatten(2).transpose(1, 2)  # [B, H*W, C]\n","                x = blk(x, H, W)\n","                x = x.transpose(1, 2).view(B, -1, H, W)\n","            encodings.append(x)\n","            if i < self.depth - 1:\n","                downsample = getattr(self, f\"downsample_{i}\")\n","                x = downsample(x)\n","                _, _, H, W = x.shape\n","\n","        # Bottleneck\n","        for blk in self.bottleneck_layers:\n","            x = x.flatten(2).transpose(1, 2)\n","            x = blk(x, H, W)\n","            x = x.transpose(1, 2).view(B, -1, H, W)\n","\n","        # Decoder\n","        for i in range(self.depth):\n","            if i > 0:\n","                upsample = getattr(self, f\"upsample_{i}\")\n","                x = upsample(x)\n","                _, _, H, W = x.shape\n","\n","                # Concatenate with skip connection\n","                skip_connection = encodings[self.depth - i - 1]\n","                x = torch.cat([x, skip_connection], dim=1)  # Concatenate along channels\n","\n","            for blk in self.decoder_layers[i]:\n","                x = x.flatten(2).transpose(1, 2)\n","                x = blk(x, H, W)\n","                x = x.transpose(1, 2).view(B, -1, H, W)\n","\n","        # Final Convolution\n","        x = self.final_conv(x)\n","        x = torch.sigmoid(x)\n","        return x\n","# ================================== Dataset Class ======================================\n","\n","class SegmentationDataset(Dataset):\n","    \"\"\"\n","    Custom Dataset for image segmentation tasks.\n","    Expects images in 'x' folder and masks in 'y' folder.\n","    \"\"\"\n","    def __init__(self, images_dir, masks_dir, transform=None):\n","        super(SegmentationDataset, self).__init__()\n","        self.images_dir = images_dir\n","        self.masks_dir = masks_dir\n","        self.transform = transform\n","\n","        self.images = sorted(os.listdir(images_dir))\n","        self.masks = sorted(os.listdir(masks_dir))\n","\n","    def __len__(self):\n","        return len(self.images)\n","\n","    def __getitem__(self, idx):\n","        # Load image\n","        img_path = os.path.join(self.images_dir, self.images[idx])\n","        image = Image.open(img_path).convert('RGB')  # Ensure RGB\n","\n","        # Load mask\n","        mask_path = os.path.join(self.masks_dir, self.masks[idx])\n","        mask = Image.open(mask_path).convert('L')    # Grayscale\n","\n","        # Apply transformations\n","        if self.transform:\n","            image = self.transform(image)\n","            mask = self.transform(mask)\n","\n","        return image, mask\n","\n","# =============================== Data Loading and Preprocessing ========================\n","\n","# Define image dimensions\n","im_height = 256\n","im_width = 256\n","\n","# Define transformations\n","transform = transforms.Compose([\n","    transforms.Resize((im_height, im_width)),\n","    transforms.ToTensor(),  # Converts to [0,1]\n","])\n","\n","# Paths to the dataset (update these paths as per your directory structure)\n","train_images_dir = r'/content/drive/MyDrive/ML/dataset/ISIC2018_Task1-2_Training_Input'\n","train_masks_dir = r'/content/drive/MyDrive/ML/dataset/ISIC2018_Task1_Training_GroundTruth'\n","test_images_dir = r'/content/drive/MyDrive/ML/dataset/ISIC2018_Task1-2_Test_Input'\n","test_masks_dir = r'/content/drive/MyDrive/ML/dataset/ISIC2018_Task1_Test_GroundTruth'\n","\n","# Create datasets\n","train_dataset = SegmentationDataset(train_images_dir, train_masks_dir, transform=transform)\n","test_dataset = SegmentationDataset(test_images_dir, test_masks_dir, transform=transform)\n","\n","# Split training data into training and validation sets (80-20 split)\n","train_size = int(0.8 * len(train_dataset))\n","valid_size = len(train_dataset) - train_size\n","train_subset, valid_subset = torch.utils.data.random_split(train_dataset, [train_size, valid_size])\n","\n","# Create DataLoaders\n","batch_size = 5\n","\n","train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True, num_workers=4)\n","valid_loader = DataLoader(valid_subset, batch_size=batch_size, shuffle=False, num_workers=4)\n","test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n","\n","# =============================== Training Setup ==========================================\n","\n","# Instantiate the model\n","model = SwinUNet(input_channels=3, output_channels=1,\n","                 filter_num_begin=32, depth=4, stack_num_down=2, stack_num_up=2,\n","                 num_heads=[4, 8, 8, 8], window_size=[4, 2, 2, 2], num_mlp=512,\n","                 shift_window=True)\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","model = model.to(device)  # Move to GPU if available\n","\n","# Initialize weights using Kaiming Normal initialization\n","def initialize_weights(module):\n","    if isinstance(module, (nn.Conv2d, nn.ConvTranspose2d)):\n","        nn.init.kaiming_normal_(module.weight, mode='fan_in', nonlinearity='relu')\n","        if module.bias is not None:\n","            nn.init.zeros_(module.bias)\n","    elif isinstance(module, nn.Linear):\n","        nn.init.kaiming_normal_(module.weight, mode='fan_in', nonlinearity='relu')\n","        if module.bias is not None:\n","            nn.init.zeros_(module.bias)\n","    elif isinstance(module, nn.Conv3d):\n","        nn.init.kaiming_normal_(module.weight, mode='fan_in', nonlinearity='relu')\n","        if module.bias is not None:\n","            nn.init.zeros_(module.bias)\n","\n","model.apply(initialize_weights)\n","\n","# Define loss function and optimizer\n","criterion = DiceLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n","\n","# Define learning rate scheduler and early stopping parameters\n","scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.25, patience=5, verbose=True, min_lr=1e-9)\n","early_stopping_patience = 9\n","best_val_loss = np.inf\n","epochs_no_improve = 0\n","\n","# =============================== Training Loop ===========================================\n","\n","num_epochs = 1  # You can adjust the number of epochs\n","\n","for epoch in range(num_epochs):\n","    print(f\"Epoch {epoch+1}/{num_epochs}\")\n","    model.train()\n","    running_loss = 0.0\n","    train_loader_count = 0\n","    for images, masks in train_loader:\n","        train_loader_count += 1\n","        print(f\"Train loader batch count: {train_loader_count}\")\n","        images = images.to(device)  # (B, 3, 256, 256)\n","        masks = masks.to(device)    # (B, 1, 256, 256)\n","\n","        optimizer.zero_grad()\n","        outputs = model(images)      # (B, 1, 256, 256)\n","        loss = criterion(outputs, masks)\n","        loss.backward()\n","        optimizer.step()\n","\n","        running_loss += loss.item() * images.size(0)\n","\n","    epoch_loss = running_loss / len(train_loader.dataset)\n","\n","    # Validation\n","    model.eval()\n","    val_loss = 0.0\n","    with torch.no_grad():\n","        for images, masks in valid_loader:\n","            images = images.to(device)\n","            masks = masks.to(device)\n","\n","            outputs = model(images)\n","            loss = criterion(outputs, masks)\n","\n","            val_loss += loss.item() * images.size(0)\n","\n","    val_loss /= len(valid_loader.dataset)\n","\n","    print(f'Epoch {epoch+1}/{num_epochs}, Training Loss: {epoch_loss:.4f}, Validation Loss: {val_loss:.4f}')\n","\n","    # Scheduler step\n","    scheduler.step(val_loss)\n","\n","    # Early stopping\n","    if val_loss < best_val_loss:\n","        best_val_loss = val_loss\n","        epochs_no_improve = 0\n","        # Save the best model\n","        torch.save(model.state_dict(), r'/content/drive/MyDrive/model/modelWeights_Swin_UNet.pth')\n","    else:\n","        epochs_no_improve += 1\n","        if epochs_no_improve >= early_stopping_patience:\n","            print('Early stopping!')\n","            break\n","\n","# ================================== Prediction ==========================================\n","\n","# Load the best model weights\n","model.load_state_dict(torch.load(r'/content/drive/MyDrive/model/modelWeights_Swin_UNet.pth'))\n","model.eval()\n","\n","# Function to save predictions and ground truth\n","def save_predictions(model, dataloader, save_dir_pred, save_dir_gt, device):\n","    \"\"\"\n","    Saves the predicted masks and ground truth masks.\n","    Args:\n","        model (nn.Module): Trained model.\n","        dataloader (DataLoader): DataLoader for test data.\n","        save_dir_pred (str): Directory to save predicted masks.\n","        save_dir_gt (str): Directory to save ground truth masks.\n","        device (str): Device to run the model on.\n","    \"\"\"\n","    os.makedirs(save_dir_pred, exist_ok=True)\n","    os.makedirs(save_dir_gt, exist_ok=True)\n","\n","    with torch.no_grad():\n","        for i, (images, masks) in enumerate(dataloader):\n","            if (i % 100 == 0):\n","                print(f\"{i}th Test Batch\")  # Adjust as per your dataset\n","\n","            images = images.to(device)\n","            masks = masks.to(device)\n","\n","            outputs = model(images)\n","            preds = outputs.cpu().numpy()\n","            gts = masks.cpu().numpy()\n","\n","            for j in range(preds.shape[0]):\n","                pred_mask = preds[j, 0, :, :]\n","                gt_mask = gts[j, 0, :, :]\n","\n","                # Save predicted mask\n","                pred_img = Image.fromarray((pred_mask * 255).astype(np.uint8))\n","                pred_img.save(os.path.join(save_dir_pred, f\"{i * dataloader.batch_size + j + 1}.png\"))\n","\n","                # Save ground truth mask\n","                gt_img = Image.fromarray((gt_mask * 255).astype(np.uint8))\n","                gt_img.save(os.path.join(save_dir_gt, f\"{i * dataloader.batch_size + j + 1}.tiff\"))\n","\n","# Define directories to save predictions and ground truth\n","save_dir_pred = r'/content/drive/MyDrive/output/segmented_predicted_images'\n","save_dir_gt = r'/content/drive/MyDrive/output/segmented_ground_truth'\n","\n","# Save predictions\n","save_predictions(model, test_loader, save_dir_pred, save_dir_gt, device)\n","\n","# =================================== Evaluation =========================================\n","\n","def evaluate_metrics_pytorch(model, dataloader, device):\n","    \"\"\"\n","    Evaluates various metrics for segmentation performance.\n","    Args:\n","        model (nn.Module): Trained model.\n","        dataloader (DataLoader): DataLoader for test data.\n","        device (str): Device to run the model on.\n","    Returns:\n","        dict: Dictionary containing average metrics.\n","    \"\"\"\n","    model.eval()\n","    all_metrics = {\n","        'Accuracy': [],\n","        'Dice': [],\n","        'Jaccard': [],\n","        'Sensitivity': [],\n","        'Specificity': [],\n","        'Precision': [],\n","        'Recall': [],\n","        'F1-Score': []\n","    }\n","\n","    with torch.no_grad():\n","        for images, masks in dataloader:\n","            images = images.to(device)\n","            masks = masks.to(device)\n","\n","            outputs = model(images)\n","            preds = outputs > 0.5  # Binary mask\n","\n","            preds = preds.cpu().numpy().astype(np.uint8)\n","            masks = masks.cpu().numpy().astype(np.uint8)\n","            masks = (masks > 0).astype(np.uint8)  # Convert to binary masks\n","\n","            for pred, mask in zip(preds, masks):\n","                pred_flat = pred.flatten()\n","                mask_flat = mask.flatten()\n","\n","                # Calculate metrics\n","                tn, fp, fn, tp = confusion_matrix(mask_flat, pred_flat, labels=[0,1]).ravel()\n","\n","                accuracy = (tp + tn) / (tp + tn + fp + fn + 1e-8)\n","                iou = jaccard_score(mask_flat, pred_flat, zero_division=0)\n","                dice = f1_score(mask_flat, pred_flat, zero_division=0)\n","                specificity = tn / (tn + fp + 1e-8)\n","                sensitivity = recall_score(mask_flat, pred_flat, zero_division=0)\n","                precision = precision_score(mask_flat, pred_flat, zero_division=0)\n","                recall = sensitivity\n","                f1 = dice  # F1-Score is the same as Dice coefficient for binary classification\n","\n","                all_metrics['Accuracy'].append(accuracy)\n","                all_metrics['Jaccard'].append(iou)\n","                all_metrics['Dice'].append(dice)\n","                all_metrics['Specificity'].append(specificity)\n","                all_metrics['Sensitivity'].append(sensitivity)\n","                all_metrics['Precision'].append(precision)\n","                all_metrics['Recall'].append(recall)\n","                all_metrics['F1-Score'].append(f1)\n","\n","    # Compute average metrics\n","    avg_metrics = {metric: np.mean(values) for metric, values in all_metrics.items()}\n","\n","    print(\"Evaluation Metrics:\")\n","    for metric, value in avg_metrics.items():\n","        print(f\"{metric}: {value:.4f}\")\n","\n","    return avg_metrics\n","\n","# Evaluate the model\n","metrics = evaluate_metrics_pytorch(model, test_loader, device)"]},{"cell_type":"code","source":[],"metadata":{"id":"KDY0l7fmjBhU"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[{"file_id":"1tyh5BJftbDOhqZ9OpLTDU-DFlc59eVr-","timestamp":1729386285240},{"file_id":"165pm1jr1hGwqd61GSH0AYMo8ymYJvxkx","timestamp":1729005117786}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}