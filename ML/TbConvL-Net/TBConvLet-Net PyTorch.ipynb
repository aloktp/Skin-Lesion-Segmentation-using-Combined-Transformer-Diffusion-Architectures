{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After pool1: torch.Size([1, 24, 128, 128])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [24, 48, 1, 1], expected input[1, 16, 16, 48] to have 48 channels, but got 16 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 246\u001b[0m\n\u001b[1;32m    243\u001b[0m dummy_input \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m256\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    245\u001b[0m \u001b[38;5;66;03m# Perform a forward pass\u001b[39;00m\n\u001b[0;32m--> 246\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdummy_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutput shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, output\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[2], line 204\u001b[0m, in \u001b[0;36mTBConvLNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    201\u001b[0m swin1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mswin_unet_e1(pool1)\n\u001b[1;32m    203\u001b[0m \u001b[38;5;66;03m# Adjust channels and spatial size\u001b[39;00m\n\u001b[0;32m--> 204\u001b[0m swin1 \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mConv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m48\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m24\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mswin1\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Reduce channels to 24\u001b[39;00m\n\u001b[1;32m    205\u001b[0m swin1 \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39minterpolate(swin1, size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m128\u001b[39m), mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbilinear\u001b[39m\u001b[38;5;124m'\u001b[39m, align_corners\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)  \u001b[38;5;66;03m# Resize to match expected spatial size\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAfter swin_unet_e1:\u001b[39m\u001b[38;5;124m\"\u001b[39m, swin1\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [24, 48, 1, 1], expected input[1, 16, 16, 48] to have 48 channels, but got 16 channels instead"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "import timm\n",
    "import numpy as np\n",
    "\n",
    "# Define SeparableConv2d\n",
    "class SeparableConv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, bias=False):\n",
    "        super(SeparableConv2d, self).__init__()\n",
    "        self.depthwise = nn.Conv2d(\n",
    "            in_channels, in_channels, kernel_size, stride, padding,\n",
    "            groups=in_channels, bias=bias\n",
    "        )\n",
    "        self.pointwise = nn.Conv2d(\n",
    "            in_channels, out_channels, kernel_size=1, bias=bias\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.depthwise(x)\n",
    "        x = self.pointwise(x)\n",
    "        return x\n",
    "\n",
    "# Define ConvLSTMCell\n",
    "class ConvLSTMCell(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size, bias=True):\n",
    "        super(ConvLSTMCell, self).__init__()\n",
    "\n",
    "        if isinstance(kernel_size, tuple):\n",
    "            padding = tuple(k // 2 for k in kernel_size)\n",
    "        else:\n",
    "            padding = kernel_size // 2\n",
    "\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels=input_dim + hidden_dim,\n",
    "            out_channels=4 * hidden_dim,\n",
    "            kernel_size=kernel_size,\n",
    "            padding=padding,\n",
    "            bias=bias\n",
    "        )\n",
    "\n",
    "    def forward(self, x, h_cur, c_cur):\n",
    "        combined = torch.cat([x, h_cur], dim=1)  # concatenate along channel axis\n",
    "\n",
    "        conv_output = self.conv(combined)\n",
    "        cc_i, cc_f, cc_o, cc_g = torch.split(conv_output, conv_output.shape[1] // 4, dim=1)\n",
    "\n",
    "        i = torch.sigmoid(cc_i)\n",
    "        f = torch.sigmoid(cc_f)\n",
    "        o = torch.sigmoid(cc_o)\n",
    "        g = torch.tanh(cc_g)\n",
    "\n",
    "        c_next = f * c_cur + i * g\n",
    "        h_next = o * torch.tanh(c_next)\n",
    "\n",
    "        return h_next, c_next\n",
    "\n",
    "# Define ConvLSTM module\n",
    "class ConvLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size, num_layers, batch_first=True, bias=True, go_backwards=False):\n",
    "        super(ConvLSTM, self).__init__()\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "        self.go_backwards = go_backwards\n",
    "        self.cell_list = nn.ModuleList()\n",
    "\n",
    "        for i in range(0, self.num_layers):\n",
    "            cur_input_dim = input_dim if i == 0 else hidden_dim\n",
    "\n",
    "            self.cell_list.append(ConvLSTMCell(\n",
    "                input_dim=cur_input_dim,\n",
    "                hidden_dim=hidden_dim,\n",
    "                kernel_size=kernel_size,\n",
    "                bias=bias\n",
    "            ))\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        # input_tensor: (batch, seq_len, channels, height, width)\n",
    "        b, seq_len, _, h, w = input_tensor.size()\n",
    "        hidden_state = self._init_hidden(b, h, w, input_tensor.device)\n",
    "\n",
    "        seq_indices = range(seq_len)\n",
    "        if self.go_backwards:\n",
    "            seq_indices = reversed(seq_indices)\n",
    "\n",
    "        cur_layer_input = input_tensor\n",
    "\n",
    "        for layer_idx in range(self.num_layers):\n",
    "            h, c = hidden_state[layer_idx]\n",
    "            output_inner = []\n",
    "\n",
    "            for t in seq_indices:\n",
    "                h, c = self.cell_list[layer_idx](cur_layer_input[:, t, :, :, :], h, c)\n",
    "                output_inner.append(h)\n",
    "\n",
    "            layer_output = torch.stack(output_inner, dim=1)\n",
    "            cur_layer_input = layer_output\n",
    "\n",
    "        return layer_output[:, -1], h\n",
    "\n",
    "    def _init_hidden(self, batch_size, image_height, image_width, device):\n",
    "        init_states = []\n",
    "        for i in range(self.num_layers):\n",
    "            hidden_dim = self.cell_list[i].conv.out_channels // 4\n",
    "            init_states.append((\n",
    "                torch.zeros(batch_size, hidden_dim, image_height, image_width).to(device),\n",
    "                torch.zeros(batch_size, hidden_dim, image_height, image_width).to(device)\n",
    "            ))\n",
    "        return init_states\n",
    "\n",
    "# Swin Transformer Block using timm\n",
    "class SwinTransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, depths, num_heads, window_size, img_size, mlp_ratio=4., qkv_bias=True):\n",
    "        super(SwinTransformerBlock, self).__init__()\n",
    "        self.model = timm.models.swin_transformer.SwinTransformer(\n",
    "            img_size=img_size,\n",
    "            patch_size=4,\n",
    "            in_chans=embed_dim,\n",
    "            num_classes=0,\n",
    "            embed_dim=embed_dim,\n",
    "            depths=depths,\n",
    "            num_heads=num_heads,\n",
    "            window_size=window_size,\n",
    "            mlp_ratio=mlp_ratio,\n",
    "            qkv_bias=qkv_bias,\n",
    "            drop_rate=0.,\n",
    "            drop_path_rate=0.,\n",
    "            ape=False,\n",
    "            patch_norm=True,\n",
    "            use_checkpoint=False\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model.forward_features(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# TBConvL-Net Model Definition\n",
    "class TBConvLNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TBConvLNet, self).__init__()\n",
    "\n",
    "        # Convergence Path (Downsampling)\n",
    "        self.conv1_1 = SeparableConv2d(3, 24, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1_1 = nn.BatchNorm2d(24)\n",
    "        self.conv1_2 = SeparableConv2d(24, 24, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1_2 = nn.BatchNorm2d(24)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)  # 128x128\n",
    "        \n",
    "        # Swin Transformer at downsampled stage\n",
    "        self.swin_unet_e1 = SwinTransformerBlock(embed_dim=24, depths=[2, 2], num_heads=[3, 6], window_size=8, img_size=128)\n",
    "\n",
    "        self.conv2_1 = SeparableConv2d(24, 48, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2_1 = nn.BatchNorm2d(48)\n",
    "        self.conv2_2 = SeparableConv2d(48, 48, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2_2 = nn.BatchNorm2d(48)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)  # 64x64\n",
    "        \n",
    "        self.swin_unet_e2 = SwinTransformerBlock(embed_dim=48, depths=[2, 2], num_heads=[6, 12], window_size=8,img_size=64)\n",
    "\n",
    "        self.conv3_1 = SeparableConv2d(48, 96, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn3_1 = nn.BatchNorm2d(96)\n",
    "        self.conv3_2 = SeparableConv2d(96, 96, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn3_2 = nn.BatchNorm2d(96)\n",
    "        self.drop3 = nn.Dropout2d(0.5)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)  # 32x32\n",
    "\n",
    "        # Skip connection to Swin Transformer\n",
    "        self.swin_unet_e3 = SwinTransformerBlock(embed_dim=96, depths=[2, 2], num_heads=[12, 24], window_size=8,img_size=32)\n",
    "\n",
    "        # Dense blocks\n",
    "        self.conv4_1 = SeparableConv2d(96, 192, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn4_1 = nn.BatchNorm2d(192)\n",
    "        self.conv4_2 = SeparableConv2d(192, 192, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn4_2 = nn.BatchNorm2d(192)\n",
    "        self.drop4_1 = nn.Dropout2d(0.5)\n",
    "\n",
    "        # Deconvergence Path (Upsampling)\n",
    "        self.up6 = nn.ConvTranspose2d(192, 96, kernel_size=2, stride=2)  # Up to 64x64\n",
    "        self.bn6 = nn.BatchNorm2d(96)\n",
    "\n",
    "        self.up7 = nn.ConvTranspose2d(96, 48, kernel_size=2, stride=2)  # Up to 128x128\n",
    "        self.bn7 = nn.BatchNorm2d(48)\n",
    "\n",
    "        self.up8 = nn.ConvTranspose2d(48, 24, kernel_size=2, stride=2)  # Up to 256x256\n",
    "        self.bn8 = nn.BatchNorm2d(24)\n",
    "\n",
    "        # Final convolutions\n",
    "        self.conv_final = nn.Conv2d(24, 1, kernel_size=1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Convergence Path\n",
    "        conv1 = F.relu(self.bn1_1(self.conv1_1(x)))\n",
    "        conv1 = F.relu(self.bn1_2(self.conv1_2(conv1)))\n",
    "        pool1 = self.pool1(conv1)\n",
    "        print(\"After pool1:\", pool1.shape)\n",
    "\n",
    "        swin1 = self.swin_unet_e1(pool1)\n",
    "\n",
    "        # Adjust channels and spatial size\n",
    "        swin1 = nn.Conv2d(48, 24, kernel_size=1)(swin1)  # Reduce channels to 24\n",
    "        swin1 = F.interpolate(swin1, size=(128, 128), mode='bilinear', align_corners=False)  # Resize to match expected spatial size\n",
    "        print(\"After swin_unet_e1:\", swin1.shape)\n",
    "\n",
    "        conv2 = F.relu(self.bn2_1(self.conv2_1(swin1)))\n",
    "        conv2 = F.relu(self.bn2_2(self.conv2_2(conv2)))\n",
    "        pool2 = self.pool2(conv2)\n",
    "        print(\"After pool2:\", pool2.shape)\n",
    "\n",
    "        swin2 = self.swin_unet_e2(pool2)\n",
    "\n",
    "        conv3 = F.relu(self.bn3_1(self.conv3_1(swin2)))\n",
    "        conv3 = F.relu(self.bn3_2(self.conv3_2(conv3)))\n",
    "        drop3 = self.drop3(conv3)\n",
    "        pool3 = self.pool3(drop3)\n",
    "\n",
    "        swin3 = self.swin_unet_e3(pool3)\n",
    "\n",
    "        # Dense block at bottom of network\n",
    "        conv4 = F.relu(self.bn4_1(self.conv4_1(swin3)))\n",
    "        dense_output = self.drop4_1(F.relu(self.bn4_2(self.conv4_2(conv4))))\n",
    "\n",
    "        # Deconvergence Path\n",
    "        up6 = F.relu(self.bn6(self.up6(dense_output) + conv3))  # Skip connection with conv3\n",
    "        up7 = F.relu(self.bn7(self.up7(up6) + conv2))           # Skip connection with conv2\n",
    "        up8 = F.relu(self.bn8(self.up8(up7) + conv1))           # Skip connection with conv1\n",
    "\n",
    "        # Final Convolution\n",
    "        out = self.sigmoid(self.conv_final(up8))\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "# Instantiate and test the model\n",
    "model = TBConvLNet()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Create a dummy input tensor with the expected input size\n",
    "dummy_input = torch.randn(1, 3, 256, 256).to(device)\n",
    "\n",
    "# Perform a forward pass\n",
    "output = model(dummy_input)\n",
    "print(\"Output shape:\", output.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset Class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, image_paths, mask_paths, transform=None, target_transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.mask_paths = mask_paths\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.image_paths[idx]).convert('RGB')\n",
    "        mask = Image.open(self.mask_paths[idx]).convert('L')\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        if self.target_transform:\n",
    "            mask = self.target_transform(mask)\n",
    "        else:\n",
    "            mask = transforms.ToTensor()(mask)\n",
    "\n",
    "        return img, mask\n",
    "\n",
    "# Image and Mask Paths\n",
    "def get_image_mask_paths(path):\n",
    "    images_dir = os.path.join(path, 'x')\n",
    "    masks_dir = os.path.join(path, 'y')\n",
    "\n",
    "    image_paths = sorted([os.path.join(images_dir, f) for f in os.listdir(images_dir)])\n",
    "    mask_paths = sorted([os.path.join(masks_dir, f) for f in os.listdir(masks_dir)])\n",
    "\n",
    "    return image_paths, mask_paths\n",
    "\n",
    "# Define Transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "target_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Get Data Paths\n",
    "train_data_path = '/path/to/train'\n",
    "valid_data_path = '/path/to/valid'\n",
    "\n",
    "train_image_paths, train_mask_paths = get_image_mask_paths(train_data_path)\n",
    "valid_image_paths, valid_mask_paths = get_image_mask_paths(valid_data_path)\n",
    "\n",
    "# Create Datasets\n",
    "train_dataset = CustomDataset(train_image_paths, train_mask_paths, transform=transform, target_transform=target_transform)\n",
    "valid_dataset = CustomDataset(valid_image_paths, valid_mask_paths, transform=transform, target_transform=target_transform)\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 4\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Dice Loss\n",
    "def dice_loss(pred, target, smooth=1.):\n",
    "    pred = pred.contiguous()\n",
    "    target = target.contiguous()\n",
    "\n",
    "    intersection = (pred * target).sum(dim=2).sum(dim=2)\n",
    "    loss = 1 - ((2. * intersection + smooth) / (pred.sum(dim=2).sum(dim=2) + target.sum(dim=2).sum(dim=2) + smooth))\n",
    "    return loss.mean()\n",
    "\n",
    "# Define Optimizer and Loss Function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = dice_loss\n",
    "\n",
    "# Training Loop\n",
    "num_epochs = 100\n",
    "best_loss = float('inf')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for images, masks in train_loader:\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item() * images.size(0)\n",
    "\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    valid_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, masks in valid_loader:\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "\n",
    "            valid_loss += loss.item() * images.size(0)\n",
    "\n",
    "    valid_loss /= len(valid_loader.dataset)\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {valid_loss:.4f}')\n",
    "\n",
    "    # Save Best Model\n",
    "    if valid_loss < best_loss:\n",
    "        best_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "        print('Model saved.')\n",
    "\n",
    "# Load Best Model\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation Function\n",
    "def evaluate_metrics(y_true, y_pred):\n",
    "    n = y_pred.shape[0]\n",
    "    all_accuracy = np.zeros(n)\n",
    "    all_dice = np.zeros(n)\n",
    "    all_jaccard = np.zeros(n)\n",
    "    all_sensitivity = np.zeros(n)\n",
    "    all_specificity = np.zeros(n)\n",
    "\n",
    "    for i in range(n):\n",
    "        gt = y_true[i].cpu().numpy().flatten()\n",
    "        pred = y_pred[i].cpu().numpy().flatten()\n",
    "\n",
    "        precisions, recalls, thresholds = precision_recall_curve(gt, pred)\n",
    "        f1 = 2 * (precisions * recalls) / (precisions + recalls + 1e-8)\n",
    "        max_value = np.argmax(f1)\n",
    "        thres = thresholds[max_value]\n",
    "        pred_label = (pred >= thres).astype(np.uint8)\n",
    "\n",
    "        tn, fp, fn, tp = confusion_matrix(gt, pred_label).ravel()\n",
    "\n",
    "        accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "        iou = tp / (tp + fp + fn + 1e-8)\n",
    "        dice = 2 * tp / (2 * tp + fp + fn + 1e-8)\n",
    "        specificity = tn / (tn + fp + 1e-8)\n",
    "        recall = tp / (tp + fn + 1e-8)\n",
    "\n",
    "        all_accuracy[i] = accuracy\n",
    "        all_dice[i] = dice\n",
    "        all_jaccard[i] = iou\n",
    "        all_sensitivity[i] = recall\n",
    "        all_specificity[i] = specificity\n",
    "\n",
    "    print('Accuracy: {:4f}, Dice: {:4f}, Jaccard: {:4f}, Sensitivity: {:4f}, Specificity: {:4f}'.format(\n",
    "        np.nanmean(all_accuracy), np.nanmean(all_dice), np.nanmean(all_jaccard), np.nanmean(all_sensitivity), np.nanmean(all_specificity)\n",
    "    ))\n",
    "    return all_accuracy, all_dice, all_jaccard, all_sensitivity, all_specificity\n",
    "\n",
    "# Evaluation on Validation Set\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_masks = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, masks in valid_loader:\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        all_preds.append(outputs)\n",
    "        all_masks.append(masks)\n",
    "\n",
    "all_preds = torch.cat(all_preds, dim=0)\n",
    "all_masks = torch.cat(all_masks, dim=0)\n",
    "\n",
    "evl = evaluate_metrics(all_masks, all_preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Predicted Masks\n",
    "output_dir = '/path/to/save/segmentations'\n",
    "gt_dir = '/path/to/save/GT'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "os.makedirs(gt_dir, exist_ok=True)\n",
    "\n",
    "all_preds_np = all_preds.cpu().numpy()\n",
    "all_masks_np = all_masks.cpu().numpy()\n",
    "\n",
    "for i in range(all_preds_np.shape[0]):\n",
    "    pred_mask = all_preds_np[i, 0]\n",
    "    gt_mask = all_masks_np[i, 0]\n",
    "\n",
    "    plt.imsave(os.path.join(output_dir, f\"{i+1}.png\"), pred_mask, cmap='gray')\n",
    "    plt.imsave(os.path.join(gt_dir, f\"{i+1}.png\"), gt_mask, cmap='gray')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
