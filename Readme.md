# Potatoes Medical Image Segmenter

## **Project Overview**

The Potatoes Medical Image Segmenter aims to segment medical images to identify and generate masks surrounding skin lesions. It leverages advanced deep learning architectures for precise image segmentation.

---

## **Project Architecture**

### **AI/**

Contains the core scripts and dependencies for AI-related functionalities:

- `.dockerignore`: Lists files/folders to be ignored by Docker during build.
- `app.py`: Main script for running AI-related functions.
- `Dockerfile`: Defines the Docker environment for the AI service.
- `requirement.txt`: Lists Python dependencies required for the AI component.

### **Dataset_setup/**

Handles dataset preparation tasks, either inside or outside Docker:

- `data_setup.py`: Script for dataset preparation.
- `requirements.txt`: Lists dependencies for dataset setup.

### **ML/**

Includes scripts and files for training machine learning models.

- `MedsegDiff/`: root folder of your project related to medical image segmentation using diffusion models. Contains subfolders `guided_diffusion`, `scripts`.
  - `guided_diffusion/`: hold the core implementation for diffusion models, including utilities, loaders, and training scripts.
    - `btcvloader.py`: Loading the BTCV dataset or related data.
    - `gaussian_diffusion.py`: Implements the Gaussian diffusion process for the model.
    - `isicloader.py`: For loading ISIC dataset images (a dataset for skin lesion analysis).
    - `train_util.py`: Utility functions or classes for training the model.
    - `unet.py`: The implementation of the U-Net model, possibly adapted for diffusion tasks.
    - `losses.py`: Custom loss functions for training.
    - `logger.py`: Handles logging during training and testing.
    - `utils.py`: Generic utility functions used throughout the project.
    - `resample.py`: Handles sampling and resampling in the diffusion process.
    - `respacing.py`: Manages custom timestep schedules for diffusion.
    - `script_util.py`: Provides helper functions for scripting tasks.
    - `dpm_solver.py`: Implements solvers for accelerated diffusion steps.
    - `fp16_util.py`: Utilities for mixed-precision (FP16) training.
    - `nn.py`: Custom neural network layers and utilities.
  - `scripts/`: This directory seems to include high-level scripts for tasks related to segmentation.
    - `segmentation_env.py`: Likely sets up the environment or configuration for segmentation tasks.
    - `segmentation_sample.py`: A script for sampling segmented images using the trained diffusion model.
    - `segmentation_train.py`: Handles the training of the segmentation model.
    - `segmentation_env_PerClass.py`: Could be specialized for per-class segmentation tasks or metrics.
- `TbConvL-Net/`: Contains files related to the TBConvL-Net model, which might be integrated into your segmentation or diffusion architecture.
  - `evaluate.py`: Script for evaluating the TBConvL-Net model’s performance.
  - `SegmentationDataset.py`: Custom dataset class for handling segmentation-specific data.
  - `segmentation.py`: Script for sampling the TBConvL-Net model’s with respective ground truth images.
  - `SwinUnet.py`: Likely the implementation of the Swin Transformer U-Net.
  - `train.py`: Script for training TBConvL-Net.
  - `TBConvLet-Net PyTorch.ipynb`: A Jupyter notebook for experimenting with the TBConvL-Net in PyTorch.

### **shared/**

Shared directory for data, logs, models, and results:

- `data/`: Stores datasets as configured via the UI.
- `logs/`: Contains log files for application and model activities.
- `models/`: Stores model-related data:
  - `{model_name}/`: Each model has a dedicated folder.
    - `model/`: Contains trained model files.
    - `sampled/`: Contains images generated by the model.
- `result/`: Stores output/results generated from predictions.

### **Web/**

Files related to the web interface:

- **`app/`**: Backend logic for the web application:
  - `data_setup.py`: Handles data setup operations.
  - `routes.py`: Defines web app routes and endpoints.
  - `utils.py`: Provides utility functions.
- **`static/`**: Contains static files (CSS, JavaScript, and images):
  - `css/main.css`: Stylesheets for the web interface.
  - `images/`: Stores web app images (e.g., backgrounds, favicons).
- **`templates/`**: HTML templates for the web interface:
  - `index.html`: Main landing page.
  - `img_upload.html`: Image upload page.
  - `folder_upload.html`: Folder upload page.
  - `data_setup.html`: Data Setup page.
  - `result.html`: Result page.
- **Other files:**
  - `.dockerignore`: Lists ignored files/folders for Docker.
  - `Dockerfile`: Docker configuration for the web app.
  - `main.py`: Entry point for the web application.
  - `requirements.txt`: Dependencies for the web application.

### **Root Files**

- `.dockerignore`: Global Docker ignore file.
- `.env`: Contains environment variables for configuration.
- `.gitattributes`: Git attribute settings.
- `.gitignore`: Specifies ignored files/folders for Git.
- `docker-compose.yml`: Defines multi-container Docker application (connects web and AI services).
- `Readme.md`: Project documentation.

---

## **Models**

### **Trained Models**

The models were trained on the ISIC 2018 dataset:

- **MedSegDiff-v2**: A state-of-the-art model for medical image segmentation.
  - [Paper](https://arxiv.org/abs/2301.11798)
- **TBConvL-Net**: A robust model for segmentation tasks.
  - [Paper](https://www.sciencedirect.com/science/article/abs/pii/S0031320324007799)

### **Dataset Source**

[ISIC 2018 Dataset](https://challenge.isic-archive.com/data/#2018)

### **Model Handling**

- Pre-sampled images for MedSegDiff-v2 are stored in `shared/models/MedSegDiffv2/sampled` due to high processing requirements.
- New datasets/images will undergo fresh sampling.

---

## **Model Training, Sampling and Evaluation**

### Medsegdiff

1. Navigate to the `ML/` folder:
   ```bash
   cd ML
   ```
2. Install required dependencies:
   ```bash
   pip install -r requirements.txt
   ```
3. Navigate to the `ML/MedsegDiff/scripts` folder:
   ```bash
   cd scripts
   ```
4. Train the model using following terminal command line:
   ```bash
   python ML/Medsegdiff/scripts/segmentation_train.py --data_name ISIC --data_dir /dataset --out_dir ML/Medsegdiff/output --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 12  --lr_anneal_steps 50000 --save_interval 1000 --log_interval 500 --multi_gpu 2
   ```
5. Sampling the trained model using command line:
   ```bash
   python ML/Medsegdiff/scripts/segmentation_sample.py --data_name ISIC --data_dir /dataset --out_dir ML/Medsegdiff/output/segmentation/ --model_path ML/Medsegdiff/output/emasavedmodel_0.9999_050000.pt --image_size 256 --num_channels 512 --class_cond False --num_res_blocks 12 --num_heads 8 --learn_sigma True --use_scale_shift_norm True --attention_resolutions 24 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas True --rescale_timesteps True --num_ensemble 5 --dpm_solver True --multi_gpu 2
   ```
6. Evaluation:
   ```bash
   python ML/Medsegdiff/scripts/segmentation_env.py --inp_pth  ML/Medsegdiff/output/segmentation --out_pth dataset/ISIC2018_Task1_Validation_GroundTruth
   ```

### TbConvL-Net

1. Navigate to the `ML/` folder:
   ```bash
   cd ML
   ```
2. Install required dependencies:
   ```bash
   pip install -r requirements.txt
   ```
3. Navigate to the `ML/TbConvL-Net` folder:
   ```bash
   cd TbConvL-Net
   ```
4. Train the model using following terminal command line:
   ```bash
   python train.py --train_images_dir 'dataset/ISIC2018_Task1-2_Training_Input' --train_masks_dir 'dataset/ISIC2018_Task1_Training_GroundTruth' --save_model_path 'ML/TbConvL-Net/model/trainedmodel.pth' --batch_size 24  --learning_rate 0.0001  --num_epochs 100
   ```
5. Model sampling:
   ```bash
   python segmentation.py --model_pth 'ML/TbConvL-Net/model/trainedmodel.pth' --test_images_dir 'dataset/ISIC2018_Task1-2_Validation_Input' --test_masks_dir 'dataset/ISIC2018_Task1_Validation_GroundTruth' --save_dir_pred 'ML/TbConvL-Net/output/segmented predicted images' --save_dir_gt 'ML/TbConvL-Net/output/segmented ground truth'
   ```
6. Evaluation:
   ```bash
   python evaluate.py --test_images_dir 'dataset/ISIC2018_Task1-2_Validation_Input' --test_masks_dir 'dataset/ISIC2018_Task1_Validation_GroundTruth' --model_pth 'ML/TbConvL-Net/model/trainedmodel.pth'
   ```

---

## **Dataset Setup**

You can download the sample data from [Sample Data](https://unsw-my.sharepoint.com/:f:/g/personal/z5445071_ad_unsw_edu_au/Em32wh9uRHhOgPMbT1qhfCMBZYw9jWQGnNOxMy8imSUE3g?e=Qde6R6).

### Method 1: Outside Docker

1. Navigate to the `Dataset_setup` folder:
   ```bash
   cd Dataset_setup/
   ```
2. Install required dependencies:
   ```bash
   pip install -r requirements.txt
   ```
3. Run the data setup script:
   ```bash
   python data_setup.py
   ```

### Method 2: Inside Docker

1. Set up the dataset through the web interface:
   - Navigate to: [http://127.0.0.1:8000/datasetup](http://127.0.0.1:8000/datasetup)

---

## **Run Instructions**

1. Clone the repository:
   ```bash
   git clone https://github.com/unsw-cse-comp99-3900/capstone-project-2024-t3-9900t15apotatoes.git
   ```
2. Start Docker on your system.
3. Reset Docker to free used ports:
   ```bash
   docker system prune -a --volumes
   ```
4. Set `PORT` and `HOST` in the `.env` file.
5. Download the trained models and place them in `shared/models/`.
   [Download Models](https://unsw-my.sharepoint.com/:f:/g/personal/z5445071_ad_unsw_edu_au/EgPtT5GxVFtOqbRYOh0MUEIBGz5QsKHNUygqVuDO_FxTXQ?e=XS6KZ9)
6. Build and start the Docker containers:
   ```bash
   docker-compose up --build
   ```
7. Access the application in your browser:  
   [http://127.0.0.1:8000](http://127.0.0.1:8000)
8. Stop Docker containers:
   ```bash
   docker-compose down
   ```

---

## **Additional Information**

- **Training Environment**: The MedSegDiff-v2 model was trained using the UNSW Katana supercomputer. utilizing high-performance hardware and resources.
- **GPU Specifications**: Katana provided access to 2x NVIDIA Tesla V100-SXM2 GPUs, each with 32GB of VRAM. These GPUs are optimized for large-scale deep learning tasks, enabling efficient parallel computations for model training and sampling.
- **Pre-sampled Images**: For MedSegDiff-v2, Pre-sampled images for the ISIC dataset are provided as part of the project. These images are ready for evaluation without requiring additional sampling.
- **Training Environment**: The MedSegDiff-v2 model was trained using the UNSW Katana supercomputer. utilizing high-performance hardware and resources.
- **GPU Specifications**: Katana provided access to 2x NVIDIA Tesla V100-SXM2 GPUs, each with 32GB of VRAM. These GPUs are optimized for large-scale deep learning tasks, enabling efficient parallel computations for model training and sampling.
- **Pre-sampled Images**: For MedSegDiff-v2, Pre-sampled images for the ISIC dataset are provided as part of the project. These images are ready for evaluation without requiring additional sampling.

- **Performance Tuning**: Performance of the MedSegDiff-v2 model can be improved based on the system performance by adjusting/modifying the argument (e.g., `multi_gpu`, `dpm_solver`) in `/sample` in `AI/app.py` and `segmentation_sample.py` in `shared/models/MedSegDiffv2/segmentation_sample.py`.

- **Training Time**: Training time depends on the dataset size, hyperparameter configuration, and available resources.For training on the ISIC dataset (256x256 image resolution) with the MedSegDiff-v2 architecture training time would be ~12–24 hours per model, utilizing both Tesla V100 GPUs in parallel. We have adjusted batch Size to 18 (optimal for GPU memory usage).
- **Performance Tuning**: Performance of the MedSegDiff-v2 model can be improved based on the system performance by adjusting/modifying the argument (e.g., `multi_gpu`, `dpm_solver`) in `/sample` in `AI/app.py` and `segmentation_sample.py` in `shared/models/MedSegDiffv2/segmentation_sample.py`.

- **Training Time**: Training time depends on the dataset size, hyperparameter configuration, and available resources.For training on the ISIC dataset (256x256 image resolution) with the MedSegDiff-v2 architecture training time would be ~12–24 hours per model, utilizing both Tesla V100 GPUs in parallel. We have adjusted batch Size to 18 (optimal for GPU memory usage).

### **Adding New Models**

To add new models, follow these steps:

1. Place the model folder under `shared/models`.
2. Place the pre-trained model file (e.g., `.pt`, `.pth`) in `shared/models/{model folder}/model`.
3. Modify `/sample` and `/evaluation` based on your code in `AI/app.py`.

---

## **Contributors**

This project was developed as part of UNSW COMP9900.
