{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q89EJdGXhDCj",
        "outputId": "c346eb70-92dc-4673-f59e-1b455e09dd11"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "# Mount the Google Drive at /content/drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "eMci0igOtRnD"
      },
      "outputs": [],
      "source": [
        "# !pip install -r /content/drive/MyDrive/ML/requirement.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bjLkgDgJboEo"
      },
      "outputs": [],
      "source": [
        "# !pip install py-spy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !python /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Xofp_A5JFzE7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HiTuFoHJOShH"
      },
      "source": [
        "Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aj4UGuTHu5IS",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# !python /content/drive/MyDrive/ML/scripts/segmentation_train.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --lr 1e-4 --batch_size 4 --lr_anneal_steps 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ioPPz6VpOV5I"
      },
      "source": [
        "segmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XiHBHP5dOXAi",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# !python /content/drive/MyDrive/ML/scripts/segmentation_sample.py --data_name ISIC --data_dir /content/drive/MyDrive/ML/dataset --out_dir /content/drive/MyDrive/ML/output/Test/segmentation --model_path /content/drive/MyDrive/ML/output/Test/emasavedmodel_0.9999_000002.pt --image_size 256 --num_channels 128 --class_cond False --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16 --diffusion_steps 100 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --num_ensemble 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "khPxUystzWv0"
      },
      "outputs": [],
      "source": [
        "# !python /content/drive/MyDrive/ML/scripts/segmentation_env.py --inp_pth /content/drive/MyDrive/ML/output/Test/segmentation --out_pth /content/drive/MyDrive/ML/dataset/ISIC2018_Task1_Validation_GroundTruth\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WaC-dKUFIClx",
        "outputId": "e428dc24-5847-4cfc-eaab-feaa1a08731d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/functional.py:534: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3595.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# ============================== Imports and Dependencies ==============================\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from zipfile import ZipFile\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_recall_curve, confusion_matrix\n",
        "\n",
        "# ================================ Separable Convolution =================================\n",
        "\n",
        "class SeparableConv2d(nn.Module):\n",
        "    \"\"\"\n",
        "    Implements a separable convolution layer using depthwise and pointwise convolutions.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, padding=1, bias=True):\n",
        "        super(SeparableConv2d, self).__init__()\n",
        "        # Depthwise convolution (groups=in_channels)\n",
        "        self.depthwise = nn.Conv2d(in_channels, in_channels, kernel_size=kernel_size,\n",
        "                                   padding=padding, groups=in_channels, bias=bias)\n",
        "        # Pointwise convolution\n",
        "        self.pointwise = nn.Conv2d(in_channels, out_channels, kernel_size=1,\n",
        "                                   padding=0, bias=bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.depthwise(x)\n",
        "        x = self.pointwise(x)\n",
        "        return x\n",
        "\n",
        "# ================================== ConvLSTM2D ========================================\n",
        "\n",
        "class ConvLSTMCell(nn.Module):\n",
        "    \"\"\"\n",
        "    Implements a ConvLSTM cell.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_channels, hidden_channels, kernel_size, bias=True):\n",
        "        super(ConvLSTMCell, self).__init__()\n",
        "\n",
        "        padding = kernel_size // 2  # To maintain spatial dimensions\n",
        "        self.input_channels = input_channels\n",
        "        self.hidden_channels = hidden_channels\n",
        "\n",
        "        self.conv = nn.Conv2d(in_channels=input_channels + hidden_channels,\n",
        "                              out_channels=4 * hidden_channels,\n",
        "                              kernel_size=kernel_size,\n",
        "                              padding=padding,\n",
        "                              bias=bias)\n",
        "\n",
        "    def forward(self, input_tensor, cur_state):\n",
        "        h_cur, c_cur = cur_state\n",
        "\n",
        "        # Concatenate input and hidden state\n",
        "        combined = torch.cat([input_tensor, h_cur], dim=1)  # along channel axis\n",
        "\n",
        "        # Compute all gates at once\n",
        "        conv_output = self.conv(combined)\n",
        "        cc_i, cc_f, cc_o, cc_g = torch.split(conv_output, self.hidden_channels, dim=1)\n",
        "\n",
        "        i = torch.sigmoid(cc_i)   # input gate\n",
        "        f = torch.sigmoid(cc_f)   # forget gate\n",
        "        o = torch.sigmoid(cc_o)   # output gate\n",
        "        g = torch.tanh(cc_g)      # gate gate\n",
        "\n",
        "        c_next = f * c_cur + i * g\n",
        "        h_next = o * torch.tanh(c_next)\n",
        "\n",
        "        return h_next, c_next\n",
        "\n",
        "    def init_hidden(self, batch_size, spatial_size, device):\n",
        "        height, width = spatial_size\n",
        "        return (torch.zeros(batch_size, self.hidden_channels, height, width, device=device),\n",
        "                torch.zeros(batch_size, self.hidden_channels, height, width, device=device))\n",
        "\n",
        "class ConvLSTM2D(nn.Module):\n",
        "    \"\"\"\n",
        "    Implements a ConvLSTM2D layer that processes a sequence of inputs.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_channels, hidden_channels, kernel_size=3, bias=True, num_layers=1):\n",
        "        super(ConvLSTM2D, self).__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.hidden_channels = hidden_channels\n",
        "\n",
        "        layers = []\n",
        "        for i in range(num_layers):\n",
        "            input_c = input_channels if i == 0 else hidden_channels\n",
        "            layers.append(ConvLSTMCell(input_c, hidden_channels, kernel_size, bias))\n",
        "        self.layers = nn.ModuleList(layers)\n",
        "\n",
        "    def forward(self, input_tensor):\n",
        "        # input_tensor shape: (batch, seq_len, channels, height, width)\n",
        "        batch_size, seq_len, channels, height, width = input_tensor.size()\n",
        "        device = input_tensor.device\n",
        "\n",
        "        # Initialize hidden and cell states for all layers\n",
        "        hidden_state = []\n",
        "        cell_state = []\n",
        "        for i in range(self.num_layers):\n",
        "            h, c = self.layers[i].init_hidden(batch_size, (height, width), device)\n",
        "            hidden_state.append(h)\n",
        "            cell_state.append(c)\n",
        "\n",
        "        # Iterate over time steps\n",
        "        for t in range(seq_len):\n",
        "            x = input_tensor[:, t, :, :, :]  # (batch, channels, height, width)\n",
        "            for i, layer in enumerate(self.layers):\n",
        "                h, c = layer(x, (hidden_state[i], cell_state[i]))\n",
        "                hidden_state[i] = h\n",
        "                cell_state[i] = c\n",
        "                x = h  # input to next layer\n",
        "        return x  # Return the hidden state of the last layer\n",
        "\n",
        "# ============================== Swin Transformer Blocks ================================\n",
        "\n",
        "class WindowAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Window based multi-head self attention (W-MSA) module with relative position bias.\n",
        "    \"\"\"\n",
        "    def __init__(self, dim, window_size, num_heads, qkv_bias=True, attn_drop=0., proj_drop=0.):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            dim (int): Number of input channels.\n",
        "            window_size (tuple): Height and width of the window.\n",
        "            num_heads (int): Number of attention heads.\n",
        "            qkv_bias (bool): If True, add a learnable bias to query, key, value.\n",
        "            attn_drop (float): Dropout ratio of attention weights.\n",
        "            proj_drop (float): Dropout ratio of output.\n",
        "        \"\"\"\n",
        "        super(WindowAttention, self).__init__()\n",
        "        self.dim = dim\n",
        "        self.window_size = window_size  # Wh, Ww\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // num_heads\n",
        "        self.scale = head_dim ** -0.5\n",
        "\n",
        "        # Define a parameter table of relative position bias\n",
        "        self.relative_position_bias_table = nn.Parameter(\n",
        "            torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads)\n",
        "        )  # 2*Wh-1 * 2*Ww-1, nH\n",
        "\n",
        "        # Get pair-wise relative position index for each token inside the window\n",
        "        coords_h = torch.arange(self.window_size[0])\n",
        "        coords_w = torch.arange(self.window_size[1])\n",
        "        coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n",
        "        coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n",
        "        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n",
        "        relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n",
        "        relative_coords[:, :, 0] += self.window_size[0] - 1  # shift to start from 0\n",
        "        relative_coords[:, :, 1] += self.window_size[1] - 1\n",
        "        relative_coords[:, :, 0] *= (2 * self.window_size[1] - 1)\n",
        "        relative_position_index = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n",
        "        self.register_buffer(\"relative_position_index\", relative_position_index)\n",
        "\n",
        "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)  # Query, Key, Value\n",
        "        self.attn_drop = nn.Dropout(attn_drop)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.proj_drop = nn.Dropout(proj_drop)\n",
        "\n",
        "        # Initialize relative position bias table\n",
        "        nn.init.trunc_normal_(self.relative_position_bias_table, std=.02)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: input features with shape of (num_windows*B, Wh*Ww, C)\n",
        "            mask: (num_windows, Wh*Ww, Wh*Ww) or None\n",
        "        \"\"\"\n",
        "        B_, N, C = x.shape\n",
        "        qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)  # 3, B_, nH, N, C//nH\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]  # each has shape (B_, nH, N, C//nH)\n",
        "\n",
        "        q = q * self.scale\n",
        "        attn = (q @ k.transpose(-2, -1))  # (B_, nH, N, N)\n",
        "\n",
        "        # Add relative position bias\n",
        "        relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(\n",
        "            self.window_size[0] * self.window_size[1], self.window_size[0] * self.window_size[1], -1\n",
        "        )  # Wh*Ww, Wh*Ww, nH\n",
        "        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n",
        "        attn = attn + relative_position_bias.unsqueeze(0)  # (B_, nH, N, N)\n",
        "\n",
        "        if mask is not None:\n",
        "            nW = mask.shape[0]\n",
        "            attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)\n",
        "            attn = attn.view(-1, self.num_heads, N, N)\n",
        "            attn = F.softmax(attn, dim=-1)\n",
        "        else:\n",
        "            attn = F.softmax(attn, dim=-1)\n",
        "\n",
        "        attn = self.attn_drop(attn)\n",
        "\n",
        "        out = (attn @ v).transpose(1, 2).reshape(B_, N, C)  # (B_, N, C)\n",
        "        out = self.proj(out)\n",
        "        out = self.proj_drop(out)\n",
        "        return out\n",
        "\n",
        "class SwinTransformerBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Swin Transformer Block with W-MSA and SW-MSA.\n",
        "    \"\"\"\n",
        "    def __init__(self, dim, num_heads, window_size=7, shift_size=0, mlp_ratio=4., qkv_bias=True,\n",
        "                 attn_drop=0., proj_drop=0.):\n",
        "        super(SwinTransformerBlock, self).__init__()\n",
        "        self.dim = dim\n",
        "        self.num_heads = num_heads\n",
        "        self.window_size = window_size  # W\n",
        "        self.shift_size = shift_size    # S\n",
        "        self.mlp_ratio = mlp_ratio\n",
        "\n",
        "        assert 0 <= self.shift_size < self.window_size, \"shift_size must be in [0, window_size)\"\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(dim)\n",
        "        self.attn = WindowAttention(dim, (window_size, window_size), num_heads, qkv_bias, attn_drop, proj_drop)\n",
        "\n",
        "        self.drop_path = nn.Identity()  # Can implement stochastic depth if desired\n",
        "        self.norm2 = nn.LayerNorm(dim)\n",
        "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(dim, mlp_hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(mlp_hidden_dim, dim),\n",
        "            nn.Dropout(proj_drop)\n",
        "        )\n",
        "\n",
        "        if self.shift_size > 0:\n",
        "            # Shift the window by shift_size\n",
        "            self.shift_partition = True\n",
        "        else:\n",
        "            self.shift_partition = False\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: input features with shape (B, H*W, C)\n",
        "        \"\"\"\n",
        "        H = W = int(np.sqrt(x.shape[1]))\n",
        "        B, L, C = x.shape\n",
        "        assert L == H * W, \"Input feature has wrong size\"\n",
        "\n",
        "        shortcut = x\n",
        "        x = self.norm1(x)\n",
        "        x = x.view(B, H, W, C)\n",
        "\n",
        "        # Cyclic shift\n",
        "        if self.shift_size > 0:\n",
        "            shifted_x = torch.roll(x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2))\n",
        "        else:\n",
        "            shifted_x = x\n",
        "\n",
        "        # Partition windows\n",
        "        window_size = self.window_size\n",
        "        # Pad H and W to be multiples of window_size\n",
        "        pad_b = (window_size - H % window_size) % window_size\n",
        "        pad_r = (window_size - W % window_size) % window_size\n",
        "        shifted_x = F.pad(shifted_x, (0, 0, 0, pad_r, 0, pad_b))  # pad H and W\n",
        "        _, Hp, Wp, _ = shifted_x.shape\n",
        "\n",
        "        # Window partition\n",
        "        x_windows = shifted_x.view(B, Hp // window_size, window_size, Wp // window_size, window_size, C)\n",
        "        x_windows = x_windows.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size * window_size, C)  # (num_windows*B, window_size*window_size, C)\n",
        "\n",
        "        # Attention\n",
        "        attn_windows = self.attn(x_windows)  # (num_windows*B, window_size*window_size, C)\n",
        "\n",
        "        # Merge windows\n",
        "        shifted_x = attn_windows.view(-1, window_size, window_size, C)\n",
        "        shifted_x = shifted_x.view(B, Hp // window_size, Wp // window_size, window_size, window_size, C)\n",
        "        shifted_x = shifted_x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, Hp, Wp, C)\n",
        "\n",
        "        # Reverse cyclic shift\n",
        "        if self.shift_size > 0:\n",
        "            x = torch.roll(shifted_x, shifts=(self.shift_size, self.shift_size), dims=(1, 2))\n",
        "        else:\n",
        "            x = shifted_x\n",
        "\n",
        "        # Remove padding\n",
        "        x = x[:, :H, :W, :].contiguous().view(B, H * W, C)\n",
        "\n",
        "        # FFN\n",
        "        x = shortcut + self.drop_path(attn_windows.view(B, H * W, C))\n",
        "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
        "\n",
        "        return x\n",
        "\n",
        "# =============================== Dice Loss Function ====================================\n",
        "\n",
        "class DiceLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Dice Loss function to maximize the Dice coefficient.\n",
        "    Suitable for binary segmentation tasks.\n",
        "    \"\"\"\n",
        "    def __init__(self, smooth=1.0):\n",
        "        super(DiceLoss, self).__init__()\n",
        "        self.smooth = smooth\n",
        "\n",
        "    def forward(self, y_pred, y_true):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            y_pred (torch.Tensor): Predicted mask probabilities with shape (B, 1, H, W)\n",
        "            y_true (torch.Tensor): Ground truth masks with shape (B, 1, H, W)\n",
        "        Returns:\n",
        "            torch.Tensor: Dice loss\n",
        "        \"\"\"\n",
        "        y_pred = y_pred.view(-1)\n",
        "        y_true = y_true.view(-1)\n",
        "\n",
        "        intersection = (y_pred * y_true).sum()\n",
        "        dice = (2. * intersection + self.smooth) / (y_pred.sum() + y_true.sum() + self.smooth)\n",
        "\n",
        "        return 1 - dice\n",
        "\n",
        "# ================================ Main Model ============================================\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class SwinUNet(nn.Module):\n",
        "    \"\"\"\n",
        "    Swin U-Net architecture for image segmentation.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_channels=3, output_channels=1,\n",
        "                 embed_dim=32, num_heads=[4, 8], window_size=4,\n",
        "                 mlp_ratio=4., depth=2):\n",
        "        super(SwinUNet, self).__init__()\n",
        "        self.input_channels = input_channels\n",
        "        self.output_channels = output_channels\n",
        "\n",
        "        # Initial convolutional layers\n",
        "        self.conv1 = SeparableConv2d(input_channels, 24, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(24)\n",
        "        self.conv2 = SeparableConv2d(24, 24, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(24)\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)  # 256x256 -> 128x128\n",
        "\n",
        "        # First Swin Transformer Block\n",
        "        self.swin_unet_E1 = SwinTransformerBlock(\n",
        "            dim=24,  # Changed from embed_dim=32 to 24\n",
        "            num_heads=num_heads[0],\n",
        "            window_size=window_size,\n",
        "            shift_size=window_size//2 if True else 0,\n",
        "            mlp_ratio=mlp_ratio\n",
        "        )\n",
        "\n",
        "        # Second convolutional block\n",
        "        self.conv3 = SeparableConv2d(24, 48, kernel_size=3, padding=1)  # Changed input from embed_dim=32 to 24\n",
        "        self.bn3 = nn.BatchNorm2d(48)\n",
        "        self.conv4 = SeparableConv2d(48, 48, kernel_size=3, padding=1)\n",
        "        self.bn4 = nn.BatchNorm2d(48)\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)  # 128x128 -> 64x64\n",
        "\n",
        "        # Second Swin Transformer Block\n",
        "        self.swin_unet_E2 = SwinTransformerBlock(\n",
        "            dim=48,  # Changed from embed_dim=32 to 48\n",
        "            num_heads=num_heads[1],\n",
        "            window_size=window_size,\n",
        "            shift_size=window_size//2 if True else 0,\n",
        "            mlp_ratio=mlp_ratio\n",
        "        )\n",
        "\n",
        "        # Third convolutional block (Bottleneck)\n",
        "        self.conv5 = SeparableConv2d(48, 96, kernel_size=3, padding=1)  # Changed input from embed_dim=32 to 48\n",
        "        self.bn5 = nn.BatchNorm2d(96)\n",
        "        self.conv6 = SeparableConv2d(96, 96, kernel_size=3, padding=1)\n",
        "        self.bn6 = nn.BatchNorm2d(96)\n",
        "        self.drop5 = nn.Dropout(0.5)\n",
        "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)  # 64x64 -> 32x32\n",
        "\n",
        "        # Bottleneck convolutions with dense connections\n",
        "        self.conv7 = SeparableConv2d(96, 192, kernel_size=3, padding=1)\n",
        "        self.bn7 = nn.BatchNorm2d(192)\n",
        "        self.conv8 = SeparableConv2d(192, 192, kernel_size=3, padding=1)\n",
        "        self.bn8 = nn.BatchNorm2d(192)\n",
        "        self.drop6_1 = nn.Dropout(0.5)\n",
        "\n",
        "        self.conv9 = SeparableConv2d(192, 192, kernel_size=3, padding=1)\n",
        "        self.bn9 = nn.BatchNorm2d(192)\n",
        "        self.conv10 = SeparableConv2d(192, 192, kernel_size=3, padding=1)\n",
        "        self.bn10 = nn.BatchNorm2d(192)\n",
        "        self.drop6_2 = nn.Dropout(0.5)\n",
        "\n",
        "        self.concat1 = nn.Sequential(\n",
        "            SeparableConv2d(384, 192, kernel_size=3, padding=1),\n",
        "            SeparableConv2d(192, 192, kernel_size=3, padding=1)\n",
        "        )\n",
        "        self.drop6_3 = nn.Dropout(0.5)\n",
        "\n",
        "        # First Upsampling Block\n",
        "        self.up1 = nn.ConvTranspose2d(192, 96, kernel_size=2, stride=2)  # 32x32 -> 64x64\n",
        "        self.bn_up1 = nn.BatchNorm2d(96)\n",
        "        self.relu_up1 = nn.ReLU(inplace=True)\n",
        "        self.convLSTM1 = ConvLSTM2D(input_channels=96, hidden_channels=384, kernel_size=3, num_layers=1)\n",
        "        self.swin_unet_D1 = SwinTransformerBlock(\n",
        "            dim=384,  # Changed from embed_dim=32 to 384\n",
        "            num_heads=num_heads[0],\n",
        "            window_size=window_size,\n",
        "            shift_size=window_size//2 if True else 0,\n",
        "            mlp_ratio=mlp_ratio\n",
        "        )\n",
        "        self.conv11 = SeparableConv2d(384, 48, kernel_size=3, padding=1)\n",
        "        self.conv12 = SeparableConv2d(48, 48, kernel_size=3, padding=1)\n",
        "\n",
        "        # Second Upsampling Block\n",
        "        self.up2 = nn.ConvTranspose2d(48, 48, kernel_size=2, stride=2)  # 64x64 -> 128x128\n",
        "        self.bn_up2 = nn.BatchNorm2d(48)\n",
        "        self.relu_up2 = nn.ReLU(inplace=True)\n",
        "        self.convLSTM2 = ConvLSTM2D(input_channels=48, hidden_channels=96, kernel_size=3, num_layers=1)\n",
        "        self.swin_unet_D2 = SwinTransformerBlock(\n",
        "            dim=96,  # Changed from embed_dim=32 to 96\n",
        "            num_heads=num_heads[1],\n",
        "            window_size=window_size,\n",
        "            shift_size=window_size//2 if True else 0,\n",
        "            mlp_ratio=mlp_ratio\n",
        "        )\n",
        "        self.conv13 = SeparableConv2d(96, 24, kernel_size=3, padding=1)\n",
        "        self.conv14 = SeparableConv2d(24, 24, kernel_size=3, padding=1)\n",
        "\n",
        "        # Third Upsampling Block\n",
        "        self.up3 = nn.ConvTranspose2d(24, 24, kernel_size=2, stride=2)  # 128x128 -> 256x256\n",
        "        self.bn_up3 = nn.BatchNorm2d(24)\n",
        "        self.relu_up3 = nn.ReLU(inplace=True)\n",
        "        self.convLSTM3 = ConvLSTM2D(input_channels=24, hidden_channels=48, kernel_size=3, num_layers=1)\n",
        "        self.swin_unet_D3 = SwinTransformerBlock(\n",
        "            dim=48,  # Changed from embed_dim=32 to 48\n",
        "            num_heads=num_heads[1],\n",
        "            window_size=window_size,\n",
        "            shift_size=window_size//2 if True else 0,\n",
        "            mlp_ratio=mlp_ratio\n",
        "        )\n",
        "        self.conv15 = SeparableConv2d(48, 24, kernel_size=3, padding=1)\n",
        "        self.conv16 = SeparableConv2d(24, 24, kernel_size=3, padding=1)\n",
        "\n",
        "        # Output Layer\n",
        "        self.final_conv1 = nn.Conv2d(24, 2, kernel_size=3, padding=1)\n",
        "        self.final_relu = nn.ReLU(inplace=True)\n",
        "        self.final_conv2 = nn.Conv2d(2, 1, kernel_size=1, padding=0)\n",
        "        self.final_sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass of the Swin U-Net model.\n",
        "        Args:\n",
        "            x: Input tensor with shape (B, 3, 256, 256)\n",
        "        Returns:\n",
        "            torch.Tensor: Output segmentation mask with shape (B, 1, 256, 256)\n",
        "        \"\"\"\n",
        "        # Initial Convolutions\n",
        "        x1 = self.conv1(x)          # (B, 24, 256, 256)\n",
        "        x1 = self.bn1(x1)\n",
        "        x1 = self.conv2(x1)         # (B, 24, 256, 256)\n",
        "        x1 = self.bn2(x1)\n",
        "        p1 = self.pool1(x1)         # (B, 24, 128, 128)\n",
        "\n",
        "        # First Swin Transformer Block\n",
        "        p1_flat = p1.flatten(2).transpose(1, 2)  # (B, 128*128, 24)\n",
        "        swin_E1 = self.swin_unet_E1(p1_flat)     # (B, 128*128, 24)\n",
        "        swin_E1 = swin_E1.transpose(1, 2).view(-1, 24, 128, 128)  # Reshape for Conv2d\n",
        "\n",
        "        # Second Convolutional Block\n",
        "        x2 = self.conv3(swin_E1)    # (B, 48, 128, 128)\n",
        "        x2 = self.bn3(x2)\n",
        "        x2 = self.conv4(x2)          # (B, 48, 128, 128)\n",
        "        x2 = self.bn4(x2)\n",
        "        p2 = self.pool2(x2)          # (B, 48, 64, 64)\n",
        "\n",
        "        # Second Swin Transformer Block\n",
        "        p2_flat = p2.flatten(2).transpose(1, 2)  # (B, 64*64, 48)\n",
        "        swin_E2 = self.swin_unet_E2(p2_flat)     # (B, 64*64, 48)\n",
        "        swin_E2 = swin_E2.transpose(1, 2).view(-1, 48, 64, 64)  # Reshape for Conv2d\n",
        "\n",
        "        # Third Convolutional Block (Bottleneck)\n",
        "        x3 = self.conv5(swin_E2)    # (B, 96, 64, 64)\n",
        "        x3 = self.bn5(x3)\n",
        "        x3 = self.conv6(x3)          # (B, 96, 64, 64)\n",
        "        x3 = self.bn6(x3)\n",
        "        x3 = self.drop5(x3)\n",
        "        p3 = self.pool3(x3)          # (B, 96, 32, 32)\n",
        "\n",
        "        # Bottleneck Convolutions with Dense Connections\n",
        "        x4 = self.conv7(p3)          # (B, 192, 32, 32)\n",
        "        x4 = self.bn7(x4)\n",
        "        x4 = self.conv8(x4)          # (B, 192, 32, 32)\n",
        "        x4 = self.bn8(x4)\n",
        "        x4 = self.drop6_1(x4)\n",
        "\n",
        "        x5 = self.conv9(x4)          # (B, 192, 32, 32)\n",
        "        x5 = self.bn9(x5)\n",
        "        x5 = self.conv10(x5)         # (B, 192, 32, 32)\n",
        "        x5 = self.bn10(x5)\n",
        "        x5 = self.drop6_2(x5)\n",
        "\n",
        "        concat = torch.cat([x5, x4], dim=1)  # (B, 384, 32, 32)\n",
        "        concat = self.concat1(concat)         # (B, 192, 32, 32)\n",
        "        concat = self.drop6_3(concat)         # (B, 192, 32, 32)\n",
        "\n",
        "        # First Upsampling Block\n",
        "        up1 = self.up1(concat)                 # (B, 96, 64, 64)\n",
        "        up1 = self.bn_up1(up1)\n",
        "        up1 = self.relu_up1(up1)\n",
        "\n",
        "        # Prepare for ConvLSTM2D\n",
        "        # ConvLSTM2D expects input of shape (B, seq_len, C, H, W)\n",
        "        up1_seq = up1.unsqueeze(1)             # (B, 1, 96, 64, 64)\n",
        "        x3_seq = x3.unsqueeze(1)               # (B, 1, 96, 64, 64)\n",
        "        merge1 = torch.cat([x3_seq, up1_seq], dim=1)  # (B, 2, 96, 64, 64)\n",
        "\n",
        "        # Apply ConvLSTM2D\n",
        "        convLSTM1_out = self.convLSTM1(merge1)       # (B, 384, 64, 64)\n",
        "\n",
        "        # Swin Transformer Block in Decoder\n",
        "        convLSTM1_flat = convLSTM1_out.flatten(2).transpose(1, 2)  # (B, 64*64, 384)\n",
        "        swin_D1 = self.swin_unet_D1(convLSTM1_flat)               # (B, 64*64, 384)\n",
        "        swin_D1 = swin_D1.transpose(1, 2).view(-1, 384, 64, 64)    # Reshape for Conv2d\n",
        "\n",
        "        # Further Convolutions\n",
        "        conv6 = self.conv11(swin_D1)        # (B, 48, 64, 64)\n",
        "        conv6 = self.conv12(conv6)          # (B, 48, 64, 64)\n",
        "\n",
        "        # Second Upsampling Block\n",
        "        up2 = self.up2(conv6)               # (B, 48, 128, 128)\n",
        "        up2 = self.bn_up2(up2)\n",
        "        up2 = self.relu_up2(up2)\n",
        "\n",
        "        # Prepare for ConvLSTM2D\n",
        "        up2_seq = up2.unsqueeze(1)           # (B, 1, 48, 128, 128)\n",
        "        x2_seq = x2.unsqueeze(1)             # (B, 1, 48, 128, 128)\n",
        "        merge2 = torch.cat([x2_seq, up2_seq], dim=1)  # (B, 2, 48, 128, 128)\n",
        "\n",
        "        # Apply ConvLSTM2D\n",
        "        convLSTM2_out = self.convLSTM2(merge2)       # (B, 96, 128, 128)\n",
        "\n",
        "        # Swin Transformer Block in Decoder\n",
        "        convLSTM2_flat = convLSTM2_out.flatten(2).transpose(1, 2)  # (B, 128*128, 96)\n",
        "        swin_D2 = self.swin_unet_D2(convLSTM2_flat)               # (B, 128*128, 96)\n",
        "        swin_D2 = swin_D2.transpose(1, 2).view(-1, 96, 128, 128)    # Reshape for Conv2d\n",
        "\n",
        "        # Further Convolutions\n",
        "        conv7 = self.conv13(swin_D2)        # (B, 24, 128, 128)\n",
        "        conv7 = self.conv14(conv7)          # (B, 24, 128, 128)\n",
        "\n",
        "        # Third Upsampling Block\n",
        "        up3 = self.up3(conv7)               # (B, 24, 256, 256)\n",
        "        up3 = self.bn_up3(up3)\n",
        "        up3 = self.relu_up3(up3)\n",
        "\n",
        "        # Prepare for ConvLSTM2D\n",
        "        up3_seq = up3.unsqueeze(1)           # (B, 1, 24, 256, 256)\n",
        "        x1_seq = x1.unsqueeze(1)             # (B, 1, 24, 256, 256)\n",
        "        merge3 = torch.cat([x1_seq, up3_seq], dim=1)  # (B, 2, 24, 256, 256)\n",
        "\n",
        "        # Apply ConvLSTM2D\n",
        "        convLSTM3_out = self.convLSTM3(merge3)       # (B, 48, 256, 256)\n",
        "\n",
        "        # Swin Transformer Block in Decoder\n",
        "        convLSTM3_flat = convLSTM3_out.flatten(2).transpose(1, 2)  # (B, 256*256, 48)\n",
        "        swin_D3 = self.swin_unet_D3(convLSTM3_flat)               # (B, 256*256, 48)\n",
        "        swin_D3 = swin_D3.transpose(1, 2).view(-1, 48, 256, 256)    # Reshape for Conv2d\n",
        "\n",
        "        # Further Convolutions\n",
        "        conv8 = self.conv15(swin_D3)        # (B, 24, 256, 256)\n",
        "        conv8 = self.conv16(conv8)          # (B, 24, 256, 256)\n",
        "\n",
        "        # Final Output Convolutions\n",
        "        final = self.final_conv1(conv8)      # (B, 2, 256, 256)\n",
        "        final = self.final_relu(final)\n",
        "        final = self.final_conv2(final)      # (B, 1, 256, 256)\n",
        "        final = self.final_sigmoid(final)    # (B, 1, 256, 256)\n",
        "\n",
        "        return final\n",
        "\n",
        "# ================================== Dataset Class ======================================\n",
        "\n",
        "class SegmentationDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Custom Dataset for image segmentation tasks.\n",
        "    Expects images in 'x' folder and masks in 'y' folder.\n",
        "    \"\"\"\n",
        "    def __init__(self, images_dir, masks_dir, transform=None):\n",
        "        super(SegmentationDataset, self).__init__()\n",
        "        self.images_dir = images_dir\n",
        "        self.masks_dir = masks_dir\n",
        "        self.transform = transform\n",
        "\n",
        "        self.images = sorted(os.listdir(images_dir))\n",
        "        self.masks = sorted(os.listdir(masks_dir))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Load image\n",
        "        img_path = os.path.join(self.images_dir, self.images[idx])\n",
        "        image = Image.open(img_path).convert('RGB')  # Ensure RGB\n",
        "\n",
        "        # Load mask\n",
        "        mask_path = os.path.join(self.masks_dir, self.masks[idx])\n",
        "        mask = Image.open(mask_path).convert('L')    # Grayscale\n",
        "\n",
        "        # Apply transformations\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "            mask = self.transform(mask)\n",
        "\n",
        "        return image, mask\n",
        "\n",
        "# =============================== Data Loading and Preprocessing ========================\n",
        "\n",
        "# Define image dimensions\n",
        "im_height = 256\n",
        "im_width = 256\n",
        "\n",
        "# Define transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((im_height, im_width)),\n",
        "    transforms.ToTensor(),  # Converts to [0,1]\n",
        "])\n",
        "\n",
        "# Paths to the dataset (update these paths as per your directory structure)\n",
        "train_images_dir = r'/content/drive/MyDrive/ML/dataset/ISIC2018_Task1-2_Training_Input'\n",
        "train_masks_dir = r'/content/drive/MyDrive/ML/dataset/ISIC2018_Task1_Training_GroundTruth'\n",
        "test_images_dir = r'/content/drive/MyDrive/ML/dataset/ISIC2018_Task1-2_Test_Input'\n",
        "test_masks_dir = r'/content/drive/MyDrive/ML/dataset/ISIC2018_Task1_Test_GroundTruth'\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = SegmentationDataset(train_images_dir, train_masks_dir, transform=transform)\n",
        "test_dataset = SegmentationDataset(test_images_dir, test_masks_dir, transform=transform)\n",
        "\n",
        "\n",
        "# Split training data into training and validation sets (80-20 split)\n",
        "train_size = int(0.8 * len(train_dataset))\n",
        "valid_size = len(train_dataset) - train_size\n",
        "train_subset, valid_subset = torch.utils.data.random_split(train_dataset, [train_size, valid_size])\n",
        "\n",
        "# Create DataLoaders\n",
        "batch_size = 5\n",
        "\n",
        "train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
        "valid_loader = DataLoader(valid_subset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
        "\n",
        "# =============================== Training Setup ==========================================\n",
        "\n",
        "# Instantiate the model\n",
        "model = SwinUNet(input_channels=3, output_channels=1, embed_dim=32, num_heads=[4, 8], window_size=4, mlp_ratio=4., depth=2)\n",
        "model = model.to('cuda' if torch.cuda.is_available() else 'cpu')  # Move to GPU if available\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = DiceLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "# Define learning rate scheduler and early stopping parameters\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.25, patience=5, verbose=True, min_lr=1e-9)\n",
        "early_stopping_patience = 9\n",
        "best_val_loss = np.inf\n",
        "epochs_no_improve = 0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9e5t3DhAAAUG",
        "outputId": "cf5ad1af-b5b4-4820-cd9f-74a97071dfb6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
            "Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.8.0\n"
          ]
        }
      ],
      "source": [
        "!pip install torchinfo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7rRAMJmIAgXR",
        "outputId": "5444f287-7107-4b3c-8563-fd1faad2424b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "SwinUNet                                 [1, 1, 256, 256]          --\n",
              "├─SeparableConv2d: 1-1                   [1, 24, 256, 256]         --\n",
              "│    └─Conv2d: 2-1                       [1, 3, 256, 256]          30\n",
              "│    └─Conv2d: 2-2                       [1, 24, 256, 256]         96\n",
              "├─BatchNorm2d: 1-2                       [1, 24, 256, 256]         48\n",
              "├─SeparableConv2d: 1-3                   [1, 24, 256, 256]         --\n",
              "│    └─Conv2d: 2-3                       [1, 24, 256, 256]         240\n",
              "│    └─Conv2d: 2-4                       [1, 24, 256, 256]         600\n",
              "├─BatchNorm2d: 1-4                       [1, 24, 256, 256]         48\n",
              "├─MaxPool2d: 1-5                         [1, 24, 128, 128]         --\n",
              "├─SwinTransformerBlock: 1-6              [1, 16384, 24]            --\n",
              "│    └─LayerNorm: 2-5                    [1, 16384, 24]            48\n",
              "│    └─WindowAttention: 2-6              [1024, 16, 24]            196\n",
              "│    │    └─Linear: 3-1                  [1024, 16, 72]            1,800\n",
              "│    │    └─Dropout: 3-2                 [1024, 4, 16, 16]         --\n",
              "│    │    └─Linear: 3-3                  [1024, 16, 24]            600\n",
              "│    │    └─Dropout: 3-4                 [1024, 16, 24]            --\n",
              "│    └─Identity: 2-7                     [1, 16384, 24]            --\n",
              "│    └─LayerNorm: 2-8                    [1, 16384, 24]            48\n",
              "│    └─Sequential: 2-9                   [1, 16384, 24]            --\n",
              "│    │    └─Linear: 3-5                  [1, 16384, 96]            2,400\n",
              "│    │    └─GELU: 3-6                    [1, 16384, 96]            --\n",
              "│    │    └─Linear: 3-7                  [1, 16384, 24]            2,328\n",
              "│    │    └─Dropout: 3-8                 [1, 16384, 24]            --\n",
              "│    └─Identity: 2-10                    [1, 16384, 24]            --\n",
              "├─SeparableConv2d: 1-7                   [1, 48, 128, 128]         --\n",
              "│    └─Conv2d: 2-11                      [1, 24, 128, 128]         240\n",
              "│    └─Conv2d: 2-12                      [1, 48, 128, 128]         1,200\n",
              "├─BatchNorm2d: 1-8                       [1, 48, 128, 128]         96\n",
              "├─SeparableConv2d: 1-9                   [1, 48, 128, 128]         --\n",
              "│    └─Conv2d: 2-13                      [1, 48, 128, 128]         480\n",
              "│    └─Conv2d: 2-14                      [1, 48, 128, 128]         2,352\n",
              "├─BatchNorm2d: 1-10                      [1, 48, 128, 128]         96\n",
              "├─MaxPool2d: 1-11                        [1, 48, 64, 64]           --\n",
              "├─SwinTransformerBlock: 1-12             [1, 4096, 48]             --\n",
              "│    └─LayerNorm: 2-15                   [1, 4096, 48]             96\n",
              "│    └─WindowAttention: 2-16             [256, 16, 48]             392\n",
              "│    │    └─Linear: 3-9                  [256, 16, 144]            7,056\n",
              "│    │    └─Dropout: 3-10                [256, 8, 16, 16]          --\n",
              "│    │    └─Linear: 3-11                 [256, 16, 48]             2,352\n",
              "│    │    └─Dropout: 3-12                [256, 16, 48]             --\n",
              "│    └─Identity: 2-17                    [1, 4096, 48]             --\n",
              "│    └─LayerNorm: 2-18                   [1, 4096, 48]             96\n",
              "│    └─Sequential: 2-19                  [1, 4096, 48]             --\n",
              "│    │    └─Linear: 3-13                 [1, 4096, 192]            9,408\n",
              "│    │    └─GELU: 3-14                   [1, 4096, 192]            --\n",
              "│    │    └─Linear: 3-15                 [1, 4096, 48]             9,264\n",
              "│    │    └─Dropout: 3-16                [1, 4096, 48]             --\n",
              "│    └─Identity: 2-20                    [1, 4096, 48]             --\n",
              "├─SeparableConv2d: 1-13                  [1, 96, 64, 64]           --\n",
              "│    └─Conv2d: 2-21                      [1, 48, 64, 64]           480\n",
              "│    └─Conv2d: 2-22                      [1, 96, 64, 64]           4,704\n",
              "├─BatchNorm2d: 1-14                      [1, 96, 64, 64]           192\n",
              "├─SeparableConv2d: 1-15                  [1, 96, 64, 64]           --\n",
              "│    └─Conv2d: 2-23                      [1, 96, 64, 64]           960\n",
              "│    └─Conv2d: 2-24                      [1, 96, 64, 64]           9,312\n",
              "├─BatchNorm2d: 1-16                      [1, 96, 64, 64]           192\n",
              "├─Dropout: 1-17                          [1, 96, 64, 64]           --\n",
              "├─MaxPool2d: 1-18                        [1, 96, 32, 32]           --\n",
              "├─SeparableConv2d: 1-19                  [1, 192, 32, 32]          --\n",
              "│    └─Conv2d: 2-25                      [1, 96, 32, 32]           960\n",
              "│    └─Conv2d: 2-26                      [1, 192, 32, 32]          18,624\n",
              "├─BatchNorm2d: 1-20                      [1, 192, 32, 32]          384\n",
              "├─SeparableConv2d: 1-21                  [1, 192, 32, 32]          --\n",
              "│    └─Conv2d: 2-27                      [1, 192, 32, 32]          1,920\n",
              "│    └─Conv2d: 2-28                      [1, 192, 32, 32]          37,056\n",
              "├─BatchNorm2d: 1-22                      [1, 192, 32, 32]          384\n",
              "├─Dropout: 1-23                          [1, 192, 32, 32]          --\n",
              "├─SeparableConv2d: 1-24                  [1, 192, 32, 32]          --\n",
              "│    └─Conv2d: 2-29                      [1, 192, 32, 32]          1,920\n",
              "│    └─Conv2d: 2-30                      [1, 192, 32, 32]          37,056\n",
              "├─BatchNorm2d: 1-25                      [1, 192, 32, 32]          384\n",
              "├─SeparableConv2d: 1-26                  [1, 192, 32, 32]          --\n",
              "│    └─Conv2d: 2-31                      [1, 192, 32, 32]          1,920\n",
              "│    └─Conv2d: 2-32                      [1, 192, 32, 32]          37,056\n",
              "├─BatchNorm2d: 1-27                      [1, 192, 32, 32]          384\n",
              "├─Dropout: 1-28                          [1, 192, 32, 32]          --\n",
              "├─Sequential: 1-29                       [1, 192, 32, 32]          --\n",
              "│    └─SeparableConv2d: 2-33             [1, 192, 32, 32]          --\n",
              "│    │    └─Conv2d: 3-17                 [1, 384, 32, 32]          3,840\n",
              "│    │    └─Conv2d: 3-18                 [1, 192, 32, 32]          73,920\n",
              "│    └─SeparableConv2d: 2-34             [1, 192, 32, 32]          --\n",
              "│    │    └─Conv2d: 3-19                 [1, 192, 32, 32]          1,920\n",
              "│    │    └─Conv2d: 3-20                 [1, 192, 32, 32]          37,056\n",
              "├─Dropout: 1-30                          [1, 192, 32, 32]          --\n",
              "├─ConvTranspose2d: 1-31                  [1, 96, 64, 64]           73,824\n",
              "├─BatchNorm2d: 1-32                      [1, 96, 64, 64]           192\n",
              "├─ReLU: 1-33                             [1, 96, 64, 64]           --\n",
              "├─ConvLSTM2D: 1-34                       [1, 384, 64, 64]          --\n",
              "│    └─ModuleList: 2-35                  --                        --\n",
              "│    │    └─ConvLSTMCell: 3-21           [1, 384, 64, 64]          6,637,056\n",
              "│    │    └─ConvLSTMCell: 3-22           [1, 384, 64, 64]          (recursive)\n",
              "├─SwinTransformerBlock: 1-35             [1, 4096, 384]            --\n",
              "│    └─LayerNorm: 2-36                   [1, 4096, 384]            768\n",
              "│    └─WindowAttention: 2-37             [256, 16, 384]            196\n",
              "│    │    └─Linear: 3-23                 [256, 16, 1152]           443,520\n",
              "│    │    └─Dropout: 3-24                [256, 4, 16, 16]          --\n",
              "│    │    └─Linear: 3-25                 [256, 16, 384]            147,840\n",
              "│    │    └─Dropout: 3-26                [256, 16, 384]            --\n",
              "│    └─Identity: 2-38                    [1, 4096, 384]            --\n",
              "│    └─LayerNorm: 2-39                   [1, 4096, 384]            768\n",
              "│    └─Sequential: 2-40                  [1, 4096, 384]            --\n",
              "│    │    └─Linear: 3-27                 [1, 4096, 1536]           591,360\n",
              "│    │    └─GELU: 3-28                   [1, 4096, 1536]           --\n",
              "│    │    └─Linear: 3-29                 [1, 4096, 384]            590,208\n",
              "│    │    └─Dropout: 3-30                [1, 4096, 384]            --\n",
              "│    └─Identity: 2-41                    [1, 4096, 384]            --\n",
              "├─SeparableConv2d: 1-36                  [1, 48, 64, 64]           --\n",
              "│    └─Conv2d: 2-42                      [1, 384, 64, 64]          3,840\n",
              "│    └─Conv2d: 2-43                      [1, 48, 64, 64]           18,480\n",
              "├─SeparableConv2d: 1-37                  [1, 48, 64, 64]           --\n",
              "│    └─Conv2d: 2-44                      [1, 48, 64, 64]           480\n",
              "│    └─Conv2d: 2-45                      [1, 48, 64, 64]           2,352\n",
              "├─ConvTranspose2d: 1-38                  [1, 48, 128, 128]         9,264\n",
              "├─BatchNorm2d: 1-39                      [1, 48, 128, 128]         96\n",
              "├─ReLU: 1-40                             [1, 48, 128, 128]         --\n",
              "├─ConvLSTM2D: 1-41                       [1, 96, 128, 128]         --\n",
              "│    └─ModuleList: 2-46                  --                        --\n",
              "│    │    └─ConvLSTMCell: 3-31           [1, 96, 128, 128]         498,048\n",
              "│    │    └─ConvLSTMCell: 3-32           [1, 96, 128, 128]         (recursive)\n",
              "├─SwinTransformerBlock: 1-42             [1, 16384, 96]            --\n",
              "│    └─LayerNorm: 2-47                   [1, 16384, 96]            192\n",
              "│    └─WindowAttention: 2-48             [1024, 16, 96]            392\n",
              "│    │    └─Linear: 3-33                 [1024, 16, 288]           27,936\n",
              "│    │    └─Dropout: 3-34                [1024, 8, 16, 16]         --\n",
              "│    │    └─Linear: 3-35                 [1024, 16, 96]            9,312\n",
              "│    │    └─Dropout: 3-36                [1024, 16, 96]            --\n",
              "│    └─Identity: 2-49                    [1, 16384, 96]            --\n",
              "│    └─LayerNorm: 2-50                   [1, 16384, 96]            192\n",
              "│    └─Sequential: 2-51                  [1, 16384, 96]            --\n",
              "│    │    └─Linear: 3-37                 [1, 16384, 384]           37,248\n",
              "│    │    └─GELU: 3-38                   [1, 16384, 384]           --\n",
              "│    │    └─Linear: 3-39                 [1, 16384, 96]            36,960\n",
              "│    │    └─Dropout: 3-40                [1, 16384, 96]            --\n",
              "│    └─Identity: 2-52                    [1, 16384, 96]            --\n",
              "├─SeparableConv2d: 1-43                  [1, 24, 128, 128]         --\n",
              "│    └─Conv2d: 2-53                      [1, 96, 128, 128]         960\n",
              "│    └─Conv2d: 2-54                      [1, 24, 128, 128]         2,328\n",
              "├─SeparableConv2d: 1-44                  [1, 24, 128, 128]         --\n",
              "│    └─Conv2d: 2-55                      [1, 24, 128, 128]         240\n",
              "│    └─Conv2d: 2-56                      [1, 24, 128, 128]         600\n",
              "├─ConvTranspose2d: 1-45                  [1, 24, 256, 256]         2,328\n",
              "├─BatchNorm2d: 1-46                      [1, 24, 256, 256]         48\n",
              "├─ReLU: 1-47                             [1, 24, 256, 256]         --\n",
              "├─ConvLSTM2D: 1-48                       [1, 48, 256, 256]         --\n",
              "│    └─ModuleList: 2-57                  --                        --\n",
              "│    │    └─ConvLSTMCell: 3-41           [1, 48, 256, 256]         124,608\n",
              "│    │    └─ConvLSTMCell: 3-42           [1, 48, 256, 256]         (recursive)\n",
              "├─SwinTransformerBlock: 1-49             [1, 65536, 48]            --\n",
              "│    └─LayerNorm: 2-58                   [1, 65536, 48]            96\n",
              "│    └─WindowAttention: 2-59             [4096, 16, 48]            392\n",
              "│    │    └─Linear: 3-43                 [4096, 16, 144]           7,056\n",
              "│    │    └─Dropout: 3-44                [4096, 8, 16, 16]         --\n",
              "│    │    └─Linear: 3-45                 [4096, 16, 48]            2,352\n",
              "│    │    └─Dropout: 3-46                [4096, 16, 48]            --\n",
              "│    └─Identity: 2-60                    [1, 65536, 48]            --\n",
              "│    └─LayerNorm: 2-61                   [1, 65536, 48]            96\n",
              "│    └─Sequential: 2-62                  [1, 65536, 48]            --\n",
              "│    │    └─Linear: 3-47                 [1, 65536, 192]           9,408\n",
              "│    │    └─GELU: 3-48                   [1, 65536, 192]           --\n",
              "│    │    └─Linear: 3-49                 [1, 65536, 48]            9,264\n",
              "│    │    └─Dropout: 3-50                [1, 65536, 48]            --\n",
              "│    └─Identity: 2-63                    [1, 65536, 48]            --\n",
              "├─SeparableConv2d: 1-50                  [1, 24, 256, 256]         --\n",
              "│    └─Conv2d: 2-64                      [1, 48, 256, 256]         480\n",
              "│    └─Conv2d: 2-65                      [1, 24, 256, 256]         1,176\n",
              "├─SeparableConv2d: 1-51                  [1, 24, 256, 256]         --\n",
              "│    └─Conv2d: 2-66                      [1, 24, 256, 256]         240\n",
              "│    └─Conv2d: 2-67                      [1, 24, 256, 256]         600\n",
              "├─Conv2d: 1-52                           [1, 2, 256, 256]          434\n",
              "├─ReLU: 1-53                             [1, 2, 256, 256]          --\n",
              "├─Conv2d: 1-54                           [1, 1, 256, 256]          3\n",
              "├─Sigmoid: 1-55                          [1, 1, 256, 256]          --\n",
              "==========================================================================================\n",
              "Total params: 9,605,467\n",
              "Trainable params: 9,605,467\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (G): 88.68\n",
              "==========================================================================================\n",
              "Input size (MB): 0.79\n",
              "Forward/backward pass size (MB): 1298.40\n",
              "Params size (MB): 38.42\n",
              "Estimated Total Size (MB): 1337.60\n",
              "=========================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "from torchinfo import summary\n",
        "\n",
        "# Define the input size based on your model's expected input.\n",
        "# For example, if your model expects images with 3 channels and 256x256 dimensions:\n",
        "input_size = (1, 3, 256, 256)  # (batch_size, channels, height, width)\n",
        "\n",
        "# Generate and print the model summary\n",
        "summary(model, input_size=input_size, device='cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bFpU5JlU_p4F",
        "outputId": "9bf6daad-53c1-457c-adb4-526ef9067648"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0\n",
            "Epoch 1/1, Training Loss: 0.5978, Validation Loss: 0.5450\n"
          ]
        }
      ],
      "source": [
        "# =============================== Training Loop ===========================================\n",
        "\n",
        "num_epochs = 1\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(\"Epoch\", epoch)\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for images, masks in train_loader:\n",
        "        images = images.to(device)  # (B, 3, 256, 256)\n",
        "        masks = masks.to(device)    # (B, 1, 256, 256)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)      # (B, 1, 256, 256)\n",
        "        loss = criterion(outputs, masks)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader.dataset)\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for images, masks in valid_loader:\n",
        "            images = images.to(device)\n",
        "            masks = masks.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, masks)\n",
        "\n",
        "            val_loss += loss.item() * images.size(0)\n",
        "\n",
        "    val_loss /= len(valid_loader.dataset)\n",
        "\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, Training Loss: {epoch_loss:.4f}, Validation Loss: {val_loss:.4f}')\n",
        "\n",
        "    # Scheduler step\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "    # Early stopping\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        epochs_no_improve = 0\n",
        "        # Save the best model\n",
        "        torch.save(model.state_dict(), r'/content/drive/MyDrive/model/modelWeights_Swin_Trans_Weights_Swin_Trans_Leather.pth')\n",
        "    else:\n",
        "        epochs_no_improve += 1\n",
        "        if epochs_no_improve >= early_stopping_patience:\n",
        "            print('Early stopping!')\n",
        "            break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r0sihOUA_qyp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b99aecb2-04f7-47da-a20c-81cf957c9efd"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-13-bb6d65efbf8f>:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(r'/content/drive/MyDrive/model/modelWeights_Swin_Trans_Weights_Swin_Trans_Leather.pth'))\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0th Test Image\n",
            "100th Test Image\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# ================================== Prediction ==========================================\n",
        "\n",
        "# Load the best model weights\n",
        "model.load_state_dict(torch.load(r'/content/drive/MyDrive/model/modelWeights_Swin_Trans_Weights_Swin_Trans_Leather.pth'))\n",
        "model.eval()\n",
        "\n",
        "# Function to save predictions and ground truth\n",
        "def save_predictions(model, dataloader, save_dir_pred, save_dir_gt, device):\n",
        "    \"\"\"\n",
        "    Saves the predicted masks and ground truth masks.\n",
        "    Args:\n",
        "        model (nn.Module): Trained model.\n",
        "        dataloader (DataLoader): DataLoader for test data.\n",
        "        save_dir_pred (str): Directory to save predicted masks.\n",
        "        save_dir_gt (str): Directory to save ground truth masks.\n",
        "        device (str): Device to run the model on.\n",
        "    \"\"\"\n",
        "    os.makedirs(save_dir_pred, exist_ok=True)\n",
        "    os.makedirs(save_dir_gt, exist_ok=True)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (images, masks) in enumerate(dataloader):\n",
        "            if (i % 100 == 0):\n",
        "              print(f\"{i}th Test Image\") # There are total 1000 test images.\n",
        "\n",
        "            images = images.to(device)\n",
        "            masks = masks.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            preds = outputs.cpu().numpy()\n",
        "            gts = masks.cpu().numpy()\n",
        "\n",
        "            for j in range(preds.shape[0]):\n",
        "                pred_mask = preds[j, 0, :, :]\n",
        "                gt_mask = gts[j, 0, :, :]\n",
        "\n",
        "                # Save predicted mask\n",
        "                pred_img = Image.fromarray((pred_mask * 255).astype(np.uint8))\n",
        "                pred_img.save(os.path.join(save_dir_pred, f\"{i * dataloader.batch_size + j + 1}.png\"))\n",
        "\n",
        "                # Save ground truth mask\n",
        "                gt_img = Image.fromarray((gt_mask * 255).astype(np.uint8))\n",
        "                gt_img.save(os.path.join(save_dir_gt, f\"{i * dataloader.batch_size + j + 1}.tiff\"))\n",
        "\n",
        "# Define directories to save predictions and ground truth\n",
        "save_dir_pred = r'/content/drive/MyDrive/output/segmented predicted images'\n",
        "save_dir_gt = r'/content/drive/MyDrive/output/segmented ground truth'\n",
        "\n",
        "# Save predictions\n",
        "save_predictions(model, test_loader, save_dir_pred, save_dir_gt, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TRY THE BELOW TWO EVALUATION JUPYTER CELLS SEPERATELY .. I Forgot the difference between each"
      ],
      "metadata": {
        "id": "xWwI-fvj5Urz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TDKhTCBWqOQm"
      },
      "outputs": [],
      "source": [
        "# # =================================== Evaluation =========================================\n",
        "\n",
        "# def evaluate_metrics_pytorch(model, dataloader, device):\n",
        "#     \"\"\"\n",
        "#     Evaluates various metrics for segmentation performance.\n",
        "#     Args:\n",
        "#         model (nn.Module): Trained model.\n",
        "#         dataloader (DataLoader): DataLoader for test data.\n",
        "#         device (str): Device to run the model on.\n",
        "#     Returns:\n",
        "#         dict: Dictionary containing average metrics.\n",
        "#     \"\"\"\n",
        "#     model.eval()\n",
        "#     all_accuracy = []\n",
        "#     all_dice = []\n",
        "#     all_jaccard = []\n",
        "#     all_sensitivity = []\n",
        "#     all_specificity = []\n",
        "\n",
        "#     with torch.no_grad():\n",
        "#         for images, masks in dataloader:\n",
        "#             images = images.to(device)\n",
        "#             masks = masks.to(device)\n",
        "\n",
        "#             outputs = model(images)\n",
        "#             preds = outputs > 0.5  # Binary mask\n",
        "\n",
        "#             preds = preds.cpu().numpy().astype(np.uint8)\n",
        "#             masks = masks.cpu().numpy().astype(np.uint8)\n",
        "\n",
        "#             for pred, mask in zip(preds, masks):\n",
        "#                 pred_flat = pred.flatten()\n",
        "#                 mask_flat = mask.flatten()\n",
        "\n",
        "#                 # Precision-Recall Curve to find optimal threshold\n",
        "#                 precisions, recalls, thresholds = precision_recall_curve(mask_flat, pred.flatten())\n",
        "#                 f1 = 2 * (precisions * recalls) / (precisions + recalls + 1e-8)\n",
        "#                 max_idx = np.argmax(f1)\n",
        "#                 optimal_thresh = thresholds[max_idx] if max_idx < len(thresholds) else 0.5\n",
        "\n",
        "#                 # Apply optimal threshold\n",
        "#                 pred_opt = (pred_flat >= optimal_thresh).astype(np.uint8)\n",
        "\n",
        "#                 # Confusion matrix\n",
        "#                 tn, fp, fn, tp = confusion_matrix(mask_flat, pred_opt).ravel()\n",
        "\n",
        "#                 # Calculate metrics\n",
        "#                 accuracy = (tp + tn) / (tp + tn + fp + fn + 1e-8)\n",
        "#                 iou = tp / (tp + fp + fn + 1e-8)\n",
        "#                 dice = (2 * tp) / (2 * tp + fp + fn + 1e-8)\n",
        "#                 specificity = tn / (tn + fp + 1e-8)\n",
        "#                 sensitivity = tp / (tp + fn + 1e-8)\n",
        "\n",
        "#                 all_accuracy.append(accuracy)\n",
        "#                 all_jaccard.append(iou)\n",
        "#                 all_dice.append(dice)\n",
        "#                 all_specificity.append(specificity)\n",
        "#                 all_sensitivity.append(sensitivity)\n",
        "\n",
        "#     # Compute average metrics\n",
        "#     metrics = {\n",
        "#         'Accuracy': np.mean(all_accuracy),\n",
        "#         'Dice': np.mean(all_dice),\n",
        "#         'Jaccard': np.mean(all_jaccard),\n",
        "#         'Sensitivity': np.mean(all_sensitivity),\n",
        "#         'Specificity': np.mean(all_specificity)\n",
        "#     }\n",
        "\n",
        "#     print(f\"Accuracy: {metrics['Accuracy']:.4f}, Dice: {metrics['Dice']:.4f}, Jaccard: {metrics['Jaccard']:.4f}, \"\n",
        "#           f\"Sensitivity: {metrics['Sensitivity']:.4f}, Specificity: {metrics['Specificity']:.4f}\")\n",
        "\n",
        "#     return metrics\n",
        "\n",
        "# # Evaluate the model\n",
        "# metrics = evaluate_metrics_pytorch(model, test_loader, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rirMHwEit-lK"
      },
      "outputs": [],
      "source": [
        "# =================================== Evaluation =========================================\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, precision_recall_curve\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "def evaluate_metrics_pytorch(model, dataloader, device):\n",
        "    \"\"\"\n",
        "    Evaluates various metrics for segmentation performance.\n",
        "    Args:\n",
        "        model (nn.Module): Trained model.\n",
        "        dataloader (DataLoader): DataLoader for test data.\n",
        "        device (str): Device to run the model on.\n",
        "    Returns:\n",
        "        dict: Dictionary containing average metrics.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    all_accuracy = []\n",
        "    all_dice = []\n",
        "    all_jaccard = []\n",
        "    all_sensitivity = []\n",
        "    all_specificity = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, masks in dataloader:\n",
        "            images = images.to(device)\n",
        "            masks = masks.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            preds = outputs > 0.5  # Binary mask\n",
        "\n",
        "            preds = preds.cpu().numpy().astype(np.uint8)\n",
        "            masks = masks.cpu().numpy().astype(np.uint8)\n",
        "            masks = (masks > 0).astype(np.uint8)  # Convert 255 to 1\n",
        "\n",
        "            for pred, mask in zip(preds, masks):\n",
        "                pred_flat = pred.flatten()\n",
        "                mask_flat = mask.flatten()\n",
        "\n",
        "                # Precision-Recall Curve to find optimal threshold\n",
        "                precisions, recalls, thresholds = precision_recall_curve(mask_flat, pred_flat)\n",
        "                f1 = 2 * (precisions * recalls) / (precisions + recalls + 1e-8)\n",
        "                max_idx = np.argmax(f1)\n",
        "                optimal_thresh = thresholds[max_idx] if max_idx < len(thresholds) else 0.5\n",
        "\n",
        "                # Apply optimal threshold\n",
        "                pred_opt = (pred_flat >= optimal_thresh).astype(np.uint8)\n",
        "\n",
        "                # Confusion matrix with specified labels\n",
        "                cm = confusion_matrix(mask_flat, pred_opt, labels=[0,1])\n",
        "\n",
        "                # Unpack confusion matrix\n",
        "                tn, fp, fn, tp = cm.ravel()\n",
        "\n",
        "                # Calculate metrics\n",
        "                accuracy = (tp + tn) / (tp + tn + fp + fn + 1e-8)\n",
        "                iou = tp / (tp + fp + fn + 1e-8)\n",
        "                dice = (2 * tp) / (2 * tp + fp + fn + 1e-8)\n",
        "                specificity = tn / (tn + fp + 1e-8)\n",
        "                sensitivity = tp / (tp + fn + 1e-8)\n",
        "\n",
        "                all_accuracy.append(accuracy)\n",
        "                all_jaccard.append(iou)\n",
        "                all_dice.append(dice)\n",
        "                all_specificity.append(specificity)\n",
        "                all_sensitivity.append(sensitivity)\n",
        "\n",
        "    # Compute average metrics\n",
        "    metrics = {\n",
        "        'Accuracy': np.mean(all_accuracy),\n",
        "        'Dice': np.mean(all_dice),\n",
        "        'Jaccard': np.mean(all_jaccard),\n",
        "        'Sensitivity': np.mean(all_sensitivity),\n",
        "        'Specificity': np.mean(all_specificity)\n",
        "    }\n",
        "\n",
        "    print(f\"Accuracy: {metrics['Accuracy']:.4f}, Dice: {metrics['Dice']:.4f}, Jaccard: {metrics['Jaccard']:.4f}, \"\n",
        "          f\"Sensitivity: {metrics['Sensitivity']:.4f}, Specificity: {metrics['Specificity']:.4f}\")\n",
        "\n",
        "    return metrics\n",
        "\n",
        "# Evaluate the model\n",
        "metrics = evaluate_metrics_pytorch(model, test_loader, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O_HoLxb9-ogo"
      },
      "outputs": [],
      "source": [
        "# model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ANBXWn1pQp17"
      },
      "outputs": [],
      "source": [
        "1+2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s6u_PM5_FX8j"
      },
      "outputs": [],
      "source": [
        "!pip install torchinfo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HnUVFWKSCuRE"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7lvQREx2CvqH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPptVwXsXVnA"
      },
      "source": [
        "BIDIRECTIONAL CONVLSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ECZrcdEWvgVK"
      },
      "outputs": [],
      "source": [
        "# ============================== Imports and Dependencies ==============================\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from zipfile import ZipFile\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_recall_curve, confusion_matrix\n",
        "\n",
        "# ================================ Separable Convolution =================================\n",
        "\n",
        "class SeparableConv2d(nn.Module):\n",
        "    \"\"\"\n",
        "    Implements a separable convolution layer using depthwise and pointwise convolutions.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, padding=1, bias=True):\n",
        "        super(SeparableConv2d, self).__init__()\n",
        "        # Depthwise convolution (groups=in_channels)\n",
        "        self.depthwise = nn.Conv2d(in_channels, in_channels, kernel_size=kernel_size,\n",
        "                                   padding=padding, groups=in_channels, bias=bias)\n",
        "        # Pointwise convolution\n",
        "        self.pointwise = nn.Conv2d(in_channels, out_channels, kernel_size=1,\n",
        "                                   padding=0, bias=bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.depthwise(x)\n",
        "        x = self.pointwise(x)\n",
        "        return x\n",
        "\n",
        "# ================================== ConvLSTM2D ========================================\n",
        "\n",
        "class ConvLSTMCell(nn.Module):\n",
        "    \"\"\"\n",
        "    Implements a ConvLSTM cell.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_channels, hidden_channels, kernel_size, bias=True):\n",
        "        super(ConvLSTMCell, self).__init__()\n",
        "\n",
        "        padding = kernel_size // 2  # To maintain spatial dimensions\n",
        "        self.input_channels = input_channels\n",
        "        self.hidden_channels = hidden_channels\n",
        "\n",
        "        self.conv = nn.Conv2d(in_channels=input_channels + hidden_channels,\n",
        "                              out_channels=4 * hidden_channels,\n",
        "                              kernel_size=kernel_size,\n",
        "                              padding=padding,\n",
        "                              bias=bias)\n",
        "\n",
        "    def forward(self, input_tensor, cur_state):\n",
        "        h_cur, c_cur = cur_state\n",
        "\n",
        "        # Concatenate input and hidden state\n",
        "        combined = torch.cat([input_tensor, h_cur], dim=1)  # along channel axis\n",
        "\n",
        "        # Compute all gates at once\n",
        "        conv_output = self.conv(combined)\n",
        "        cc_i, cc_f, cc_o, cc_g = torch.split(conv_output, self.hidden_channels, dim=1)\n",
        "\n",
        "        i = torch.sigmoid(cc_i)   # input gate\n",
        "        f = torch.sigmoid(cc_f)   # forget gate\n",
        "        o = torch.sigmoid(cc_o)   # output gate\n",
        "        g = torch.tanh(cc_g)      # gate gate\n",
        "\n",
        "        c_next = f * c_cur + i * g\n",
        "        h_next = o * torch.tanh(c_next)\n",
        "\n",
        "        return h_next, c_next\n",
        "\n",
        "    def init_hidden(self, batch_size, spatial_size, device):\n",
        "        height, width = spatial_size\n",
        "        return (torch.zeros(batch_size, self.hidden_channels, height, width, device=device),\n",
        "                torch.zeros(batch_size, self.hidden_channels, height, width, device=device))\n",
        "\n",
        "class ConvLSTM2D(nn.Module):\n",
        "    \"\"\"\n",
        "    Implements a ConvLSTM2D layer that processes a sequence of inputs.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_channels, hidden_channels, kernel_size=3, bias=True, num_layers=1):\n",
        "        super(ConvLSTM2D, self).__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.hidden_channels = hidden_channels\n",
        "\n",
        "        layers = []\n",
        "        for i in range(num_layers):\n",
        "            input_c = input_channels if i == 0 else hidden_channels\n",
        "            layers.append(ConvLSTMCell(input_c, hidden_channels, kernel_size, bias))\n",
        "        self.layers = nn.ModuleList(layers)\n",
        "\n",
        "    def forward(self, input_tensor, reverse=False):\n",
        "        # input_tensor shape: (batch, seq_len, channels, height, width)\n",
        "        batch_size, seq_len, channels, height, width = input_tensor.size()\n",
        "        device = input_tensor.device\n",
        "\n",
        "        # Initialize hidden and cell states for all layers\n",
        "        hidden_state = []\n",
        "        cell_state = []\n",
        "        for i in range(self.num_layers):\n",
        "            h, c = self.layers[i].init_hidden(batch_size, (height, width), device)\n",
        "            hidden_state.append(h)\n",
        "            cell_state.append(c)\n",
        "\n",
        "        # Iterate over time steps\n",
        "        if reverse:\n",
        "            time_steps = reversed(range(seq_len))\n",
        "        else:\n",
        "            time_steps = range(seq_len)\n",
        "\n",
        "        outputs = []\n",
        "        for t in time_steps:\n",
        "            x = input_tensor[:, t, :, :, :]  # (batch, channels, height, width)\n",
        "            for i, layer in enumerate(self.layers):\n",
        "                h, c = layer(x, (hidden_state[i], cell_state[i]))\n",
        "                hidden_state[i] = h\n",
        "                cell_state[i] = c\n",
        "                x = h  # input to next layer\n",
        "            outputs.append(x)\n",
        "\n",
        "        outputs = torch.stack(outputs, dim=1)  # (batch, seq_len, channels, height, width)\n",
        "        if reverse:\n",
        "            outputs = outputs.flip(dims=[1])  # Reverse back to original order\n",
        "        return outputs  # Return the sequence of outputs\n",
        "\n",
        "class BidirectionalConvLSTM2D(nn.Module):\n",
        "    \"\"\"\n",
        "    Implements a Bidirectional ConvLSTM2D layer.\n",
        "    Processes the input sequence in both forward and backward directions and concatenates the outputs.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_channels, hidden_channels, kernel_size=3, num_layers=1, bias=True):\n",
        "        super(BidirectionalConvLSTM2D, self).__init__()\n",
        "        self.forward_conv_lstm = ConvLSTM2D(input_channels, hidden_channels, kernel_size, bias=bias, num_layers=num_layers)\n",
        "        self.backward_conv_lstm = ConvLSTM2D(input_channels, hidden_channels, kernel_size, bias=bias, num_layers=num_layers)\n",
        "\n",
        "    def forward(self, input_tensor):\n",
        "        # input_tensor shape: (batch, seq_len, channels, height, width)\n",
        "        # Forward direction\n",
        "        forward_output = self.forward_conv_lstm(input_tensor, reverse=False)  # (batch, seq_len, hidden_channels, H, W)\n",
        "        # Backward direction\n",
        "        backward_output = self.backward_conv_lstm(input_tensor, reverse=True)  # (batch, seq_len, hidden_channels, H, W)\n",
        "        # Concatenate outputs along the channel dimension\n",
        "        output = torch.cat([forward_output, backward_output], dim=2)  # (batch, seq_len, hidden_channels*2, H, W)\n",
        "        # Since seq_len=1 in our case after merging, we can squeeze the seq_len dimension\n",
        "        output = output[:, -1, :, :, :]  # Take the last output (batch, hidden_channels*2, H, W)\n",
        "        return output\n",
        "\n",
        "# ============================== Swin Transformer Blocks ================================\n",
        "\n",
        "class WindowAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Window based multi-head self attention (W-MSA) module with relative position bias.\n",
        "    \"\"\"\n",
        "    def __init__(self, dim, window_size, num_heads, qkv_bias=True, attn_drop=0., proj_drop=0.):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            dim (int): Number of input channels.\n",
        "            window_size (tuple): Height and width of the window.\n",
        "            num_heads (int): Number of attention heads.\n",
        "            qkv_bias (bool): If True, add a learnable bias to query, key, value.\n",
        "            attn_drop (float): Dropout ratio of attention weights.\n",
        "            proj_drop (float): Dropout ratio of output.\n",
        "        \"\"\"\n",
        "        super(WindowAttention, self).__init__()\n",
        "        self.dim = dim\n",
        "        self.window_size = window_size  # Wh, Ww\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // num_heads\n",
        "        self.scale = head_dim ** -0.5\n",
        "\n",
        "        # Define a parameter table of relative position bias\n",
        "        self.relative_position_bias_table = nn.Parameter(\n",
        "            torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads)\n",
        "        )  # 2*Wh-1 * 2*Ww-1, nH\n",
        "\n",
        "        # Get pair-wise relative position index for each token inside the window\n",
        "        coords_h = torch.arange(self.window_size[0])\n",
        "        coords_w = torch.arange(self.window_size[1])\n",
        "        coords = torch.stack(torch.meshgrid(coords_h, coords_w, indexing='ij'))  # 2, Wh, Ww\n",
        "        coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n",
        "        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n",
        "        relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n",
        "        relative_coords[:, :, 0] += self.window_size[0] - 1  # shift to start from 0\n",
        "        relative_coords[:, :, 1] += self.window_size[1] - 1\n",
        "        relative_coords[:, :, 0] *= (2 * self.window_size[1] - 1)\n",
        "        relative_position_index = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n",
        "        self.register_buffer(\"relative_position_index\", relative_position_index)\n",
        "\n",
        "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)  # Query, Key, Value\n",
        "        self.attn_drop = nn.Dropout(attn_drop)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.proj_drop = nn.Dropout(proj_drop)\n",
        "\n",
        "        # Initialize relative position bias table\n",
        "        nn.init.trunc_normal_(self.relative_position_bias_table, std=.02)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: input features with shape of (num_windows*B, Wh*Ww, C)\n",
        "            mask: (num_windows, Wh*Ww, Wh*Ww) or None\n",
        "        \"\"\"\n",
        "        B_, N, C = x.shape\n",
        "        qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)  # 3, B_, nH, N, C//nH\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]  # each has shape (B_, nH, N, C//nH)\n",
        "\n",
        "        q = q * self.scale\n",
        "        attn = (q @ k.transpose(-2, -1))  # (B_, nH, N, N)\n",
        "\n",
        "        # Add relative position bias\n",
        "        relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(\n",
        "            self.window_size[0] * self.window_size[1], self.window_size[0] * self.window_size[1], -1\n",
        "        )  # Wh*Ww, Wh*Ww, nH\n",
        "        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n",
        "        attn = attn + relative_position_bias.unsqueeze(0)  # (B_, nH, N, N)\n",
        "\n",
        "        if mask is not None:\n",
        "            nW = mask.shape[0]\n",
        "            attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)\n",
        "            attn = attn.view(-1, self.num_heads, N, N)\n",
        "            attn = F.softmax(attn, dim=-1)\n",
        "        else:\n",
        "            attn = F.softmax(attn, dim=-1)\n",
        "\n",
        "        attn = self.attn_drop(attn)\n",
        "\n",
        "        out = (attn @ v).transpose(1, 2).reshape(B_, N, C)  # (B_, N, C)\n",
        "        out = self.proj(out)\n",
        "        out = self.proj_drop(out)\n",
        "        return out\n",
        "\n",
        "class SwinTransformerBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Swin Transformer Block with W-MSA and SW-MSA.\n",
        "    \"\"\"\n",
        "    def __init__(self, dim, num_heads, window_size=7, shift_size=0, mlp_ratio=4., qkv_bias=True,\n",
        "                 attn_drop=0., proj_drop=0.):\n",
        "        super(SwinTransformerBlock, self).__init__()\n",
        "        self.dim = dim\n",
        "        self.num_heads = num_heads\n",
        "        self.window_size = window_size  # W\n",
        "        self.shift_size = shift_size    # S\n",
        "        self.mlp_ratio = mlp_ratio\n",
        "\n",
        "        assert 0 <= self.shift_size < self.window_size, \"shift_size must be in [0, window_size)\"\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(dim)\n",
        "        self.attn = WindowAttention(dim, (window_size, window_size), num_heads, qkv_bias, attn_drop, proj_drop)\n",
        "\n",
        "        self.drop_path = nn.Identity()  # Can implement stochastic depth if desired\n",
        "        self.norm2 = nn.LayerNorm(dim)\n",
        "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(dim, mlp_hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(mlp_hidden_dim, dim),\n",
        "            nn.Dropout(proj_drop)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: input features with shape (B, H*W, C)\n",
        "        \"\"\"\n",
        "        H = W = int(np.sqrt(x.shape[1]))\n",
        "        B, L, C = x.shape\n",
        "        assert L == H * W, \"Input feature has wrong size\"\n",
        "\n",
        "        shortcut = x\n",
        "        x = self.norm1(x)\n",
        "        x = x.view(B, H, W, C)\n",
        "\n",
        "        # Cyclic shift\n",
        "        if self.shift_size > 0:\n",
        "            shifted_x = torch.roll(x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2))\n",
        "        else:\n",
        "            shifted_x = x\n",
        "\n",
        "        # Partition windows\n",
        "        window_size = self.window_size\n",
        "        # Pad H and W to be multiples of window_size\n",
        "        pad_b = (window_size - H % window_size) % window_size\n",
        "        pad_r = (window_size - W % window_size) % window_size\n",
        "        shifted_x = F.pad(shifted_x, (0, 0, 0, pad_r, 0, pad_b))  # pad H and W\n",
        "        _, Hp, Wp, _ = shifted_x.shape\n",
        "\n",
        "        # Window partition\n",
        "        x_windows = shifted_x.view(B, Hp // window_size, window_size, Wp // window_size, window_size, C)\n",
        "        x_windows = x_windows.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size * window_size, C)  # (num_windows*B, window_size*window_size, C)\n",
        "\n",
        "        # Attention\n",
        "        attn_windows = self.attn(x_windows)  # (num_windows*B, window_size*window_size, C)\n",
        "\n",
        "        # Merge windows\n",
        "        shifted_x = attn_windows.view(-1, window_size, window_size, C)\n",
        "        shifted_x = shifted_x.view(B, Hp // window_size, Wp // window_size, window_size, window_size, C)\n",
        "        shifted_x = shifted_x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, Hp, Wp, C)\n",
        "\n",
        "        # Reverse cyclic shift\n",
        "        if self.shift_size > 0:\n",
        "            x = torch.roll(shifted_x, shifts=(self.shift_size, self.shift_size), dims=(1, 2))\n",
        "        else:\n",
        "            x = shifted_x\n",
        "\n",
        "        # Remove padding\n",
        "        x = x[:, :H, :W, :].contiguous().view(B, H * W, C)\n",
        "\n",
        "        # FFN\n",
        "        x = shortcut + self.drop_path(x)\n",
        "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
        "\n",
        "        return x\n",
        "\n",
        "# =============================== Dice Loss Function ====================================\n",
        "\n",
        "class DiceLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Dice Loss function to maximize the Dice coefficient.\n",
        "    Suitable for binary segmentation tasks.\n",
        "    \"\"\"\n",
        "    def __init__(self, smooth=1.0):\n",
        "        super(DiceLoss, self).__init__()\n",
        "        self.smooth = smooth\n",
        "\n",
        "    def forward(self, y_pred, y_true):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            y_pred (torch.Tensor): Predicted mask probabilities with shape (B, 1, H, W)\n",
        "            y_true (torch.Tensor): Ground truth masks with shape (B, 1, H, W)\n",
        "        Returns:\n",
        "            torch.Tensor: Dice loss\n",
        "        \"\"\"\n",
        "        y_pred = y_pred.view(-1)\n",
        "        y_true = y_true.view(-1)\n",
        "\n",
        "        intersection = (y_pred * y_true).sum()\n",
        "        dice = (2. * intersection + self.smooth) / (y_pred.sum() + y_true.sum() + self.smooth)\n",
        "\n",
        "        return 1 - dice\n",
        "\n",
        "# ================================ Main Model ============================================\n",
        "\n",
        "class SwinUNet(nn.Module):\n",
        "    \"\"\"\n",
        "    Swin U-Net architecture for image segmentation with bidirectional ConvLSTM layers.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_channels=3, output_channels=1,\n",
        "                 embed_dim=32, num_heads=[4, 8], window_size=4,\n",
        "                 mlp_ratio=4., depth=2):\n",
        "        super(SwinUNet, self).__init__()\n",
        "        self.input_channels = input_channels\n",
        "        self.output_channels = output_channels\n",
        "\n",
        "        # Initial convolutional layers\n",
        "        self.conv1 = SeparableConv2d(input_channels, 24, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(24)\n",
        "        self.conv2 = SeparableConv2d(24, 24, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(24)\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)  # 256x256 -> 128x128\n",
        "\n",
        "        # First Swin Transformer Block\n",
        "        self.swin_unet_E1 = SwinTransformerBlock(\n",
        "            dim=24,  # Changed from embed_dim=32 to 24\n",
        "            num_heads=num_heads[0],\n",
        "            window_size=window_size,\n",
        "            shift_size=window_size//2 if True else 0,\n",
        "            mlp_ratio=mlp_ratio\n",
        "        )\n",
        "\n",
        "        # Second convolutional block\n",
        "        self.conv3 = SeparableConv2d(24, 48, kernel_size=3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(48)\n",
        "        self.conv4 = SeparableConv2d(48, 48, kernel_size=3, padding=1)\n",
        "        self.bn4 = nn.BatchNorm2d(48)\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)  # 128x128 -> 64x64\n",
        "\n",
        "        # Second Swin Transformer Block\n",
        "        self.swin_unet_E2 = SwinTransformerBlock(\n",
        "            dim=48,\n",
        "            num_heads=num_heads[1],\n",
        "            window_size=window_size,\n",
        "            shift_size=window_size//2 if True else 0,\n",
        "            mlp_ratio=mlp_ratio\n",
        "        )\n",
        "\n",
        "        # Third convolutional block (Bottleneck)\n",
        "        self.conv5 = SeparableConv2d(48, 96, kernel_size=3, padding=1)\n",
        "        self.bn5 = nn.BatchNorm2d(96)\n",
        "        self.conv6 = SeparableConv2d(96, 96, kernel_size=3, padding=1)\n",
        "        self.bn6 = nn.BatchNorm2d(96)\n",
        "        self.drop5 = nn.Dropout(0.5)\n",
        "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)  # 64x64 -> 32x32\n",
        "\n",
        "        # Bottleneck convolutions with dense connections\n",
        "        self.conv7 = SeparableConv2d(96, 192, kernel_size=3, padding=1)\n",
        "        self.bn7 = nn.BatchNorm2d(192)\n",
        "        self.conv8 = SeparableConv2d(192, 192, kernel_size=3, padding=1)\n",
        "        self.bn8 = nn.BatchNorm2d(192)\n",
        "        self.drop6_1 = nn.Dropout(0.5)\n",
        "\n",
        "        self.conv9 = SeparableConv2d(192, 192, kernel_size=3, padding=1)\n",
        "        self.bn9 = nn.BatchNorm2d(192)\n",
        "        self.conv10 = SeparableConv2d(192, 192, kernel_size=3, padding=1)\n",
        "        self.bn10 = nn.BatchNorm2d(192)\n",
        "        self.drop6_2 = nn.Dropout(0.5)\n",
        "\n",
        "        self.concat1 = nn.Sequential(\n",
        "            SeparableConv2d(384, 192, kernel_size=3, padding=1),\n",
        "            SeparableConv2d(192, 192, kernel_size=3, padding=1)\n",
        "        )\n",
        "        self.drop6_3 = nn.Dropout(0.5)\n",
        "\n",
        "        # First Upsampling Block\n",
        "        self.up1 = nn.ConvTranspose2d(192, 96, kernel_size=2, stride=2)  # 32x32 -> 64x64\n",
        "        self.bn_up1 = nn.BatchNorm2d(96)\n",
        "        self.relu_up1 = nn.ReLU(inplace=True)\n",
        "        self.bidirectional_convLSTM1 = BidirectionalConvLSTM2D(input_channels=96, hidden_channels=192, kernel_size=3, num_layers=1)\n",
        "        self.swin_unet_D1 = SwinTransformerBlock(\n",
        "            dim=192 * 2,  # Adjusted for bidirectional output\n",
        "            num_heads=num_heads[0],\n",
        "            window_size=window_size,\n",
        "            shift_size=window_size//2 if True else 0,\n",
        "            mlp_ratio=mlp_ratio\n",
        "        )\n",
        "        self.conv11 = SeparableConv2d(192 * 2, 48, kernel_size=3, padding=1)\n",
        "        self.conv12 = SeparableConv2d(48, 48, kernel_size=3, padding=1)\n",
        "\n",
        "        # Second Upsampling Block\n",
        "        self.up2 = nn.ConvTranspose2d(48, 48, kernel_size=2, stride=2)  # 64x64 -> 128x128\n",
        "        self.bn_up2 = nn.BatchNorm2d(48)\n",
        "        self.relu_up2 = nn.ReLU(inplace=True)\n",
        "        self.bidirectional_convLSTM2 = BidirectionalConvLSTM2D(input_channels=48, hidden_channels=96, kernel_size=3, num_layers=1)\n",
        "        self.swin_unet_D2 = SwinTransformerBlock(\n",
        "            dim=96 * 2,\n",
        "            num_heads=num_heads[1],\n",
        "            window_size=window_size,\n",
        "            shift_size=window_size//2 if True else 0,\n",
        "            mlp_ratio=mlp_ratio\n",
        "        )\n",
        "        self.conv13 = SeparableConv2d(96 * 2, 24, kernel_size=3, padding=1)\n",
        "        self.conv14 = SeparableConv2d(24, 24, kernel_size=3, padding=1)\n",
        "\n",
        "        # Third Upsampling Block\n",
        "        self.up3 = nn.ConvTranspose2d(24, 24, kernel_size=2, stride=2)  # 128x128 -> 256x256\n",
        "        self.bn_up3 = nn.BatchNorm2d(24)\n",
        "        self.relu_up3 = nn.ReLU(inplace=True)\n",
        "        self.bidirectional_convLSTM3 = BidirectionalConvLSTM2D(input_channels=24, hidden_channels=48, kernel_size=3, num_layers=1)\n",
        "        self.swin_unet_D3 = SwinTransformerBlock(\n",
        "            dim=48 * 2,\n",
        "            num_heads=num_heads[1],\n",
        "            window_size=window_size,\n",
        "            shift_size=window_size//2 if True else 0,\n",
        "            mlp_ratio=mlp_ratio\n",
        "        )\n",
        "        self.conv15 = SeparableConv2d(48 * 2, 24, kernel_size=3, padding=1)\n",
        "        self.conv16 = SeparableConv2d(24, 24, kernel_size=3, padding=1)\n",
        "\n",
        "        # Output Layer\n",
        "        self.final_conv1 = nn.Conv2d(24, 2, kernel_size=3, padding=1)\n",
        "        self.final_relu = nn.ReLU(inplace=True)\n",
        "        self.final_conv2 = nn.Conv2d(2, 1, kernel_size=1, padding=0)\n",
        "        self.final_sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass of the Swin U-Net model.\n",
        "        Args:\n",
        "            x: Input tensor with shape (B, 3, 256, 256)\n",
        "        Returns:\n",
        "            torch.Tensor: Output segmentation mask with shape (B, 1, 256, 256)\n",
        "        \"\"\"\n",
        "        # Initial Convolutions\n",
        "        x1 = self.conv1(x)          # (B, 24, 256, 256)\n",
        "        x1 = self.bn1(x1)\n",
        "        x1 = self.conv2(x1)         # (B, 24, 256, 256)\n",
        "        x1 = self.bn2(x1)\n",
        "        p1 = self.pool1(x1)         # (B, 24, 128, 128)\n",
        "\n",
        "        # First Swin Transformer Block\n",
        "        p1_flat = p1.flatten(2).transpose(1, 2)  # (B, 128*128, 24)\n",
        "        swin_E1 = self.swin_unet_E1(p1_flat)     # (B, 128*128, 24)\n",
        "        swin_E1 = swin_E1.transpose(1, 2).view(-1, 24, 128, 128)  # Reshape for Conv2d\n",
        "\n",
        "        # Second Convolutional Block\n",
        "        x2 = self.conv3(swin_E1)    # (B, 48, 128, 128)\n",
        "        x2 = self.bn3(x2)\n",
        "        x2 = self.conv4(x2)          # (B, 48, 128, 128)\n",
        "        x2 = self.bn4(x2)\n",
        "        p2 = self.pool2(x2)          # (B, 48, 64, 64)\n",
        "\n",
        "        # Second Swin Transformer Block\n",
        "        p2_flat = p2.flatten(2).transpose(1, 2)  # (B, 64*64, 48)\n",
        "        swin_E2 = self.swin_unet_E2(p2_flat)     # (B, 64*64, 48)\n",
        "        swin_E2 = swin_E2.transpose(1, 2).view(-1, 48, 64, 64)  # Reshape for Conv2d\n",
        "\n",
        "        # Third Convolutional Block (Bottleneck)\n",
        "        x3 = self.conv5(swin_E2)    # (B, 96, 64, 64)\n",
        "        x3 = self.bn5(x3)\n",
        "        x3 = self.conv6(x3)          # (B, 96, 64, 64)\n",
        "        x3 = self.bn6(x3)\n",
        "        x3 = self.drop5(x3)\n",
        "        p3 = self.pool3(x3)          # (B, 96, 32, 32)\n",
        "\n",
        "        # Bottleneck Convolutions with Dense Connections\n",
        "        x4 = self.conv7(p3)          # (B, 192, 32, 32)\n",
        "        x4 = self.bn7(x4)\n",
        "        x4 = self.conv8(x4)          # (B, 192, 32, 32)\n",
        "        x4 = self.bn8(x4)\n",
        "        x4 = self.drop6_1(x4)\n",
        "\n",
        "        x5 = self.conv9(x4)          # (B, 192, 32, 32)\n",
        "        x5 = self.bn9(x5)\n",
        "        x5 = self.conv10(x5)         # (B, 192, 32, 32)\n",
        "        x5 = self.bn10(x5)\n",
        "        x5 = self.drop6_2(x5)\n",
        "\n",
        "        concat = torch.cat([x5, x4], dim=1)  # (B, 384, 32, 32)\n",
        "        concat = self.concat1(concat)         # (B, 192, 32, 32)\n",
        "        concat = self.drop6_3(concat)         # (B, 192, 32, 32)\n",
        "\n",
        "        # First Upsampling Block\n",
        "        up1 = self.up1(concat)                 # (B, 96, 64, 64)\n",
        "        up1 = self.bn_up1(up1)\n",
        "        up1 = self.relu_up1(up1)\n",
        "\n",
        "        # Prepare for BidirectionalConvLSTM2D\n",
        "        up1_seq = torch.stack([x3, up1], dim=1)  # (B, 2, 96, 64, 64)\n",
        "        bidir_convLSTM1_out = self.bidirectional_convLSTM1(up1_seq)  # (B, 192*2, 64, 64)\n",
        "\n",
        "        # Swin Transformer Block in Decoder\n",
        "        bidir_convLSTM1_flat = bidir_convLSTM1_out.flatten(2).transpose(1, 2)  # (B, 64*64, 192*2)\n",
        "        swin_D1 = self.swin_unet_D1(bidir_convLSTM1_flat)               # (B, 64*64, 192*2)\n",
        "        swin_D1 = swin_D1.transpose(1, 2).view(-1, 192*2, 64, 64)    # Reshape for Conv2d\n",
        "\n",
        "        # Further Convolutions\n",
        "        conv6 = self.conv11(swin_D1)        # (B, 48, 64, 64)\n",
        "        conv6 = self.conv12(conv6)          # (B, 48, 64, 64)\n",
        "\n",
        "        # Second Upsampling Block\n",
        "        up2 = self.up2(conv6)               # (B, 48, 128, 128)\n",
        "        up2 = self.bn_up2(up2)\n",
        "        up2 = self.relu_up2(up2)\n",
        "\n",
        "        # Prepare for BidirectionalConvLSTM2D\n",
        "        up2_seq = torch.stack([x2, up2], dim=1)  # (B, 2, 48, 128, 128)\n",
        "        bidir_convLSTM2_out = self.bidirectional_convLSTM2(up2_seq)  # (B, 96*2, 128, 128)\n",
        "\n",
        "        # Swin Transformer Block in Decoder\n",
        "        bidir_convLSTM2_flat = bidir_convLSTM2_out.flatten(2).transpose(1, 2)  # (B, 128*128, 96*2)\n",
        "        swin_D2 = self.swin_unet_D2(bidir_convLSTM2_flat)               # (B, 128*128, 96*2)\n",
        "        swin_D2 = swin_D2.transpose(1, 2).view(-1, 96*2, 128, 128)    # Reshape for Conv2d\n",
        "\n",
        "        # Further Convolutions\n",
        "        conv7 = self.conv13(swin_D2)        # (B, 24, 128, 128)\n",
        "        conv7 = self.conv14(conv7)          # (B, 24, 128, 128)\n",
        "\n",
        "        # Third Upsampling Block\n",
        "        up3 = self.up3(conv7)               # (B, 24, 256, 256)\n",
        "        up3 = self.bn_up3(up3)\n",
        "        up3 = self.relu_up3(up3)\n",
        "\n",
        "        # Prepare for BidirectionalConvLSTM2D\n",
        "        up3_seq = torch.stack([x1, up3], dim=1)  # (B, 2, 24, 256, 256)\n",
        "        bidir_convLSTM3_out = self.bidirectional_convLSTM3(up3_seq)  # (B, 48*2, 256, 256)\n",
        "\n",
        "        # Swin Transformer Block in Decoder\n",
        "        bidir_convLSTM3_flat = bidir_convLSTM3_out.flatten(2).transpose(1, 2)  # (B, 256*256, 48*2)\n",
        "        swin_D3 = self.swin_unet_D3(bidir_convLSTM3_flat)               # (B, 256*256, 48*2)\n",
        "        swin_D3 = swin_D3.transpose(1, 2).view(-1, 48*2, 256, 256)    # Reshape for Conv2d\n",
        "\n",
        "        # Further Convolutions\n",
        "        conv8 = self.conv15(swin_D3)        # (B, 24, 256, 256)\n",
        "        conv8 = self.conv16(conv8)          # (B, 24, 256, 256)\n",
        "\n",
        "        # Final Output Convolutions\n",
        "        final = self.final_conv1(conv8)      # (B, 2, 256, 256)\n",
        "        final = self.final_relu(final)\n",
        "        final = self.final_conv2(final)      # (B, 1, 256, 256)\n",
        "        final = self.final_sigmoid(final)    # (B, 1, 256, 256)\n",
        "\n",
        "        return final\n",
        "\n",
        "# ================================== Dataset Class ======================================\n",
        "\n",
        "class SegmentationDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Custom Dataset for image segmentation tasks.\n",
        "    Expects images in 'x' folder and masks in 'y' folder.\n",
        "    \"\"\"\n",
        "    def __init__(self, images_dir, masks_dir, transform=None):\n",
        "        super(SegmentationDataset, self).__init__()\n",
        "        self.images_dir = images_dir\n",
        "        self.masks_dir = masks_dir\n",
        "        self.transform = transform\n",
        "\n",
        "        self.images = sorted(os.listdir(images_dir))\n",
        "        self.masks = sorted(os.listdir(masks_dir))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Load image\n",
        "        img_path = os.path.join(self.images_dir, self.images[idx])\n",
        "        image = Image.open(img_path).convert('RGB')  # Ensure RGB\n",
        "\n",
        "        # Load mask\n",
        "        mask_path = os.path.join(self.masks_dir, self.masks[idx])\n",
        "        mask = Image.open(mask_path).convert('L')    # Grayscale\n",
        "\n",
        "        # Apply transformations\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "            mask = self.transform(mask)\n",
        "\n",
        "        return image, mask\n",
        "\n",
        "# =============================== Data Loading and Preprocessing ========================\n",
        "\n",
        "# Define image dimensions\n",
        "im_height = 256\n",
        "im_width = 256\n",
        "\n",
        "# Define transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((im_height, im_width)),\n",
        "    transforms.ToTensor(),  # Converts to [0,1]\n",
        "])\n",
        "\n",
        "# Paths to the dataset (update these paths as per your directory structure)\n",
        "train_images_dir = r'/content/drive/MyDrive/ML/dataset/ISIC2018_Task1-2_Training_Input'\n",
        "train_masks_dir = r'/content/drive/MyDrive/ML/dataset/ISIC2018_Task1_Training_GroundTruth'\n",
        "test_images_dir = r'/content/drive/MyDrive/ML/dataset/ISIC2018_Task1-2_Test_Input'\n",
        "test_masks_dir = r'/content/drive/MyDrive/ML/dataset/ISIC2018_Task1_Test_GroundTruth'\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = SegmentationDataset(train_images_dir, train_masks_dir, transform=transform)\n",
        "test_dataset = SegmentationDataset(test_images_dir, test_masks_dir, transform=transform)\n",
        "\n",
        "# Split training data into training and validation sets (80-20 split)\n",
        "train_size = int(0.8 * len(train_dataset))\n",
        "valid_size = len(train_dataset) - train_size\n",
        "train_subset, valid_subset = torch.utils.data.random_split(train_dataset, [train_size, valid_size])\n",
        "\n",
        "# Create DataLoaders\n",
        "batch_size = 5\n",
        "\n",
        "train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
        "valid_loader = DataLoader(valid_subset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
        "\n",
        "# =============================== Training Setup ==========================================\n",
        "\n",
        "# Instantiate the model\n",
        "model = SwinUNet(input_channels=3, output_channels=1, embed_dim=32, num_heads=[4, 8], window_size=4, mlp_ratio=4., depth=2)\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model = model.to(device)  # Move to GPU if available\n",
        "\n",
        "# Initialize weights using Kaiming Normal initialization\n",
        "def initialize_weights(module):\n",
        "    if isinstance(module, (nn.Conv2d, nn.ConvTranspose2d)):\n",
        "        nn.init.kaiming_normal_(module.weight, mode='fan_in', nonlinearity='relu')\n",
        "        if module.bias is not None:\n",
        "            nn.init.zeros_(module.bias)\n",
        "    elif isinstance(module, nn.Linear):\n",
        "        nn.init.kaiming_normal_(module.weight, mode='fan_in', nonlinearity='relu')\n",
        "        if module.bias is not None:\n",
        "            nn.init.zeros_(module.bias)\n",
        "    elif isinstance(module, nn.Conv3d):\n",
        "        nn.init.kaiming_normal_(module.weight, mode='fan_in', nonlinearity='relu')\n",
        "        if module.bias is not None:\n",
        "            nn.init.zeros_(module.bias)\n",
        "\n",
        "model.apply(initialize_weights)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = DiceLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "# Define learning rate scheduler and early stopping parameters\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.25, patience=5, verbose=True, min_lr=1e-9)\n",
        "early_stopping_patience = 9\n",
        "best_val_loss = np.inf\n",
        "epochs_no_improve = 0\n",
        "\n",
        "# =============================== Training Loop ===========================================\n",
        "\n",
        "num_epochs = 1  # You can adjust the number of epochs\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for images, masks in train_loader:\n",
        "        images = images.to(device)  # (B, 3, 256, 256)\n",
        "        masks = masks.to(device)    # (B, 1, 256, 256)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)      # (B, 1, 256, 256)\n",
        "        loss = criterion(outputs, masks)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader.dataset)\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for images, masks in valid_loader:\n",
        "            images = images.to(device)\n",
        "            masks = masks.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, masks)\n",
        "\n",
        "            val_loss += loss.item() * images.size(0)\n",
        "\n",
        "    val_loss /= len(valid_loader.dataset)\n",
        "\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, Training Loss: {epoch_loss:.4f}, Validation Loss: {val_loss:.4f}')\n",
        "\n",
        "    # Scheduler step\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "    # Early stopping\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        epochs_no_improve = 0\n",
        "        # Save the best model\n",
        "        torch.save(model.state_dict(), r'/content/drive/MyDrive/model/modelWeights_Swin_Trans_Weights_Swin_Trans_Leather.pth')\n",
        "    else:\n",
        "        epochs_no_improve += 1\n",
        "        if epochs_no_improve >= early_stopping_patience:\n",
        "            print('Early stopping!')\n",
        "            break\n",
        "\n",
        "# ================================== Prediction ==========================================\n",
        "\n",
        "# Load the best model weights\n",
        "model.load_state_dict(torch.load(r'/content/drive/MyDrive/model/modelWeights_Swin_Trans_Weights_Swin_Trans_Leather.pth'))\n",
        "model.eval()\n",
        "\n",
        "# Function to save predictions and ground truth\n",
        "def save_predictions(model, dataloader, save_dir_pred, save_dir_gt, device):\n",
        "    \"\"\"\n",
        "    Saves the predicted masks and ground truth masks.\n",
        "    Args:\n",
        "        model (nn.Module): Trained model.\n",
        "        dataloader (DataLoader): DataLoader for test data.\n",
        "        save_dir_pred (str): Directory to save predicted masks.\n",
        "        save_dir_gt (str): Directory to save ground truth masks.\n",
        "        device (str): Device to run the model on.\n",
        "    \"\"\"\n",
        "    os.makedirs(save_dir_pred, exist_ok=True)\n",
        "    os.makedirs(save_dir_gt, exist_ok=True)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (images, masks) in enumerate(dataloader):\n",
        "            if (i % 100 == 0):\n",
        "                print(f\"{i}th Test Image\")  # There are total 1000 test images.\n",
        "\n",
        "            images = images.to(device)\n",
        "            masks = masks.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            preds = outputs.cpu().numpy()\n",
        "            gts = masks.cpu().numpy()\n",
        "\n",
        "            for j in range(preds.shape[0]):\n",
        "                pred_mask = preds[j, 0, :, :]\n",
        "                gt_mask = gts[j, 0, :, :]\n",
        "\n",
        "                # Save predicted mask\n",
        "                pred_img = Image.fromarray((pred_mask * 255).astype(np.uint8))\n",
        "                pred_img.save(os.path.join(save_dir_pred, f\"{i * dataloader.batch_size + j + 1}.png\"))\n",
        "\n",
        "                # Save ground truth mask\n",
        "                gt_img = Image.fromarray((gt_mask * 255).astype(np.uint8))\n",
        "                gt_img.save(os.path.join(save_dir_gt, f\"{i * dataloader.batch_size + j + 1}.tiff\"))\n",
        "\n",
        "# Define directories to save predictions and ground truth\n",
        "save_dir_pred = r'/content/drive/MyDrive/output/segmented predicted images'\n",
        "save_dir_gt = r'/content/drive/MyDrive/output/segmented ground truth'\n",
        "\n",
        "# Save predictions\n",
        "save_predictions(model, test_loader, save_dir_pred, save_dir_gt, device)\n",
        "\n",
        "# =================================== Evaluation =========================================\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, precision_recall_curve\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "def evaluate_metrics_pytorch(model, dataloader, device):\n",
        "    \"\"\"\n",
        "    Evaluates various metrics for segmentation performance.\n",
        "    Args:\n",
        "        model (nn.Module): Trained model.\n",
        "        dataloader (DataLoader): DataLoader for test data.\n",
        "        device (str): Device to run the model on.\n",
        "    Returns:\n",
        "        dict: Dictionary containing average metrics.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    all_accuracy = []\n",
        "    all_dice = []\n",
        "    all_jaccard = []\n",
        "    all_sensitivity = []\n",
        "    all_specificity = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, masks in dataloader:\n",
        "            images = images.to(device)\n",
        "            masks = masks.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            preds = outputs > 0.5  # Binary mask\n",
        "\n",
        "            preds = preds.cpu().numpy().astype(np.uint8)\n",
        "            masks = masks.cpu().numpy().astype(np.uint8)\n",
        "            masks = (masks > 0).astype(np.uint8)  # Convert 255 to 1\n",
        "\n",
        "            for pred, mask in zip(preds, masks):\n",
        "                pred_flat = pred.flatten()\n",
        "                mask_flat = mask.flatten()\n",
        "\n",
        "                # Precision-Recall Curve to find optimal threshold\n",
        "                precisions, recalls, thresholds = precision_recall_curve(mask_flat, pred_flat)\n",
        "                f1 = 2 * (precisions * recalls) / (precisions + recalls + 1e-8)\n",
        "                max_idx = np.argmax(f1)\n",
        "                optimal_thresh = thresholds[max_idx] if max_idx < len(thresholds) else 0.5\n",
        "\n",
        "                # Apply optimal threshold\n",
        "                pred_opt = (pred_flat >= optimal_thresh).astype(np.uint8)\n",
        "\n",
        "                # Confusion matrix with specified labels\n",
        "                cm = confusion_matrix(mask_flat, pred_opt, labels=[0,1])\n",
        "\n",
        "                # Unpack confusion matrix\n",
        "                tn, fp, fn, tp = cm.ravel()\n",
        "\n",
        "                # Calculate metrics\n",
        "                accuracy = (tp + tn) / (tp + tn + fp + fn + 1e-8)\n",
        "                iou = tp / (tp + fp + fn + 1e-8)\n",
        "                dice = (2 * tp) / (2 * tp + fp + fn + 1e-8)\n",
        "                specificity = tn / (tn + fp + 1e-8)\n",
        "                sensitivity = tp / (tp + fn + 1e-8)\n",
        "\n",
        "                all_accuracy.append(accuracy)\n",
        "                all_jaccard.append(iou)\n",
        "                all_dice.append(dice)\n",
        "                all_specificity.append(specificity)\n",
        "                all_sensitivity.append(sensitivity)\n",
        "\n",
        "    # Compute average metrics\n",
        "    metrics = {\n",
        "        'Accuracy': np.mean(all_accuracy),\n",
        "        'Dice': np.mean(all_dice),\n",
        "        'Jaccard': np.mean(all_jaccard),\n",
        "        'Sensitivity': np.mean(all_sensitivity),\n",
        "        'Specificity': np.mean(all_specificity)\n",
        "    }\n",
        "\n",
        "    print(f\"Accuracy: {metrics['Accuracy']:.4f}, Dice: {metrics['Dice']:.4f}, Jaccard: {metrics['Jaccard']:.4f}, \"\n",
        "          f\"Sensitivity: {metrics['Sensitivity']:.4f}, Specificity: {metrics['Specificity']:.4f}\")\n",
        "\n",
        "    return metrics\n",
        "\n",
        "# Evaluate the model\n",
        "metrics = evaluate_metrics_pytorch(model, test_loader, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jl-MfBPqF2gz"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t6bGr9s6F2eQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K1U6hinKF2cY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GH6UQBRlF2XI"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r9jAocYfF2UM"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P3YPZryQF2Ra"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qk3C6Xi1F2OR"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q4wPms0cF2LI"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yaBzNHvSF2Hb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ugAHfwbPXbfu"
      },
      "source": [
        "BIDIRECTIONAL CONVLSTM WITH MORE EVALUATION METRICS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gKXumEbnDQfp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a7add67-1ea4-4a76-e4ec-515e25c76180"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/1\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train loader image count: 1\n",
            "Train loader image count: 2\n",
            "Train loader image count: 3\n",
            "Train loader image count: 4\n",
            "Train loader image count: 5\n",
            "Train loader image count: 6\n",
            "Train loader image count: 7\n",
            "Train loader image count: 8\n",
            "Train loader image count: 9\n",
            "Train loader image count: 10\n",
            "Train loader image count: 11\n",
            "Train loader image count: 12\n",
            "Train loader image count: 13\n",
            "Train loader image count: 14\n",
            "Train loader image count: 15\n",
            "Train loader image count: 16\n",
            "Train loader image count: 17\n",
            "Train loader image count: 18\n",
            "Train loader image count: 19\n",
            "Train loader image count: 20\n",
            "Train loader image count: 21\n",
            "Train loader image count: 22\n",
            "Train loader image count: 23\n",
            "Train loader image count: 24\n",
            "Train loader image count: 25\n",
            "Train loader image count: 26\n",
            "Train loader image count: 27\n",
            "Train loader image count: 28\n",
            "Train loader image count: 29\n",
            "Train loader image count: 30\n",
            "Train loader image count: 31\n",
            "Train loader image count: 32\n",
            "Train loader image count: 33\n",
            "Train loader image count: 34\n",
            "Train loader image count: 35\n",
            "Train loader image count: 36\n",
            "Train loader image count: 37\n",
            "Train loader image count: 38\n",
            "Train loader image count: 39\n",
            "Train loader image count: 40\n",
            "Train loader image count: 41\n",
            "Train loader image count: 42\n",
            "Train loader image count: 43\n",
            "Train loader image count: 44\n",
            "Train loader image count: 45\n",
            "Train loader image count: 46\n",
            "Train loader image count: 47\n",
            "Train loader image count: 48\n",
            "Train loader image count: 49\n",
            "Train loader image count: 50\n",
            "Train loader image count: 51\n",
            "Train loader image count: 52\n",
            "Train loader image count: 53\n",
            "Train loader image count: 54\n",
            "Train loader image count: 55\n",
            "Train loader image count: 56\n",
            "Train loader image count: 57\n",
            "Train loader image count: 58\n",
            "Train loader image count: 59\n",
            "Train loader image count: 60\n",
            "Train loader image count: 61\n",
            "Train loader image count: 62\n",
            "Train loader image count: 63\n",
            "Train loader image count: 64\n",
            "Train loader image count: 65\n",
            "Train loader image count: 66\n",
            "Train loader image count: 67\n",
            "Train loader image count: 68\n",
            "Train loader image count: 69\n",
            "Train loader image count: 70\n",
            "Train loader image count: 71\n",
            "Train loader image count: 72\n",
            "Train loader image count: 73\n",
            "Train loader image count: 74\n",
            "Train loader image count: 75\n",
            "Train loader image count: 76\n",
            "Train loader image count: 77\n",
            "Train loader image count: 78\n",
            "Train loader image count: 79\n",
            "Train loader image count: 80\n",
            "Train loader image count: 81\n",
            "Train loader image count: 82\n",
            "Train loader image count: 83\n",
            "Train loader image count: 84\n",
            "Train loader image count: 85\n",
            "Train loader image count: 86\n",
            "Train loader image count: 87\n",
            "Train loader image count: 88\n",
            "Train loader image count: 89\n",
            "Train loader image count: 90\n",
            "Train loader image count: 91\n",
            "Train loader image count: 92\n",
            "Train loader image count: 93\n",
            "Train loader image count: 94\n",
            "Train loader image count: 95\n",
            "Train loader image count: 96\n",
            "Train loader image count: 97\n",
            "Train loader image count: 98\n",
            "Train loader image count: 99\n",
            "Train loader image count: 100\n",
            "Train loader image count: 101\n",
            "Train loader image count: 102\n",
            "Train loader image count: 103\n",
            "Train loader image count: 104\n",
            "Train loader image count: 105\n",
            "Train loader image count: 106\n",
            "Train loader image count: 107\n",
            "Train loader image count: 108\n",
            "Train loader image count: 109\n",
            "Train loader image count: 110\n",
            "Train loader image count: 111\n",
            "Train loader image count: 112\n",
            "Train loader image count: 113\n",
            "Train loader image count: 114\n",
            "Train loader image count: 115\n",
            "Train loader image count: 116\n",
            "Train loader image count: 117\n",
            "Train loader image count: 118\n",
            "Train loader image count: 119\n",
            "Train loader image count: 120\n",
            "Train loader image count: 121\n",
            "Train loader image count: 122\n",
            "Train loader image count: 123\n",
            "Train loader image count: 124\n",
            "Train loader image count: 125\n",
            "Train loader image count: 126\n",
            "Train loader image count: 127\n",
            "Train loader image count: 128\n",
            "Train loader image count: 129\n",
            "Train loader image count: 130\n",
            "Train loader image count: 131\n",
            "Train loader image count: 132\n",
            "Train loader image count: 133\n",
            "Train loader image count: 134\n",
            "Train loader image count: 135\n",
            "Train loader image count: 136\n",
            "Train loader image count: 137\n",
            "Train loader image count: 138\n",
            "Train loader image count: 139\n",
            "Train loader image count: 140\n",
            "Train loader image count: 141\n",
            "Train loader image count: 142\n",
            "Train loader image count: 143\n",
            "Train loader image count: 144\n",
            "Train loader image count: 145\n",
            "Train loader image count: 146\n",
            "Train loader image count: 147\n",
            "Train loader image count: 148\n",
            "Train loader image count: 149\n",
            "Train loader image count: 150\n",
            "Train loader image count: 151\n",
            "Train loader image count: 152\n",
            "Train loader image count: 153\n",
            "Train loader image count: 154\n",
            "Train loader image count: 155\n",
            "Train loader image count: 156\n",
            "Train loader image count: 157\n",
            "Train loader image count: 158\n",
            "Train loader image count: 159\n",
            "Train loader image count: 160\n",
            "Train loader image count: 161\n",
            "Train loader image count: 162\n",
            "Train loader image count: 163\n",
            "Train loader image count: 164\n",
            "Train loader image count: 165\n",
            "Train loader image count: 166\n",
            "Train loader image count: 167\n",
            "Train loader image count: 168\n",
            "Train loader image count: 169\n",
            "Train loader image count: 170\n",
            "Train loader image count: 171\n",
            "Train loader image count: 172\n",
            "Train loader image count: 173\n",
            "Train loader image count: 174\n",
            "Train loader image count: 175\n",
            "Train loader image count: 176\n",
            "Train loader image count: 177\n",
            "Train loader image count: 178\n",
            "Train loader image count: 179\n",
            "Train loader image count: 180\n",
            "Train loader image count: 181\n",
            "Train loader image count: 182\n",
            "Train loader image count: 183\n",
            "Train loader image count: 184\n",
            "Train loader image count: 185\n",
            "Train loader image count: 186\n",
            "Train loader image count: 187\n",
            "Train loader image count: 188\n",
            "Train loader image count: 189\n",
            "Train loader image count: 190\n",
            "Train loader image count: 191\n",
            "Train loader image count: 192\n",
            "Train loader image count: 193\n",
            "Train loader image count: 194\n",
            "Train loader image count: 195\n",
            "Train loader image count: 196\n",
            "Train loader image count: 197\n",
            "Train loader image count: 198\n",
            "Train loader image count: 199\n",
            "Train loader image count: 200\n",
            "Train loader image count: 201\n",
            "Train loader image count: 202\n",
            "Train loader image count: 203\n",
            "Train loader image count: 204\n",
            "Train loader image count: 205\n",
            "Train loader image count: 206\n",
            "Train loader image count: 207\n",
            "Train loader image count: 208\n",
            "Train loader image count: 209\n",
            "Train loader image count: 210\n",
            "Train loader image count: 211\n",
            "Train loader image count: 212\n",
            "Train loader image count: 213\n",
            "Train loader image count: 214\n",
            "Train loader image count: 215\n",
            "Train loader image count: 216\n",
            "Train loader image count: 217\n",
            "Train loader image count: 218\n",
            "Train loader image count: 219\n",
            "Train loader image count: 220\n",
            "Train loader image count: 221\n",
            "Train loader image count: 222\n",
            "Train loader image count: 223\n",
            "Train loader image count: 224\n",
            "Train loader image count: 225\n",
            "Train loader image count: 226\n",
            "Train loader image count: 227\n",
            "Train loader image count: 228\n",
            "Train loader image count: 229\n",
            "Train loader image count: 230\n",
            "Train loader image count: 231\n",
            "Train loader image count: 232\n",
            "Train loader image count: 233\n",
            "Train loader image count: 234\n",
            "Train loader image count: 235\n",
            "Train loader image count: 236\n",
            "Train loader image count: 237\n",
            "Train loader image count: 238\n",
            "Train loader image count: 239\n",
            "Train loader image count: 240\n",
            "Train loader image count: 241\n",
            "Train loader image count: 242\n",
            "Train loader image count: 243\n",
            "Train loader image count: 244\n",
            "Train loader image count: 245\n",
            "Train loader image count: 246\n",
            "Train loader image count: 247\n",
            "Train loader image count: 248\n",
            "Train loader image count: 249\n",
            "Train loader image count: 250\n",
            "Train loader image count: 251\n",
            "Train loader image count: 252\n",
            "Train loader image count: 253\n",
            "Train loader image count: 254\n",
            "Train loader image count: 255\n",
            "Train loader image count: 256\n",
            "Train loader image count: 257\n",
            "Train loader image count: 258\n",
            "Train loader image count: 259\n",
            "Train loader image count: 260\n",
            "Train loader image count: 261\n",
            "Train loader image count: 262\n",
            "Train loader image count: 263\n",
            "Train loader image count: 264\n",
            "Train loader image count: 265\n",
            "Train loader image count: 266\n",
            "Train loader image count: 267\n",
            "Train loader image count: 268\n",
            "Train loader image count: 269\n",
            "Train loader image count: 270\n",
            "Train loader image count: 271\n",
            "Train loader image count: 272\n",
            "Train loader image count: 273\n",
            "Train loader image count: 274\n",
            "Train loader image count: 275\n",
            "Train loader image count: 276\n",
            "Train loader image count: 277\n",
            "Train loader image count: 278\n",
            "Train loader image count: 279\n",
            "Train loader image count: 280\n",
            "Train loader image count: 281\n",
            "Train loader image count: 282\n",
            "Train loader image count: 283\n",
            "Train loader image count: 284\n",
            "Train loader image count: 285\n",
            "Train loader image count: 286\n",
            "Train loader image count: 287\n",
            "Train loader image count: 288\n",
            "Train loader image count: 289\n",
            "Train loader image count: 290\n",
            "Train loader image count: 291\n",
            "Train loader image count: 292\n",
            "Train loader image count: 293\n",
            "Train loader image count: 294\n",
            "Train loader image count: 295\n",
            "Train loader image count: 296\n",
            "Train loader image count: 297\n",
            "Train loader image count: 298\n",
            "Train loader image count: 299\n",
            "Train loader image count: 300\n",
            "Train loader image count: 301\n",
            "Train loader image count: 302\n",
            "Train loader image count: 303\n",
            "Train loader image count: 304\n",
            "Train loader image count: 305\n",
            "Train loader image count: 306\n",
            "Train loader image count: 307\n",
            "Train loader image count: 308\n",
            "Train loader image count: 309\n",
            "Train loader image count: 310\n",
            "Train loader image count: 311\n",
            "Train loader image count: 312\n",
            "Train loader image count: 313\n",
            "Train loader image count: 314\n",
            "Train loader image count: 315\n",
            "Train loader image count: 316\n",
            "Train loader image count: 317\n",
            "Train loader image count: 318\n",
            "Train loader image count: 319\n",
            "Train loader image count: 320\n",
            "Train loader image count: 321\n",
            "Train loader image count: 322\n",
            "Train loader image count: 323\n",
            "Train loader image count: 324\n",
            "Train loader image count: 325\n",
            "Train loader image count: 326\n",
            "Train loader image count: 327\n",
            "Train loader image count: 328\n",
            "Train loader image count: 329\n",
            "Train loader image count: 330\n",
            "Train loader image count: 331\n",
            "Train loader image count: 332\n",
            "Train loader image count: 333\n",
            "Train loader image count: 334\n",
            "Train loader image count: 335\n",
            "Train loader image count: 336\n",
            "Train loader image count: 337\n",
            "Train loader image count: 338\n",
            "Train loader image count: 339\n",
            "Train loader image count: 340\n",
            "Train loader image count: 341\n",
            "Train loader image count: 342\n",
            "Train loader image count: 343\n",
            "Train loader image count: 344\n",
            "Train loader image count: 345\n",
            "Train loader image count: 346\n",
            "Train loader image count: 347\n",
            "Train loader image count: 348\n",
            "Train loader image count: 349\n",
            "Train loader image count: 350\n",
            "Train loader image count: 351\n",
            "Train loader image count: 352\n",
            "Train loader image count: 353\n",
            "Train loader image count: 354\n",
            "Train loader image count: 355\n",
            "Train loader image count: 356\n",
            "Train loader image count: 357\n",
            "Train loader image count: 358\n",
            "Train loader image count: 359\n",
            "Train loader image count: 360\n",
            "Train loader image count: 361\n",
            "Train loader image count: 362\n",
            "Train loader image count: 363\n",
            "Train loader image count: 364\n",
            "Train loader image count: 365\n",
            "Train loader image count: 366\n",
            "Train loader image count: 367\n",
            "Train loader image count: 368\n",
            "Train loader image count: 369\n",
            "Train loader image count: 370\n",
            "Train loader image count: 371\n",
            "Train loader image count: 372\n",
            "Train loader image count: 373\n",
            "Train loader image count: 374\n",
            "Train loader image count: 375\n",
            "Train loader image count: 376\n",
            "Train loader image count: 377\n",
            "Train loader image count: 378\n",
            "Train loader image count: 379\n",
            "Train loader image count: 380\n",
            "Train loader image count: 381\n",
            "Train loader image count: 382\n",
            "Train loader image count: 383\n",
            "Train loader image count: 384\n",
            "Train loader image count: 385\n",
            "Train loader image count: 386\n",
            "Train loader image count: 387\n",
            "Train loader image count: 388\n",
            "Train loader image count: 389\n",
            "Train loader image count: 390\n",
            "Train loader image count: 391\n",
            "Train loader image count: 392\n",
            "Train loader image count: 393\n",
            "Train loader image count: 394\n",
            "Train loader image count: 395\n",
            "Train loader image count: 396\n",
            "Train loader image count: 397\n",
            "Train loader image count: 398\n",
            "Train loader image count: 399\n",
            "Train loader image count: 400\n",
            "Train loader image count: 401\n",
            "Train loader image count: 402\n",
            "Train loader image count: 403\n",
            "Train loader image count: 404\n",
            "Train loader image count: 405\n",
            "Train loader image count: 406\n",
            "Train loader image count: 407\n",
            "Train loader image count: 408\n",
            "Train loader image count: 409\n",
            "Train loader image count: 410\n",
            "Train loader image count: 411\n",
            "Train loader image count: 412\n",
            "Train loader image count: 413\n",
            "Train loader image count: 414\n",
            "Train loader image count: 415\n",
            "Epoch 1/1, Training Loss: 0.5706, Validation Loss: 0.6583\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-a1466926ff51>:753: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(r'/content/drive/MyDrive/model/modelWeights_Swin_Trans_Weights_Swin_Trans_Leather.pth'))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0th Test Image\n",
            "100th Test Image\n",
            "Evaluation Metrics:\n",
            "Accuracy: 0.7963\n",
            "Dice: 0.4221\n",
            "Jaccard: 0.3285\n",
            "Sensitivity: 0.3730\n",
            "Specificity: 0.9808\n",
            "Precision: 0.7010\n",
            "Recall: 0.3730\n",
            "F1-Score: 0.4221\n"
          ]
        }
      ],
      "source": [
        "# ============================== Imports and Dependencies ==============================\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import (\n",
        "    precision_recall_curve, confusion_matrix,\n",
        "    jaccard_score, f1_score, precision_score, recall_score\n",
        ")\n",
        "\n",
        "# ================================ Separable Convolution =================================\n",
        "\n",
        "class SeparableConv2d(nn.Module):\n",
        "    \"\"\"\n",
        "    Implements a separable convolution layer using depthwise and pointwise convolutions.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, padding=1, bias=True):\n",
        "        super(SeparableConv2d, self).__init__()\n",
        "        # Depthwise convolution (groups=in_channels)\n",
        "        self.depthwise = nn.Conv2d(in_channels, in_channels, kernel_size=kernel_size,\n",
        "                                   padding=padding, groups=in_channels, bias=bias)\n",
        "        # Pointwise convolution\n",
        "        self.pointwise = nn.Conv2d(in_channels, out_channels, kernel_size=1,\n",
        "                                   padding=0, bias=bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.depthwise(x)\n",
        "        x = self.pointwise(x)\n",
        "        return x\n",
        "\n",
        "# ================================== ConvLSTM2D ========================================\n",
        "\n",
        "class ConvLSTMCell(nn.Module):\n",
        "    \"\"\"\n",
        "    Implements a ConvLSTM cell.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_channels, hidden_channels, kernel_size, bias=True):\n",
        "        super(ConvLSTMCell, self).__init__()\n",
        "\n",
        "        padding = kernel_size // 2  # To maintain spatial dimensions\n",
        "        self.input_channels = input_channels\n",
        "        self.hidden_channels = hidden_channels\n",
        "\n",
        "        self.conv = nn.Conv2d(in_channels=input_channels + hidden_channels,\n",
        "                              out_channels=4 * hidden_channels,\n",
        "                              kernel_size=kernel_size,\n",
        "                              padding=padding,\n",
        "                              bias=bias)\n",
        "\n",
        "    def forward(self, input_tensor, cur_state):\n",
        "        h_cur, c_cur = cur_state\n",
        "\n",
        "        # Concatenate input and hidden state\n",
        "        combined = torch.cat([input_tensor, h_cur], dim=1)  # along channel axis\n",
        "\n",
        "        # Compute all gates at once\n",
        "        conv_output = self.conv(combined)\n",
        "        cc_i, cc_f, cc_o, cc_g = torch.split(conv_output, self.hidden_channels, dim=1)\n",
        "\n",
        "        i = torch.sigmoid(cc_i)   # input gate\n",
        "        f = torch.sigmoid(cc_f)   # forget gate\n",
        "        o = torch.sigmoid(cc_o)   # output gate\n",
        "        g = torch.tanh(cc_g)      # gate gate\n",
        "\n",
        "        c_next = f * c_cur + i * g\n",
        "        h_next = o * torch.tanh(c_next)\n",
        "\n",
        "        return h_next, c_next\n",
        "\n",
        "    def init_hidden(self, batch_size, spatial_size, device):\n",
        "        height, width = spatial_size\n",
        "        return (torch.zeros(batch_size, self.hidden_channels, height, width, device=device),\n",
        "                torch.zeros(batch_size, self.hidden_channels, height, width, device=device))\n",
        "\n",
        "class ConvLSTM2D(nn.Module):\n",
        "    \"\"\"\n",
        "    Implements a ConvLSTM2D layer that processes a sequence of inputs.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_channels, hidden_channels, kernel_size=3, bias=True, num_layers=1):\n",
        "        super(ConvLSTM2D, self).__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.hidden_channels = hidden_channels\n",
        "\n",
        "        layers = []\n",
        "        for i in range(num_layers):\n",
        "            input_c = input_channels if i == 0 else hidden_channels\n",
        "            layers.append(ConvLSTMCell(input_c, hidden_channels, kernel_size, bias))\n",
        "        self.layers = nn.ModuleList(layers)\n",
        "\n",
        "    def forward(self, input_tensor, reverse=False):\n",
        "        # input_tensor shape: (batch, seq_len, channels, height, width)\n",
        "        batch_size, seq_len, channels, height, width = input_tensor.size()\n",
        "        device = input_tensor.device\n",
        "\n",
        "        # Initialize hidden and cell states for all layers\n",
        "        hidden_state = []\n",
        "        cell_state = []\n",
        "        for i in range(self.num_layers):\n",
        "            h, c = self.layers[i].init_hidden(batch_size, (height, width), device)\n",
        "            hidden_state.append(h)\n",
        "            cell_state.append(c)\n",
        "\n",
        "        # Iterate over time steps\n",
        "        if reverse:\n",
        "            time_steps = reversed(range(seq_len))\n",
        "        else:\n",
        "            time_steps = range(seq_len)\n",
        "\n",
        "        outputs = []\n",
        "        for t in time_steps:\n",
        "            x = input_tensor[:, t, :, :, :]  # (batch, channels, height, width)\n",
        "            for i, layer in enumerate(self.layers):\n",
        "                h, c = layer(x, (hidden_state[i], cell_state[i]))\n",
        "                hidden_state[i] = h\n",
        "                cell_state[i] = c\n",
        "                x = h  # input to next layer\n",
        "            outputs.append(x)\n",
        "\n",
        "        outputs = torch.stack(outputs, dim=1)  # (batch, seq_len, channels, height, width)\n",
        "        if reverse:\n",
        "            outputs = outputs.flip(dims=[1])  # Reverse back to original order\n",
        "        return outputs  # Return the sequence of outputs\n",
        "\n",
        "class BidirectionalConvLSTM2D(nn.Module):\n",
        "    \"\"\"\n",
        "    Implements a Bidirectional ConvLSTM2D layer.\n",
        "    Processes the input sequence in both forward and backward directions and concatenates the outputs.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_channels, hidden_channels, kernel_size=3, num_layers=1, bias=True):\n",
        "        super(BidirectionalConvLSTM2D, self).__init__()\n",
        "        self.forward_conv_lstm = ConvLSTM2D(input_channels, hidden_channels, kernel_size, bias=bias, num_layers=num_layers)\n",
        "        self.backward_conv_lstm = ConvLSTM2D(input_channels, hidden_channels, kernel_size, bias=bias, num_layers=num_layers)\n",
        "\n",
        "    def forward(self, input_tensor):\n",
        "        # input_tensor shape: (batch, seq_len, channels, height, width)\n",
        "        # Forward direction\n",
        "        forward_output = self.forward_conv_lstm(input_tensor, reverse=False)  # (batch, seq_len, hidden_channels, H, W)\n",
        "        # Backward direction\n",
        "        backward_output = self.backward_conv_lstm(input_tensor, reverse=True)  # (batch, seq_len, hidden_channels, H, W)\n",
        "        # Concatenate outputs along the channel dimension\n",
        "        output = torch.cat([forward_output, backward_output], dim=2)  # (batch, seq_len, hidden_channels*2, H, W)\n",
        "        # Since seq_len=2, we can take the last output\n",
        "        output = output[:, -1, :, :, :]  # Take the last output (batch, hidden_channels*2, H, W)\n",
        "        return output\n",
        "\n",
        "# ============================== Swin Transformer Blocks ================================\n",
        "\n",
        "class WindowAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Window based multi-head self attention (W-MSA) module with relative position bias.\n",
        "    \"\"\"\n",
        "    def __init__(self, dim, window_size, num_heads, qkv_bias=True, attn_drop=0., proj_drop=0.):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            dim (int): Number of input channels.\n",
        "            window_size (tuple): Height and width of the window.\n",
        "            num_heads (int): Number of attention heads.\n",
        "            qkv_bias (bool): If True, add a learnable bias to query, key, value.\n",
        "            attn_drop (float): Dropout ratio of attention weights.\n",
        "            proj_drop (float): Dropout ratio of output.\n",
        "        \"\"\"\n",
        "        super(WindowAttention, self).__init__()\n",
        "        self.dim = dim\n",
        "        self.window_size = window_size  # Wh, Ww\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // num_heads\n",
        "        self.scale = head_dim ** -0.5\n",
        "\n",
        "        # Define a parameter table of relative position bias\n",
        "        self.relative_position_bias_table = nn.Parameter(\n",
        "            torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads)\n",
        "        )  # 2*Wh-1 * 2*Ww-1, nH\n",
        "\n",
        "        # Get pair-wise relative position index for each token inside the window\n",
        "        coords_h = torch.arange(self.window_size[0])\n",
        "        coords_w = torch.arange(self.window_size[1])\n",
        "        coords = torch.stack(torch.meshgrid(coords_h, coords_w, indexing='ij'))  # 2, Wh, Ww\n",
        "        coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n",
        "        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n",
        "        relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n",
        "        relative_coords[:, :, 0] += self.window_size[0] - 1  # shift to start from 0\n",
        "        relative_coords[:, :, 1] += self.window_size[1] - 1\n",
        "        relative_coords[:, :, 0] *= (2 * self.window_size[1] - 1)\n",
        "        relative_position_index = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n",
        "        self.register_buffer(\"relative_position_index\", relative_position_index)\n",
        "\n",
        "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)  # Query, Key, Value\n",
        "        self.attn_drop = nn.Dropout(attn_drop)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.proj_drop = nn.Dropout(proj_drop)\n",
        "\n",
        "        # Initialize relative position bias table\n",
        "        nn.init.trunc_normal_(self.relative_position_bias_table, std=.02)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: input features with shape of (num_windows*B, Wh*Ww, C)\n",
        "            mask: (num_windows, Wh*Ww, Wh*Ww) or None\n",
        "        \"\"\"\n",
        "        B_, N, C = x.shape\n",
        "        qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)  # 3, B_, nH, N, C//nH\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]  # each has shape (B_, nH, N, C//nH)\n",
        "\n",
        "        q = q * self.scale\n",
        "        attn = (q @ k.transpose(-2, -1))  # (B_, nH, N, N)\n",
        "\n",
        "        # Add relative position bias\n",
        "        relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(\n",
        "            self.window_size[0] * self.window_size[1], self.window_size[0] * self.window_size[1], -1\n",
        "        )  # Wh*Ww, Wh*Ww, nH\n",
        "        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n",
        "        attn = attn + relative_position_bias.unsqueeze(0)  # (B_, nH, N, N)\n",
        "\n",
        "        if mask is not None:\n",
        "            nW = mask.shape[0]\n",
        "            attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)\n",
        "            attn = attn.view(-1, self.num_heads, N, N)\n",
        "            attn = F.softmax(attn, dim=-1)\n",
        "        else:\n",
        "            attn = F.softmax(attn, dim=-1)\n",
        "\n",
        "        attn = self.attn_drop(attn)\n",
        "\n",
        "        out = (attn @ v).transpose(1, 2).reshape(B_, N, C)  # (B_, N, C)\n",
        "        out = self.proj(out)\n",
        "        out = self.proj_drop(out)\n",
        "        return out\n",
        "\n",
        "class SwinTransformerBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Swin Transformer Block with W-MSA and SW-MSA.\n",
        "    \"\"\"\n",
        "    def __init__(self, dim, num_heads, window_size=7, shift_size=0, mlp_ratio=4., qkv_bias=True,\n",
        "                 attn_drop=0., proj_drop=0.):\n",
        "        super(SwinTransformerBlock, self).__init__()\n",
        "        self.dim = dim\n",
        "        self.num_heads = num_heads\n",
        "        self.window_size = window_size  # W\n",
        "        self.shift_size = shift_size    # S\n",
        "        self.mlp_ratio = mlp_ratio\n",
        "\n",
        "        assert 0 <= self.shift_size < self.window_size, \"shift_size must be in [0, window_size)\"\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(dim)\n",
        "        self.attn = WindowAttention(dim, (window_size, window_size), num_heads, qkv_bias, attn_drop, proj_drop)\n",
        "\n",
        "        self.drop_path = nn.Identity()  # Can implement stochastic depth if desired\n",
        "        self.norm2 = nn.LayerNorm(dim)\n",
        "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(dim, mlp_hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(mlp_hidden_dim, dim),\n",
        "            nn.Dropout(proj_drop)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: input features with shape (B, H*W, C)\n",
        "        \"\"\"\n",
        "        H = W = int(np.sqrt(x.shape[1]))\n",
        "        B, L, C = x.shape\n",
        "        assert L == H * W, \"Input feature has wrong size\"\n",
        "\n",
        "        shortcut = x\n",
        "        x = self.norm1(x)\n",
        "        x = x.view(B, H, W, C)\n",
        "\n",
        "        # Cyclic shift\n",
        "        if self.shift_size > 0:\n",
        "            shifted_x = torch.roll(x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2))\n",
        "        else:\n",
        "            shifted_x = x\n",
        "\n",
        "        # Partition windows\n",
        "        window_size = self.window_size\n",
        "        # Pad H and W to be multiples of window_size\n",
        "        pad_b = (window_size - H % window_size) % window_size\n",
        "        pad_r = (window_size - W % window_size) % window_size\n",
        "        shifted_x = F.pad(shifted_x, (0, 0, 0, pad_r, 0, pad_b))  # pad H and W\n",
        "        _, Hp, Wp, _ = shifted_x.shape\n",
        "\n",
        "        # Window partition\n",
        "        x_windows = shifted_x.view(B, Hp // window_size, window_size, Wp // window_size, window_size, C)\n",
        "        x_windows = x_windows.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size * window_size, C)  # (num_windows*B, window_size*window_size, C)\n",
        "\n",
        "        # Attention\n",
        "        attn_windows = self.attn(x_windows)  # (num_windows*B, window_size*window_size, C)\n",
        "\n",
        "        # Merge windows\n",
        "        shifted_x = attn_windows.view(-1, window_size, window_size, C)\n",
        "        shifted_x = shifted_x.view(B, Hp // window_size, Wp // window_size, window_size, window_size, C)\n",
        "        shifted_x = shifted_x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, Hp, Wp, C)\n",
        "\n",
        "        # Reverse cyclic shift\n",
        "        if self.shift_size > 0:\n",
        "            x = torch.roll(shifted_x, shifts=(self.shift_size, self.shift_size), dims=(1, 2))\n",
        "        else:\n",
        "            x = shifted_x\n",
        "\n",
        "        # Remove padding\n",
        "        x = x[:, :H, :W, :].contiguous().view(B, H * W, C)\n",
        "\n",
        "        # FFN\n",
        "        x = shortcut + self.drop_path(x)\n",
        "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
        "\n",
        "        return x\n",
        "\n",
        "# =============================== Dice Loss Function ====================================\n",
        "\n",
        "class DiceLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Dice Loss function to maximize the Dice coefficient.\n",
        "    Suitable for binary segmentation tasks.\n",
        "    \"\"\"\n",
        "    def __init__(self, smooth=1.0):\n",
        "        super(DiceLoss, self).__init__()\n",
        "        self.smooth = smooth\n",
        "\n",
        "    def forward(self, y_pred, y_true):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            y_pred (torch.Tensor): Predicted mask probabilities with shape (B, 1, H, W)\n",
        "            y_true (torch.Tensor): Ground truth masks with shape (B, 1, H, W)\n",
        "        Returns:\n",
        "            torch.Tensor: Dice loss\n",
        "        \"\"\"\n",
        "        y_pred = y_pred.view(-1)\n",
        "        y_true = y_true.view(-1)\n",
        "\n",
        "        intersection = (y_pred * y_true).sum()\n",
        "        dice = (2. * intersection + self.smooth) / (y_pred.sum() + y_true.sum() + self.smooth)\n",
        "\n",
        "        return 1 - dice\n",
        "\n",
        "# ================================ Main Model ============================================\n",
        "\n",
        "class SwinUNet(nn.Module):\n",
        "    \"\"\"\n",
        "    Swin U-Net architecture for image segmentation with bidirectional ConvLSTM layers.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_channels=3, output_channels=1,\n",
        "                 embed_dim=32, num_heads=[4, 8], window_size=4,\n",
        "                 mlp_ratio=4., depth=2):\n",
        "        super(SwinUNet, self).__init__()\n",
        "        self.input_channels = input_channels\n",
        "        self.output_channels = output_channels\n",
        "\n",
        "        # Initial convolutional layers\n",
        "        self.conv1 = SeparableConv2d(input_channels, 24, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(24)\n",
        "        self.conv2 = SeparableConv2d(24, 24, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(24)\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)  # 256x256 -> 128x128\n",
        "\n",
        "        # First Swin Transformer Block\n",
        "        self.swin_unet_E1 = SwinTransformerBlock(\n",
        "            dim=24,  # Changed from embed_dim=32 to 24\n",
        "            num_heads=num_heads[0],\n",
        "            window_size=window_size,\n",
        "            shift_size=window_size//2 if True else 0,\n",
        "            mlp_ratio=mlp_ratio\n",
        "        )\n",
        "\n",
        "        # Second convolutional block\n",
        "        self.conv3 = SeparableConv2d(24, 48, kernel_size=3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(48)\n",
        "        self.conv4 = SeparableConv2d(48, 48, kernel_size=3, padding=1)\n",
        "        self.bn4 = nn.BatchNorm2d(48)\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)  # 128x128 -> 64x64\n",
        "\n",
        "        # Second Swin Transformer Block\n",
        "        self.swin_unet_E2 = SwinTransformerBlock(\n",
        "            dim=48,\n",
        "            num_heads=num_heads[1],\n",
        "            window_size=window_size,\n",
        "            shift_size=window_size//2 if True else 0,\n",
        "            mlp_ratio=mlp_ratio\n",
        "        )\n",
        "\n",
        "        # Third convolutional block (Bottleneck)\n",
        "        self.conv5 = SeparableConv2d(48, 96, kernel_size=3, padding=1)\n",
        "        self.bn5 = nn.BatchNorm2d(96)\n",
        "        self.conv6 = SeparableConv2d(96, 96, kernel_size=3, padding=1)\n",
        "        self.bn6 = nn.BatchNorm2d(96)\n",
        "        self.drop5 = nn.Dropout(0.5)\n",
        "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)  # 64x64 -> 32x32\n",
        "\n",
        "        # Bottleneck convolutions with dense connections\n",
        "        self.conv7 = SeparableConv2d(96, 192, kernel_size=3, padding=1)\n",
        "        self.bn7 = nn.BatchNorm2d(192)\n",
        "        self.conv8 = SeparableConv2d(192, 192, kernel_size=3, padding=1)\n",
        "        self.bn8 = nn.BatchNorm2d(192)\n",
        "        self.drop6_1 = nn.Dropout(0.5)\n",
        "\n",
        "        self.conv9 = SeparableConv2d(192, 192, kernel_size=3, padding=1)\n",
        "        self.bn9 = nn.BatchNorm2d(192)\n",
        "        self.conv10 = SeparableConv2d(192, 192, kernel_size=3, padding=1)\n",
        "        self.bn10 = nn.BatchNorm2d(192)\n",
        "        self.drop6_2 = nn.Dropout(0.5)\n",
        "\n",
        "        self.concat1 = nn.Sequential(\n",
        "            SeparableConv2d(384, 192, kernel_size=3, padding=1),\n",
        "            SeparableConv2d(192, 192, kernel_size=3, padding=1)\n",
        "        )\n",
        "        self.drop6_3 = nn.Dropout(0.5)\n",
        "\n",
        "        # First Upsampling Block\n",
        "        self.up1 = nn.ConvTranspose2d(192, 96, kernel_size=2, stride=2)  # 32x32 -> 64x64\n",
        "        self.bn_up1 = nn.BatchNorm2d(96)\n",
        "        self.relu_up1 = nn.ReLU(inplace=True)\n",
        "        self.bidirectional_convLSTM1 = BidirectionalConvLSTM2D(input_channels=96, hidden_channels=192, kernel_size=3, num_layers=1)\n",
        "        self.swin_unet_D1 = SwinTransformerBlock(\n",
        "            dim=192 * 2,  # Adjusted for bidirectional output\n",
        "            num_heads=num_heads[0],\n",
        "            window_size=window_size,\n",
        "            shift_size=window_size//2 if True else 0,\n",
        "            mlp_ratio=mlp_ratio\n",
        "        )\n",
        "        self.conv11 = SeparableConv2d(192 * 2, 48, kernel_size=3, padding=1)\n",
        "        self.conv12 = SeparableConv2d(48, 48, kernel_size=3, padding=1)\n",
        "\n",
        "        # Second Upsampling Block\n",
        "        self.up2 = nn.ConvTranspose2d(48, 48, kernel_size=2, stride=2)  # 64x64 -> 128x128\n",
        "        self.bn_up2 = nn.BatchNorm2d(48)\n",
        "        self.relu_up2 = nn.ReLU(inplace=True)\n",
        "        self.bidirectional_convLSTM2 = BidirectionalConvLSTM2D(input_channels=48, hidden_channels=96, kernel_size=3, num_layers=1)\n",
        "        self.swin_unet_D2 = SwinTransformerBlock(\n",
        "            dim=96 * 2,\n",
        "            num_heads=num_heads[1],\n",
        "            window_size=window_size,\n",
        "            shift_size=window_size//2 if True else 0,\n",
        "            mlp_ratio=mlp_ratio\n",
        "        )\n",
        "        self.conv13 = SeparableConv2d(96 * 2, 24, kernel_size=3, padding=1)\n",
        "        self.conv14 = SeparableConv2d(24, 24, kernel_size=3, padding=1)\n",
        "\n",
        "        # Third Upsampling Block\n",
        "        self.up3 = nn.ConvTranspose2d(24, 24, kernel_size=2, stride=2)  # 128x128 -> 256x256\n",
        "        self.bn_up3 = nn.BatchNorm2d(24)\n",
        "        self.relu_up3 = nn.ReLU(inplace=True)\n",
        "        self.bidirectional_convLSTM3 = BidirectionalConvLSTM2D(input_channels=24, hidden_channels=48, kernel_size=3, num_layers=1)\n",
        "        self.swin_unet_D3 = SwinTransformerBlock(\n",
        "            dim=48 * 2,\n",
        "            num_heads=num_heads[1],\n",
        "            window_size=window_size,\n",
        "            shift_size=window_size//2 if True else 0,\n",
        "            mlp_ratio=mlp_ratio\n",
        "        )\n",
        "        self.conv15 = SeparableConv2d(48 * 2, 24, kernel_size=3, padding=1)\n",
        "        self.conv16 = SeparableConv2d(24, 24, kernel_size=3, padding=1)\n",
        "\n",
        "        # Output Layer\n",
        "        self.final_conv1 = nn.Conv2d(24, 2, kernel_size=3, padding=1)\n",
        "        self.final_relu = nn.ReLU(inplace=True)\n",
        "        self.final_conv2 = nn.Conv2d(2, 1, kernel_size=1, padding=0)\n",
        "        self.final_sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass of the Swin U-Net model.\n",
        "        Args:\n",
        "            x: Input tensor with shape (B, 3, 256, 256)\n",
        "        Returns:\n",
        "            torch.Tensor: Output segmentation mask with shape (B, 1, 256, 256)\n",
        "        \"\"\"\n",
        "        # Initial Convolutions\n",
        "        x1 = self.conv1(x)          # (B, 24, 256, 256)\n",
        "        x1 = self.bn1(x1)\n",
        "        x1 = self.conv2(x1)         # (B, 24, 256, 256)\n",
        "        x1 = self.bn2(x1)\n",
        "        p1 = self.pool1(x1)         # (B, 24, 128, 128)\n",
        "\n",
        "        # First Swin Transformer Block\n",
        "        p1_flat = p1.flatten(2).transpose(1, 2)  # (B, 128*128, 24)\n",
        "        swin_E1 = self.swin_unet_E1(p1_flat)     # (B, 128*128, 24)\n",
        "        swin_E1 = swin_E1.transpose(1, 2).view(-1, 24, 128, 128)  # Reshape for Conv2d\n",
        "\n",
        "        # Second Convolutional Block\n",
        "        x2 = self.conv3(swin_E1)    # (B, 48, 128, 128)\n",
        "        x2 = self.bn3(x2)\n",
        "        x2 = self.conv4(x2)          # (B, 48, 128, 128)\n",
        "        x2 = self.bn4(x2)\n",
        "        p2 = self.pool2(x2)          # (B, 48, 64, 64)\n",
        "\n",
        "        # Second Swin Transformer Block\n",
        "        p2_flat = p2.flatten(2).transpose(1, 2)  # (B, 64*64, 48)\n",
        "        swin_E2 = self.swin_unet_E2(p2_flat)     # (B, 64*64, 48)\n",
        "        swin_E2 = swin_E2.transpose(1, 2).view(-1, 48, 64, 64)  # Reshape for Conv2d\n",
        "\n",
        "        # Third Convolutional Block (Bottleneck)\n",
        "        x3 = self.conv5(swin_E2)    # (B, 96, 64, 64)\n",
        "        x3 = self.bn5(x3)\n",
        "        x3 = self.conv6(x3)          # (B, 96, 64, 64)\n",
        "        x3 = self.bn6(x3)\n",
        "        x3 = self.drop5(x3)\n",
        "        p3 = self.pool3(x3)          # (B, 96, 32, 32)\n",
        "\n",
        "        # Bottleneck Convolutions with Dense Connections\n",
        "        x4 = self.conv7(p3)          # (B, 192, 32, 32)\n",
        "        x4 = self.bn7(x4)\n",
        "        x4 = self.conv8(x4)          # (B, 192, 32, 32)\n",
        "        x4 = self.bn8(x4)\n",
        "        x4 = self.drop6_1(x4)\n",
        "\n",
        "        x5 = self.conv9(x4)          # (B, 192, 32, 32)\n",
        "        x5 = self.bn9(x5)\n",
        "        x5 = self.conv10(x5)         # (B, 192, 32, 32)\n",
        "        x5 = self.bn10(x5)\n",
        "        x5 = self.drop6_2(x5)\n",
        "\n",
        "        concat = torch.cat([x5, x4], dim=1)  # (B, 384, 32, 32)\n",
        "        concat = self.concat1(concat)         # (B, 192, 32, 32)\n",
        "        concat = self.drop6_3(concat)         # (B, 192, 32, 32)\n",
        "\n",
        "        # First Upsampling Block\n",
        "        up1 = self.up1(concat)                 # (B, 96, 64, 64)\n",
        "        up1 = self.bn_up1(up1)\n",
        "        up1 = self.relu_up1(up1)\n",
        "\n",
        "        # Prepare for BidirectionalConvLSTM2D\n",
        "        up1_seq = torch.stack([x3, up1], dim=1)  # (B, 2, 96, 64, 64)\n",
        "        bidir_convLSTM1_out = self.bidirectional_convLSTM1(up1_seq)  # (B, 192*2, 64, 64)\n",
        "\n",
        "        # Swin Transformer Block in Decoder\n",
        "        bidir_convLSTM1_flat = bidir_convLSTM1_out.flatten(2).transpose(1, 2)  # (B, 64*64, 192*2)\n",
        "        swin_D1 = self.swin_unet_D1(bidir_convLSTM1_flat)               # (B, 64*64, 192*2)\n",
        "        swin_D1 = swin_D1.transpose(1, 2).view(-1, 192*2, 64, 64)    # Reshape for Conv2d\n",
        "\n",
        "        # Further Convolutions\n",
        "        conv6 = self.conv11(swin_D1)        # (B, 48, 64, 64)\n",
        "        conv6 = self.conv12(conv6)          # (B, 48, 64, 64)\n",
        "\n",
        "        # Second Upsampling Block\n",
        "        up2 = self.up2(conv6)               # (B, 48, 128, 128)\n",
        "        up2 = self.bn_up2(up2)\n",
        "        up2 = self.relu_up2(up2)\n",
        "\n",
        "        # Prepare for BidirectionalConvLSTM2D\n",
        "        up2_seq = torch.stack([x2, up2], dim=1)  # (B, 2, 48, 128, 128)\n",
        "        bidir_convLSTM2_out = self.bidirectional_convLSTM2(up2_seq)  # (B, 96*2, 128, 128)\n",
        "\n",
        "        # Swin Transformer Block in Decoder\n",
        "        bidir_convLSTM2_flat = bidir_convLSTM2_out.flatten(2).transpose(1, 2)  # (B, 128*128, 96*2)\n",
        "        swin_D2 = self.swin_unet_D2(bidir_convLSTM2_flat)               # (B, 128*128, 96*2)\n",
        "        swin_D2 = swin_D2.transpose(1, 2).view(-1, 96*2, 128, 128)    # Reshape for Conv2d\n",
        "\n",
        "        # Further Convolutions\n",
        "        conv7 = self.conv13(swin_D2)        # (B, 24, 128, 128)\n",
        "        conv7 = self.conv14(conv7)          # (B, 24, 128, 128)\n",
        "\n",
        "        # Third Upsampling Block\n",
        "        up3 = self.up3(conv7)               # (B, 24, 256, 256)\n",
        "        up3 = self.bn_up3(up3)\n",
        "        up3 = self.relu_up3(up3)\n",
        "\n",
        "        # Prepare for BidirectionalConvLSTM2D\n",
        "        up3_seq = torch.stack([x1, up3], dim=1)  # (B, 2, 24, 256, 256)\n",
        "        bidir_convLSTM3_out = self.bidirectional_convLSTM3(up3_seq)  # (B, 48*2, 256, 256)\n",
        "\n",
        "        # Swin Transformer Block in Decoder\n",
        "        bidir_convLSTM3_flat = bidir_convLSTM3_out.flatten(2).transpose(1, 2)  # (B, 256*256, 48*2)\n",
        "        swin_D3 = self.swin_unet_D3(bidir_convLSTM3_flat)               # (B, 256*256, 48*2)\n",
        "        swin_D3 = swin_D3.transpose(1, 2).view(-1, 48*2, 256, 256)    # Reshape for Conv2d\n",
        "\n",
        "        # Further Convolutions\n",
        "        conv8 = self.conv15(swin_D3)        # (B, 24, 256, 256)\n",
        "        conv8 = self.conv16(conv8)          # (B, 24, 256, 256)\n",
        "\n",
        "        # Final Output Convolutions\n",
        "        final = self.final_conv1(conv8)      # (B, 2, 256, 256)\n",
        "        final = self.final_relu(final)\n",
        "        final = self.final_conv2(final)      # (B, 1, 256, 256)\n",
        "        final = self.final_sigmoid(final)    # (B, 1, 256, 256)\n",
        "\n",
        "        return final\n",
        "\n",
        "# ================================== Dataset Class ======================================\n",
        "\n",
        "class SegmentationDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Custom Dataset for image segmentation tasks.\n",
        "    Expects images in 'x' folder and masks in 'y' folder.\n",
        "    \"\"\"\n",
        "    def __init__(self, images_dir, masks_dir, transform=None):\n",
        "        super(SegmentationDataset, self).__init__()\n",
        "        self.images_dir = images_dir\n",
        "        self.masks_dir = masks_dir\n",
        "        self.transform = transform\n",
        "\n",
        "        self.images = sorted(os.listdir(images_dir))\n",
        "        self.masks = sorted(os.listdir(masks_dir))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Load image\n",
        "        img_path = os.path.join(self.images_dir, self.images[idx])\n",
        "        image = Image.open(img_path).convert('RGB')  # Ensure RGB\n",
        "\n",
        "        # Load mask\n",
        "        mask_path = os.path.join(self.masks_dir, self.masks[idx])\n",
        "        mask = Image.open(mask_path).convert('L')    # Grayscale\n",
        "\n",
        "        # Apply transformations\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "            mask = self.transform(mask)\n",
        "\n",
        "        return image, mask\n",
        "\n",
        "# =============================== Data Loading and Preprocessing ========================\n",
        "\n",
        "# Define image dimensions\n",
        "im_height = 256\n",
        "im_width = 256\n",
        "\n",
        "# Define transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((im_height, im_width)),\n",
        "    transforms.ToTensor(),  # Converts to [0,1]\n",
        "])\n",
        "\n",
        "# Paths to the dataset (update these paths as per your directory structure)\n",
        "train_images_dir = r'/content/drive/MyDrive/ML/dataset/ISIC2018_Task1-2_Training_Input'\n",
        "train_masks_dir = r'/content/drive/MyDrive/ML/dataset/ISIC2018_Task1_Training_GroundTruth'\n",
        "test_images_dir = r'/content/drive/MyDrive/ML/dataset/ISIC2018_Task1-2_Test_Input'\n",
        "test_masks_dir = r'/content/drive/MyDrive/ML/dataset/ISIC2018_Task1_Test_GroundTruth'\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = SegmentationDataset(train_images_dir, train_masks_dir, transform=transform)\n",
        "test_dataset = SegmentationDataset(test_images_dir, test_masks_dir, transform=transform)\n",
        "\n",
        "# Split training data into training and validation sets (80-20 split)\n",
        "train_size = int(0.8 * len(train_dataset))\n",
        "valid_size = len(train_dataset) - train_size\n",
        "train_subset, valid_subset = torch.utils.data.random_split(train_dataset, [train_size, valid_size])\n",
        "\n",
        "# Create DataLoaders\n",
        "batch_size = 5\n",
        "\n",
        "train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
        "valid_loader = DataLoader(valid_subset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
        "\n",
        "# =============================== Training Setup ==========================================\n",
        "\n",
        "# Instantiate the model\n",
        "model = SwinUNet(input_channels=3, output_channels=1, embed_dim=32, num_heads=[4, 8], window_size=4, mlp_ratio=4., depth=2)\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model = model.to(device)  # Move to GPU if available\n",
        "\n",
        "# Initialize weights using Kaiming Normal initialization\n",
        "def initialize_weights(module):\n",
        "    if isinstance(module, (nn.Conv2d, nn.ConvTranspose2d)):\n",
        "        nn.init.kaiming_normal_(module.weight, mode='fan_in', nonlinearity='relu')\n",
        "        if module.bias is not None:\n",
        "            nn.init.zeros_(module.bias)\n",
        "    elif isinstance(module, nn.Linear):\n",
        "        nn.init.kaiming_normal_(module.weight, mode='fan_in', nonlinearity='relu')\n",
        "        if module.bias is not None:\n",
        "            nn.init.zeros_(module.bias)\n",
        "    elif isinstance(module, nn.Conv3d):\n",
        "        nn.init.kaiming_normal_(module.weight, mode='fan_in', nonlinearity='relu')\n",
        "        if module.bias is not None:\n",
        "            nn.init.zeros_(module.bias)\n",
        "\n",
        "model.apply(initialize_weights)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = DiceLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "# Define learning rate scheduler and early stopping parameters\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.25, patience=5, verbose=True, min_lr=1e-9)\n",
        "early_stopping_patience = 9\n",
        "best_val_loss = np.inf\n",
        "epochs_no_improve = 0\n",
        "\n",
        "# =============================== Training Loop ===========================================\n",
        "\n",
        "num_epochs = 1  # You can adjust the number of epochs\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    train_loader_count = 0\n",
        "    for images, masks in train_loader:\n",
        "        train_loader_count += 1\n",
        "        print(f\"Train loader image count: {train_loader_count}\")\n",
        "        images = images.to(device)  # (B, 3, 256, 256)\n",
        "        masks = masks.to(device)    # (B, 1, 256, 256)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)      # (B, 1, 256, 256)\n",
        "        loss = criterion(outputs, masks)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader.dataset)\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for images, masks in valid_loader:\n",
        "            images = images.to(device)\n",
        "            masks = masks.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, masks)\n",
        "\n",
        "            val_loss += loss.item() * images.size(0)\n",
        "\n",
        "    val_loss /= len(valid_loader.dataset)\n",
        "\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, Training Loss: {epoch_loss:.4f}, Validation Loss: {val_loss:.4f}')\n",
        "\n",
        "    # Scheduler step\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "    # Early stopping\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        epochs_no_improve = 0\n",
        "        # Save the best model\n",
        "        torch.save(model.state_dict(), r'/content/drive/MyDrive/model/modelWeights_Swin_Trans_Weights_Swin_Trans_Leather.pth')\n",
        "    else:\n",
        "        epochs_no_improve += 1\n",
        "        if epochs_no_improve >= early_stopping_patience:\n",
        "            print('Early stopping!')\n",
        "            break\n",
        "\n",
        "# ================================== Prediction ==========================================\n",
        "\n",
        "# Load the best model weights\n",
        "model.load_state_dict(torch.load(r'/content/drive/MyDrive/model/modelWeights_Swin_Trans_Weights_Swin_Trans_Leather.pth'))\n",
        "model.eval()\n",
        "\n",
        "# Function to save predictions and ground truth\n",
        "def save_predictions(model, dataloader, save_dir_pred, save_dir_gt, device):\n",
        "    \"\"\"\n",
        "    Saves the predicted masks and ground truth masks.\n",
        "    Args:\n",
        "        model (nn.Module): Trained model.\n",
        "        dataloader (DataLoader): DataLoader for test data.\n",
        "        save_dir_pred (str): Directory to save predicted masks.\n",
        "        save_dir_gt (str): Directory to save ground truth masks.\n",
        "        device (str): Device to run the model on.\n",
        "    \"\"\"\n",
        "    os.makedirs(save_dir_pred, exist_ok=True)\n",
        "    os.makedirs(save_dir_gt, exist_ok=True)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (images, masks) in enumerate(dataloader):\n",
        "            if (i % 100 == 0):\n",
        "                print(f\"{i}th Test Image\")  # Adjust as per your dataset\n",
        "\n",
        "            images = images.to(device)\n",
        "            masks = masks.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            preds = outputs.cpu().numpy()\n",
        "            gts = masks.cpu().numpy()\n",
        "\n",
        "            for j in range(preds.shape[0]):\n",
        "                pred_mask = preds[j, 0, :, :]\n",
        "                gt_mask = gts[j, 0, :, :]\n",
        "\n",
        "                # Save predicted mask\n",
        "                pred_img = Image.fromarray((pred_mask * 255).astype(np.uint8))\n",
        "                pred_img.save(os.path.join(save_dir_pred, f\"{i * dataloader.batch_size + j + 1}.png\"))\n",
        "\n",
        "                # Save ground truth mask\n",
        "                gt_img = Image.fromarray((gt_mask * 255).astype(np.uint8))\n",
        "                gt_img.save(os.path.join(save_dir_gt, f\"{i * dataloader.batch_size + j + 1}.tiff\"))\n",
        "\n",
        "# Define directories to save predictions and ground truth\n",
        "save_dir_pred = r'/content/drive/MyDrive/output/segmented predicted images'\n",
        "save_dir_gt = r'/content/drive/MyDrive/output/segmented ground truth'\n",
        "\n",
        "# Save predictions\n",
        "save_predictions(model, test_loader, save_dir_pred, save_dir_gt, device)\n",
        "\n",
        "# =================================== Evaluation =========================================\n",
        "\n",
        "def evaluate_metrics_pytorch(model, dataloader, device):\n",
        "    \"\"\"\n",
        "    Evaluates various metrics for segmentation performance.\n",
        "    Args:\n",
        "        model (nn.Module): Trained model.\n",
        "        dataloader (DataLoader): DataLoader for test data.\n",
        "        device (str): Device to run the model on.\n",
        "    Returns:\n",
        "        dict: Dictionary containing average metrics.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    all_metrics = {\n",
        "        'Accuracy': [],\n",
        "        'Dice': [],\n",
        "        'Jaccard': [],\n",
        "        'Sensitivity': [],\n",
        "        'Specificity': [],\n",
        "        'Precision': [],\n",
        "        'Recall': [],\n",
        "        'F1-Score': []\n",
        "    }\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, masks in dataloader:\n",
        "            images = images.to(device)\n",
        "            masks = masks.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            preds = outputs > 0.5  # Binary mask\n",
        "\n",
        "            preds = preds.cpu().numpy().astype(np.uint8)\n",
        "            masks = masks.cpu().numpy().astype(np.uint8)\n",
        "            masks = (masks > 0).astype(np.uint8)  # Convert to binary masks\n",
        "\n",
        "            for pred, mask in zip(preds, masks):\n",
        "                pred_flat = pred.flatten()\n",
        "                mask_flat = mask.flatten()\n",
        "\n",
        "                # Calculate metrics\n",
        "                tn, fp, fn, tp = confusion_matrix(mask_flat, pred_flat, labels=[0,1]).ravel()\n",
        "\n",
        "                accuracy = (tp + tn) / (tp + tn + fp + fn + 1e-8)\n",
        "                iou = jaccard_score(mask_flat, pred_flat, zero_division=0)\n",
        "                dice = f1_score(mask_flat, pred_flat, zero_division=0)\n",
        "                specificity = tn / (tn + fp + 1e-8)\n",
        "                sensitivity = recall_score(mask_flat, pred_flat, zero_division=0)\n",
        "                precision = precision_score(mask_flat, pred_flat, zero_division=0)\n",
        "                recall = sensitivity\n",
        "                f1 = dice  # F1-Score is the same as Dice coefficient for binary classification\n",
        "\n",
        "                all_metrics['Accuracy'].append(accuracy)\n",
        "                all_metrics['Jaccard'].append(iou)\n",
        "                all_metrics['Dice'].append(dice)\n",
        "                all_metrics['Specificity'].append(specificity)\n",
        "                all_metrics['Sensitivity'].append(sensitivity)\n",
        "                all_metrics['Precision'].append(precision)\n",
        "                all_metrics['Recall'].append(recall)\n",
        "                all_metrics['F1-Score'].append(f1)\n",
        "\n",
        "    # Compute average metrics\n",
        "    avg_metrics = {metric: np.mean(values) for metric, values in all_metrics.items()}\n",
        "\n",
        "    print(\"Evaluation Metrics:\")\n",
        "    for metric, value in avg_metrics.items():\n",
        "        print(f\"{metric}: {value:.4f}\")\n",
        "\n",
        "    return avg_metrics\n",
        "\n",
        "# Evaluate the model\n",
        "metrics = evaluate_metrics_pytorch(model, test_loader, device)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchinfo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2bsD10BqZav3",
        "outputId": "5059a547-31e8-4dae-9df8-8b0e30423d2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
            "Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchinfo import summary\n",
        "\n",
        "# Define the input size based on your model's expected input.\n",
        "# For example, if your model expects images with 3 channels and 256x256 dimensions:\n",
        "input_size = (1, 3, 256, 256)  # (batch_size, channels, height, width)\n",
        "\n",
        "# Generate and print the model summary\n",
        "summary(model, input_size=input_size, device='cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bEj5XH54YDDi",
        "outputId": "23a7863c-0c13-41df-8ca4-cf29c41b2383"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "SwinUNet                                 [1, 1, 256, 256]          --\n",
              "├─SeparableConv2d: 1-1                   [1, 24, 256, 256]         --\n",
              "│    └─Conv2d: 2-1                       [1, 3, 256, 256]          30\n",
              "│    └─Conv2d: 2-2                       [1, 24, 256, 256]         96\n",
              "├─BatchNorm2d: 1-2                       [1, 24, 256, 256]         48\n",
              "├─SeparableConv2d: 1-3                   [1, 24, 256, 256]         --\n",
              "│    └─Conv2d: 2-3                       [1, 24, 256, 256]         240\n",
              "│    └─Conv2d: 2-4                       [1, 24, 256, 256]         600\n",
              "├─BatchNorm2d: 1-4                       [1, 24, 256, 256]         48\n",
              "├─MaxPool2d: 1-5                         [1, 24, 128, 128]         --\n",
              "├─SwinTransformerBlock: 1-6              [1, 16384, 24]            --\n",
              "│    └─LayerNorm: 2-5                    [1, 16384, 24]            48\n",
              "│    └─WindowAttention: 2-6              [1024, 16, 24]            196\n",
              "│    │    └─Linear: 3-1                  [1024, 16, 72]            1,800\n",
              "│    │    └─Dropout: 3-2                 [1024, 4, 16, 16]         --\n",
              "│    │    └─Linear: 3-3                  [1024, 16, 24]            600\n",
              "│    │    └─Dropout: 3-4                 [1024, 16, 24]            --\n",
              "│    └─Identity: 2-7                     [1, 16384, 24]            --\n",
              "│    └─LayerNorm: 2-8                    [1, 16384, 24]            48\n",
              "│    └─Sequential: 2-9                   [1, 16384, 24]            --\n",
              "│    │    └─Linear: 3-5                  [1, 16384, 96]            2,400\n",
              "│    │    └─GELU: 3-6                    [1, 16384, 96]            --\n",
              "│    │    └─Linear: 3-7                  [1, 16384, 24]            2,328\n",
              "│    │    └─Dropout: 3-8                 [1, 16384, 24]            --\n",
              "│    └─Identity: 2-10                    [1, 16384, 24]            --\n",
              "├─SeparableConv2d: 1-7                   [1, 48, 128, 128]         --\n",
              "│    └─Conv2d: 2-11                      [1, 24, 128, 128]         240\n",
              "│    └─Conv2d: 2-12                      [1, 48, 128, 128]         1,200\n",
              "├─BatchNorm2d: 1-8                       [1, 48, 128, 128]         96\n",
              "├─SeparableConv2d: 1-9                   [1, 48, 128, 128]         --\n",
              "│    └─Conv2d: 2-13                      [1, 48, 128, 128]         480\n",
              "│    └─Conv2d: 2-14                      [1, 48, 128, 128]         2,352\n",
              "├─BatchNorm2d: 1-10                      [1, 48, 128, 128]         96\n",
              "├─MaxPool2d: 1-11                        [1, 48, 64, 64]           --\n",
              "├─SwinTransformerBlock: 1-12             [1, 4096, 48]             --\n",
              "│    └─LayerNorm: 2-15                   [1, 4096, 48]             96\n",
              "│    └─WindowAttention: 2-16             [256, 16, 48]             392\n",
              "│    │    └─Linear: 3-9                  [256, 16, 144]            7,056\n",
              "│    │    └─Dropout: 3-10                [256, 8, 16, 16]          --\n",
              "│    │    └─Linear: 3-11                 [256, 16, 48]             2,352\n",
              "│    │    └─Dropout: 3-12                [256, 16, 48]             --\n",
              "│    └─Identity: 2-17                    [1, 4096, 48]             --\n",
              "│    └─LayerNorm: 2-18                   [1, 4096, 48]             96\n",
              "│    └─Sequential: 2-19                  [1, 4096, 48]             --\n",
              "│    │    └─Linear: 3-13                 [1, 4096, 192]            9,408\n",
              "│    │    └─GELU: 3-14                   [1, 4096, 192]            --\n",
              "│    │    └─Linear: 3-15                 [1, 4096, 48]             9,264\n",
              "│    │    └─Dropout: 3-16                [1, 4096, 48]             --\n",
              "│    └─Identity: 2-20                    [1, 4096, 48]             --\n",
              "├─SeparableConv2d: 1-13                  [1, 96, 64, 64]           --\n",
              "│    └─Conv2d: 2-21                      [1, 48, 64, 64]           480\n",
              "│    └─Conv2d: 2-22                      [1, 96, 64, 64]           4,704\n",
              "├─BatchNorm2d: 1-14                      [1, 96, 64, 64]           192\n",
              "├─SeparableConv2d: 1-15                  [1, 96, 64, 64]           --\n",
              "│    └─Conv2d: 2-23                      [1, 96, 64, 64]           960\n",
              "│    └─Conv2d: 2-24                      [1, 96, 64, 64]           9,312\n",
              "├─BatchNorm2d: 1-16                      [1, 96, 64, 64]           192\n",
              "├─Dropout: 1-17                          [1, 96, 64, 64]           --\n",
              "├─MaxPool2d: 1-18                        [1, 96, 32, 32]           --\n",
              "├─SeparableConv2d: 1-19                  [1, 192, 32, 32]          --\n",
              "│    └─Conv2d: 2-25                      [1, 96, 32, 32]           960\n",
              "│    └─Conv2d: 2-26                      [1, 192, 32, 32]          18,624\n",
              "├─BatchNorm2d: 1-20                      [1, 192, 32, 32]          384\n",
              "├─SeparableConv2d: 1-21                  [1, 192, 32, 32]          --\n",
              "│    └─Conv2d: 2-27                      [1, 192, 32, 32]          1,920\n",
              "│    └─Conv2d: 2-28                      [1, 192, 32, 32]          37,056\n",
              "├─BatchNorm2d: 1-22                      [1, 192, 32, 32]          384\n",
              "├─Dropout: 1-23                          [1, 192, 32, 32]          --\n",
              "├─SeparableConv2d: 1-24                  [1, 192, 32, 32]          --\n",
              "│    └─Conv2d: 2-29                      [1, 192, 32, 32]          1,920\n",
              "│    └─Conv2d: 2-30                      [1, 192, 32, 32]          37,056\n",
              "├─BatchNorm2d: 1-25                      [1, 192, 32, 32]          384\n",
              "├─SeparableConv2d: 1-26                  [1, 192, 32, 32]          --\n",
              "│    └─Conv2d: 2-31                      [1, 192, 32, 32]          1,920\n",
              "│    └─Conv2d: 2-32                      [1, 192, 32, 32]          37,056\n",
              "├─BatchNorm2d: 1-27                      [1, 192, 32, 32]          384\n",
              "├─Dropout: 1-28                          [1, 192, 32, 32]          --\n",
              "├─Sequential: 1-29                       [1, 192, 32, 32]          --\n",
              "│    └─SeparableConv2d: 2-33             [1, 192, 32, 32]          --\n",
              "│    │    └─Conv2d: 3-17                 [1, 384, 32, 32]          3,840\n",
              "│    │    └─Conv2d: 3-18                 [1, 192, 32, 32]          73,920\n",
              "│    └─SeparableConv2d: 2-34             [1, 192, 32, 32]          --\n",
              "│    │    └─Conv2d: 3-19                 [1, 192, 32, 32]          1,920\n",
              "│    │    └─Conv2d: 3-20                 [1, 192, 32, 32]          37,056\n",
              "├─Dropout: 1-30                          [1, 192, 32, 32]          --\n",
              "├─ConvTranspose2d: 1-31                  [1, 96, 64, 64]           73,824\n",
              "├─BatchNorm2d: 1-32                      [1, 96, 64, 64]           192\n",
              "├─ReLU: 1-33                             [1, 96, 64, 64]           --\n",
              "├─BidirectionalConvLSTM2D: 1-34          [1, 384, 64, 64]          --\n",
              "│    └─ConvLSTM2D: 2-35                  [1, 2, 192, 64, 64]       --\n",
              "│    │    └─ModuleList: 3-21             --                        1,991,424\n",
              "│    └─ConvLSTM2D: 2-36                  [1, 2, 192, 64, 64]       --\n",
              "│    │    └─ModuleList: 3-22             --                        1,991,424\n",
              "├─SwinTransformerBlock: 1-35             [1, 4096, 384]            --\n",
              "│    └─LayerNorm: 2-37                   [1, 4096, 384]            768\n",
              "│    └─WindowAttention: 2-38             [256, 16, 384]            196\n",
              "│    │    └─Linear: 3-23                 [256, 16, 1152]           443,520\n",
              "│    │    └─Dropout: 3-24                [256, 4, 16, 16]          --\n",
              "│    │    └─Linear: 3-25                 [256, 16, 384]            147,840\n",
              "│    │    └─Dropout: 3-26                [256, 16, 384]            --\n",
              "│    └─Identity: 2-39                    [1, 4096, 384]            --\n",
              "│    └─LayerNorm: 2-40                   [1, 4096, 384]            768\n",
              "│    └─Sequential: 2-41                  [1, 4096, 384]            --\n",
              "│    │    └─Linear: 3-27                 [1, 4096, 1536]           591,360\n",
              "│    │    └─GELU: 3-28                   [1, 4096, 1536]           --\n",
              "│    │    └─Linear: 3-29                 [1, 4096, 384]            590,208\n",
              "│    │    └─Dropout: 3-30                [1, 4096, 384]            --\n",
              "│    └─Identity: 2-42                    [1, 4096, 384]            --\n",
              "├─SeparableConv2d: 1-36                  [1, 48, 64, 64]           --\n",
              "│    └─Conv2d: 2-43                      [1, 384, 64, 64]          3,840\n",
              "│    └─Conv2d: 2-44                      [1, 48, 64, 64]           18,480\n",
              "├─SeparableConv2d: 1-37                  [1, 48, 64, 64]           --\n",
              "│    └─Conv2d: 2-45                      [1, 48, 64, 64]           480\n",
              "│    └─Conv2d: 2-46                      [1, 48, 64, 64]           2,352\n",
              "├─ConvTranspose2d: 1-38                  [1, 48, 128, 128]         9,264\n",
              "├─BatchNorm2d: 1-39                      [1, 48, 128, 128]         96\n",
              "├─ReLU: 1-40                             [1, 48, 128, 128]         --\n",
              "├─BidirectionalConvLSTM2D: 1-41          [1, 192, 128, 128]        --\n",
              "│    └─ConvLSTM2D: 2-47                  [1, 2, 96, 128, 128]      --\n",
              "│    │    └─ModuleList: 3-31             --                        498,048\n",
              "│    └─ConvLSTM2D: 2-48                  [1, 2, 96, 128, 128]      --\n",
              "│    │    └─ModuleList: 3-32             --                        498,048\n",
              "├─SwinTransformerBlock: 1-42             [1, 16384, 192]           --\n",
              "│    └─LayerNorm: 2-49                   [1, 16384, 192]           384\n",
              "│    └─WindowAttention: 2-50             [1024, 16, 192]           392\n",
              "│    │    └─Linear: 3-33                 [1024, 16, 576]           111,168\n",
              "│    │    └─Dropout: 3-34                [1024, 8, 16, 16]         --\n",
              "│    │    └─Linear: 3-35                 [1024, 16, 192]           37,056\n",
              "│    │    └─Dropout: 3-36                [1024, 16, 192]           --\n",
              "│    └─Identity: 2-51                    [1, 16384, 192]           --\n",
              "│    └─LayerNorm: 2-52                   [1, 16384, 192]           384\n",
              "│    └─Sequential: 2-53                  [1, 16384, 192]           --\n",
              "│    │    └─Linear: 3-37                 [1, 16384, 768]           148,224\n",
              "│    │    └─GELU: 3-38                   [1, 16384, 768]           --\n",
              "│    │    └─Linear: 3-39                 [1, 16384, 192]           147,648\n",
              "│    │    └─Dropout: 3-40                [1, 16384, 192]           --\n",
              "│    └─Identity: 2-54                    [1, 16384, 192]           --\n",
              "├─SeparableConv2d: 1-43                  [1, 24, 128, 128]         --\n",
              "│    └─Conv2d: 2-55                      [1, 192, 128, 128]        1,920\n",
              "│    └─Conv2d: 2-56                      [1, 24, 128, 128]         4,632\n",
              "├─SeparableConv2d: 1-44                  [1, 24, 128, 128]         --\n",
              "│    └─Conv2d: 2-57                      [1, 24, 128, 128]         240\n",
              "│    └─Conv2d: 2-58                      [1, 24, 128, 128]         600\n",
              "├─ConvTranspose2d: 1-45                  [1, 24, 256, 256]         2,328\n",
              "├─BatchNorm2d: 1-46                      [1, 24, 256, 256]         48\n",
              "├─ReLU: 1-47                             [1, 24, 256, 256]         --\n",
              "├─BidirectionalConvLSTM2D: 1-48          [1, 96, 256, 256]         --\n",
              "│    └─ConvLSTM2D: 2-59                  [1, 2, 48, 256, 256]      --\n",
              "│    │    └─ModuleList: 3-41             --                        124,608\n",
              "│    └─ConvLSTM2D: 2-60                  [1, 2, 48, 256, 256]      --\n",
              "│    │    └─ModuleList: 3-42             --                        124,608\n",
              "├─SwinTransformerBlock: 1-49             [1, 65536, 96]            --\n",
              "│    └─LayerNorm: 2-61                   [1, 65536, 96]            192\n",
              "│    └─WindowAttention: 2-62             [4096, 16, 96]            392\n",
              "│    │    └─Linear: 3-43                 [4096, 16, 288]           27,936\n",
              "│    │    └─Dropout: 3-44                [4096, 8, 16, 16]         --\n",
              "│    │    └─Linear: 3-45                 [4096, 16, 96]            9,312\n",
              "│    │    └─Dropout: 3-46                [4096, 16, 96]            --\n",
              "│    └─Identity: 2-63                    [1, 65536, 96]            --\n",
              "│    └─LayerNorm: 2-64                   [1, 65536, 96]            192\n",
              "│    └─Sequential: 2-65                  [1, 65536, 96]            --\n",
              "│    │    └─Linear: 3-47                 [1, 65536, 384]           37,248\n",
              "│    │    └─GELU: 3-48                   [1, 65536, 384]           --\n",
              "│    │    └─Linear: 3-49                 [1, 65536, 96]            36,960\n",
              "│    │    └─Dropout: 3-50                [1, 65536, 96]            --\n",
              "│    └─Identity: 2-66                    [1, 65536, 96]            --\n",
              "├─SeparableConv2d: 1-50                  [1, 24, 256, 256]         --\n",
              "│    └─Conv2d: 2-67                      [1, 96, 256, 256]         960\n",
              "│    └─Conv2d: 2-68                      [1, 24, 256, 256]         2,328\n",
              "├─SeparableConv2d: 1-51                  [1, 24, 256, 256]         --\n",
              "│    └─Conv2d: 2-69                      [1, 24, 256, 256]         240\n",
              "│    └─Conv2d: 2-70                      [1, 24, 256, 256]         600\n",
              "├─Conv2d: 1-52                           [1, 2, 256, 256]          434\n",
              "├─ReLU: 1-53                             [1, 2, 256, 256]          --\n",
              "├─Conv2d: 1-54                           [1, 1, 256, 256]          3\n",
              "├─Sigmoid: 1-55                          [1, 1, 256, 256]          --\n",
              "==========================================================================================\n",
              "Total params: 7,995,403\n",
              "Trainable params: 7,995,403\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (G): 99.98\n",
              "==========================================================================================\n",
              "Input size (MB): 0.79\n",
              "Forward/backward pass size (MB): 2053.37\n",
              "Params size (MB): 31.98\n",
              "Estimated Total Size (MB): 2086.14\n",
              "=========================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vJlUbzgnLeKk"
      },
      "outputs": [],
      "source": [
        "# ============================== Imports and Dependencies ==============================\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import (\n",
        "    precision_recall_curve, confusion_matrix,\n",
        "    jaccard_score, f1_score, precision_score, recall_score\n",
        ")\n",
        "\n",
        "# ================================ Separable Convolution =================================\n",
        "\n",
        "class SeparableConv2d(nn.Module):\n",
        "    \"\"\"\n",
        "    Implements a separable convolution layer using depthwise and pointwise convolutions.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, padding=1, bias=True):\n",
        "        super(SeparableConv2d, self).__init__()\n",
        "        # Depthwise convolution (groups=in_channels)\n",
        "        self.depthwise = nn.Conv2d(in_channels, in_channels, kernel_size=kernel_size,\n",
        "                                   padding=padding, groups=in_channels, bias=bias)\n",
        "        # Pointwise convolution\n",
        "        self.pointwise = nn.Conv2d(in_channels, out_channels, kernel_size=1,\n",
        "                                   padding=0, bias=bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.depthwise(x)\n",
        "        x = self.pointwise(x)\n",
        "        return x\n",
        "\n",
        "# ================================== ConvLSTM2D ========================================\n",
        "\n",
        "class ConvLSTMCell(nn.Module):\n",
        "    \"\"\"\n",
        "    Implements a ConvLSTM cell.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_channels, hidden_channels, kernel_size, bias=True):\n",
        "        super(ConvLSTMCell, self).__init__()\n",
        "\n",
        "        padding = kernel_size // 2  # To maintain spatial dimensions\n",
        "        self.input_channels = input_channels\n",
        "        self.hidden_channels = hidden_channels\n",
        "\n",
        "        self.conv = nn.Conv2d(in_channels=input_channels + hidden_channels,\n",
        "                              out_channels=4 * hidden_channels,\n",
        "                              kernel_size=kernel_size,\n",
        "                              padding=padding,\n",
        "                              bias=bias)\n",
        "\n",
        "    def forward(self, input_tensor, cur_state):\n",
        "        h_cur, c_cur = cur_state\n",
        "\n",
        "        # Concatenate input and hidden state\n",
        "        combined = torch.cat([input_tensor, h_cur], dim=1)  # along channel axis\n",
        "\n",
        "        # Compute all gates at once\n",
        "        conv_output = self.conv(combined)\n",
        "        cc_i, cc_f, cc_o, cc_g = torch.split(conv_output, self.hidden_channels, dim=1)\n",
        "\n",
        "        i = torch.sigmoid(cc_i)   # input gate\n",
        "        f = torch.sigmoid(cc_f)   # forget gate\n",
        "        o = torch.sigmoid(cc_o)   # output gate\n",
        "        g = torch.tanh(cc_g)      # gate gate\n",
        "\n",
        "        c_next = f * c_cur + i * g\n",
        "        h_next = o * torch.tanh(c_next)\n",
        "\n",
        "        return h_next, c_next\n",
        "\n",
        "    def init_hidden(self, batch_size, spatial_size, device):\n",
        "        height, width = spatial_size\n",
        "        return (torch.zeros(batch_size, self.hidden_channels, height, width, device=device),\n",
        "                torch.zeros(batch_size, self.hidden_channels, height, width, device=device))\n",
        "\n",
        "class ConvLSTM2D(nn.Module):\n",
        "    \"\"\"\n",
        "    Implements a ConvLSTM2D layer that processes a sequence of inputs.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_channels, hidden_channels, kernel_size=3, bias=True, num_layers=1):\n",
        "        super(ConvLSTM2D, self).__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.hidden_channels = hidden_channels\n",
        "\n",
        "        layers = []\n",
        "        for i in range(num_layers):\n",
        "            input_c = input_channels if i == 0 else hidden_channels\n",
        "            layers.append(ConvLSTMCell(input_c, hidden_channels, kernel_size, bias))\n",
        "        self.layers = nn.ModuleList(layers)\n",
        "\n",
        "    def forward(self, input_tensor, reverse=False):\n",
        "        # input_tensor shape: (batch, seq_len, channels, height, width)\n",
        "        batch_size, seq_len, channels, height, width = input_tensor.size()\n",
        "        device = input_tensor.device\n",
        "\n",
        "        # Initialize hidden and cell states for all layers\n",
        "        hidden_state = []\n",
        "        cell_state = []\n",
        "        for i in range(self.num_layers):\n",
        "            h, c = self.layers[i].init_hidden(batch_size, (height, width), device)\n",
        "            hidden_state.append(h)\n",
        "            cell_state.append(c)\n",
        "\n",
        "        # Iterate over time steps\n",
        "        if reverse:\n",
        "            time_steps = reversed(range(seq_len))\n",
        "        else:\n",
        "            time_steps = range(seq_len)\n",
        "\n",
        "        outputs = []\n",
        "        for t in time_steps:\n",
        "            x = input_tensor[:, t, :, :, :]  # (batch, channels, height, width)\n",
        "            for i, layer in enumerate(self.layers):\n",
        "                h, c = layer(x, (hidden_state[i], cell_state[i]))\n",
        "                hidden_state[i] = h\n",
        "                cell_state[i] = c\n",
        "                x = h  # input to next layer\n",
        "            outputs.append(x)\n",
        "\n",
        "        outputs = torch.stack(outputs, dim=1)  # (batch, seq_len, channels, height, width)\n",
        "        if reverse:\n",
        "            outputs = outputs.flip(dims=[1])  # Reverse back to original order\n",
        "        return outputs  # Return the sequence of outputs\n",
        "\n",
        "class BidirectionalConvLSTM2D(nn.Module):\n",
        "    \"\"\"\n",
        "    Implements a Bidirectional ConvLSTM2D layer.\n",
        "    Processes the input sequence in both forward and backward directions and concatenates the outputs.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_channels, hidden_channels, kernel_size=3, num_layers=1, bias=True):\n",
        "        super(BidirectionalConvLSTM2D, self).__init__()\n",
        "        self.forward_conv_lstm = ConvLSTM2D(input_channels, hidden_channels, kernel_size, bias=bias, num_layers=num_layers)\n",
        "        self.backward_conv_lstm = ConvLSTM2D(input_channels, hidden_channels, kernel_size, bias=bias, num_layers=num_layers)\n",
        "\n",
        "    def forward(self, input_tensor):\n",
        "        # input_tensor shape: (batch, seq_len, channels, height, width)\n",
        "        # Forward direction\n",
        "        forward_output = self.forward_conv_lstm(input_tensor, reverse=False)  # (batch, seq_len, hidden_channels, H, W)\n",
        "        # Backward direction\n",
        "        backward_output = self.backward_conv_lstm(input_tensor, reverse=True)  # (batch, seq_len, hidden_channels, H, W)\n",
        "        # Concatenate outputs along the channel dimension\n",
        "        output = torch.cat([forward_output, backward_output], dim=2)  # (batch, seq_len, hidden_channels*2, H, W)\n",
        "        # Since seq_len=2, we can take the last output\n",
        "        output = output[:, -1, :, :, :]  # Take the last output (batch, hidden_channels*2, H, W)\n",
        "        return output\n",
        "\n",
        "# ============================== Swin Transformer Blocks ================================\n",
        "\n",
        "class WindowAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Window based multi-head self attention (W-MSA) module with relative position bias.\n",
        "    \"\"\"\n",
        "    def __init__(self, dim, window_size, num_heads, qkv_bias=True, attn_drop=0., proj_drop=0.):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            dim (int): Number of input channels.\n",
        "            window_size (tuple): Height and width of the window.\n",
        "            num_heads (int): Number of attention heads.\n",
        "            qkv_bias (bool): If True, add a learnable bias to query, key, value.\n",
        "            attn_drop (float): Dropout ratio of attention weights.\n",
        "            proj_drop (float): Dropout ratio of output.\n",
        "        \"\"\"\n",
        "        super(WindowAttention, self).__init__()\n",
        "        self.dim = dim\n",
        "        self.window_size = window_size  # Wh, Ww\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // num_heads\n",
        "        self.scale = head_dim ** -0.5\n",
        "\n",
        "        # Define a parameter table of relative position bias\n",
        "        self.relative_position_bias_table = nn.Parameter(\n",
        "            torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads)\n",
        "        )  # 2*Wh-1 * 2*Ww-1, nH\n",
        "\n",
        "        # Get pair-wise relative position index for each token inside the window\n",
        "        coords_h = torch.arange(self.window_size[0])\n",
        "        coords_w = torch.arange(self.window_size[1])\n",
        "        coords = torch.stack(torch.meshgrid(coords_h, coords_w, indexing='ij'))  # 2, Wh, Ww\n",
        "        coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n",
        "        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n",
        "        relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n",
        "        relative_coords[:, :, 0] += self.window_size[0] - 1  # shift to start from 0\n",
        "        relative_coords[:, :, 1] += self.window_size[1] - 1\n",
        "        relative_coords[:, :, 0] *= (2 * self.window_size[1] - 1)\n",
        "        relative_position_index = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n",
        "        self.register_buffer(\"relative_position_index\", relative_position_index)\n",
        "\n",
        "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)  # Query, Key, Value\n",
        "        self.attn_drop = nn.Dropout(attn_drop)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.proj_drop = nn.Dropout(proj_drop)\n",
        "\n",
        "        # Initialize relative position bias table\n",
        "        nn.init.trunc_normal_(self.relative_position_bias_table, std=.02)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: input features with shape of (num_windows*B, Wh*Ww, C)\n",
        "            mask: (num_windows, Wh*Ww, Wh*Ww) or None\n",
        "        \"\"\"\n",
        "        B_, N, C = x.shape\n",
        "        qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)  # 3, B_, nH, N, C//nH\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]  # each has shape (B_, nH, N, C//nH)\n",
        "\n",
        "        q = q * self.scale\n",
        "        attn = (q @ k.transpose(-2, -1))  # (B_, nH, N, N)\n",
        "\n",
        "        # Add relative position bias\n",
        "        relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(\n",
        "            self.window_size[0] * self.window_size[1], self.window_size[0] * self.window_size[1], -1\n",
        "        )  # Wh*Ww, Wh*Ww, nH\n",
        "        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n",
        "        attn = attn + relative_position_bias.unsqueeze(0)  # (B_, nH, N, N)\n",
        "\n",
        "        if mask is not None:\n",
        "            nW = mask.shape[0]\n",
        "            attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)\n",
        "            attn = attn.view(-1, self.num_heads, N, N)\n",
        "            attn = F.softmax(attn, dim=-1)\n",
        "        else:\n",
        "            attn = F.softmax(attn, dim=-1)\n",
        "\n",
        "        attn = self.attn_drop(attn)\n",
        "\n",
        "        out = (attn @ v).transpose(1, 2).reshape(B_, N, C)  # (B_, N, C)\n",
        "        out = self.proj(out)\n",
        "        out = self.proj_drop(out)\n",
        "        return out\n",
        "\n",
        "class SwinTransformerBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Swin Transformer Block with W-MSA and SW-MSA.\n",
        "    \"\"\"\n",
        "    def __init__(self, dim, num_heads, window_size=7, shift_size=0, mlp_ratio=4., qkv_bias=True,\n",
        "                 attn_drop=0., proj_drop=0.):\n",
        "        super(SwinTransformerBlock, self).__init__()\n",
        "        self.dim = dim\n",
        "        self.num_heads = num_heads\n",
        "        self.window_size = window_size  # W\n",
        "        self.shift_size = shift_size    # S\n",
        "        self.mlp_ratio = mlp_ratio\n",
        "\n",
        "        assert 0 <= self.shift_size < self.window_size, \"shift_size must be in [0, window_size)\"\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(dim)\n",
        "        self.attn = WindowAttention(dim, (window_size, window_size), num_heads, qkv_bias, attn_drop, proj_drop)\n",
        "\n",
        "        self.drop_path = nn.Identity()  # Can implement stochastic depth if desired\n",
        "        self.norm2 = nn.LayerNorm(dim)\n",
        "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(dim, mlp_hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(mlp_hidden_dim, dim),\n",
        "            nn.Dropout(proj_drop)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: input features with shape (B, H*W, C)\n",
        "        \"\"\"\n",
        "        H = W = int(np.sqrt(x.shape[1]))\n",
        "        B, L, C = x.shape\n",
        "        assert L == H * W, \"Input feature has wrong size\"\n",
        "\n",
        "        shortcut = x\n",
        "        x = self.norm1(x)\n",
        "        x = x.view(B, H, W, C)\n",
        "\n",
        "        # Cyclic shift\n",
        "        if self.shift_size > 0:\n",
        "            shifted_x = torch.roll(x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2))\n",
        "        else:\n",
        "            shifted_x = x\n",
        "\n",
        "        # Partition windows\n",
        "        window_size = self.window_size\n",
        "        # Pad H and W to be multiples of window_size\n",
        "        pad_b = (window_size - H % window_size) % window_size\n",
        "        pad_r = (window_size - W % window_size) % window_size\n",
        "        shifted_x = F.pad(shifted_x, (0, 0, 0, pad_r, 0, pad_b))  # pad H and W\n",
        "        _, Hp, Wp, _ = shifted_x.shape\n",
        "\n",
        "        # Window partition\n",
        "        x_windows = shifted_x.view(B, Hp // window_size, window_size, Wp // window_size, window_size, C)\n",
        "        x_windows = x_windows.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size * window_size, C)  # (num_windows*B, window_size*window_size, C)\n",
        "\n",
        "        # Attention\n",
        "        attn_windows = self.attn(x_windows)  # (num_windows*B, window_size*window_size, C)\n",
        "\n",
        "        # Merge windows\n",
        "        shifted_x = attn_windows.view(-1, window_size, window_size, C)\n",
        "        shifted_x = shifted_x.view(B, Hp // window_size, Wp // window_size, window_size, window_size, C)\n",
        "        shifted_x = shifted_x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, Hp, Wp, C)\n",
        "\n",
        "        # Reverse cyclic shift\n",
        "        if self.shift_size > 0:\n",
        "            x = torch.roll(shifted_x, shifts=(self.shift_size, self.shift_size), dims=(1, 2))\n",
        "        else:\n",
        "            x = shifted_x\n",
        "\n",
        "        # Remove padding\n",
        "        x = x[:, :H, :W, :].contiguous().view(B, H * W, C)\n",
        "\n",
        "        # FFN\n",
        "        x = shortcut + self.drop_path(x)\n",
        "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
        "\n",
        "        return x\n",
        "\n",
        "# =============================== Dice Loss Function ====================================\n",
        "\n",
        "class DiceLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Dice Loss function to maximize the Dice coefficient.\n",
        "    Suitable for binary segmentation tasks.\n",
        "    \"\"\"\n",
        "    def __init__(self, smooth=1.0):\n",
        "        super(DiceLoss, self).__init__()\n",
        "        self.smooth = smooth\n",
        "\n",
        "    def forward(self, y_pred, y_true):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            y_pred (torch.Tensor): Predicted mask probabilities with shape (B, 1, H, W)\n",
        "            y_true (torch.Tensor): Ground truth masks with shape (B, 1, H, W)\n",
        "        Returns:\n",
        "            torch.Tensor: Dice loss\n",
        "        \"\"\"\n",
        "        y_pred = y_pred.view(-1)\n",
        "        y_true = y_true.view(-1)\n",
        "\n",
        "        intersection = (y_pred * y_true).sum()\n",
        "        dice = (2. * intersection + self.smooth) / (y_pred.sum() + y_true.sum() + self.smooth)\n",
        "\n",
        "        return 1 - dice\n",
        "\n",
        "# ================================ Main Model ============================================\n",
        "\n",
        "class SwinUNet(nn.Module):\n",
        "    \"\"\"\n",
        "    Swin U-Net architecture for image segmentation with bidirectional ConvLSTM layers.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_channels=3, output_channels=1,\n",
        "                 embed_dim=32, num_heads=[4, 8], window_size=4,\n",
        "                 mlp_ratio=4., depth=2):\n",
        "        super(SwinUNet, self).__init__()\n",
        "        self.input_channels = input_channels\n",
        "        self.output_channels = output_channels\n",
        "\n",
        "        # Initial convolutional layers\n",
        "        self.conv1 = SeparableConv2d(input_channels, 24, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(24)\n",
        "        self.conv2 = SeparableConv2d(24, 24, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(24)\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)  # 256x256 -> 128x128\n",
        "\n",
        "        # First Swin Transformer Block\n",
        "        self.swin_unet_E1 = SwinTransformerBlock(\n",
        "            dim=24,  # Changed from embed_dim=32 to 24\n",
        "            num_heads=num_heads[0],\n",
        "            window_size=window_size,\n",
        "            shift_size=window_size//2 if True else 0,\n",
        "            mlp_ratio=mlp_ratio\n",
        "        )\n",
        "\n",
        "        # Second convolutional block\n",
        "        self.conv3 = SeparableConv2d(24, 48, kernel_size=3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(48)\n",
        "        self.conv4 = SeparableConv2d(48, 48, kernel_size=3, padding=1)\n",
        "        self.bn4 = nn.BatchNorm2d(48)\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)  # 128x128 -> 64x64\n",
        "\n",
        "        # Second Swin Transformer Block\n",
        "        self.swin_unet_E2 = SwinTransformerBlock(\n",
        "            dim=48,\n",
        "            num_heads=num_heads[1],\n",
        "            window_size=window_size,\n",
        "            shift_size=window_size//2 if True else 0,\n",
        "            mlp_ratio=mlp_ratio\n",
        "        )\n",
        "\n",
        "        # Third convolutional block (Bottleneck)\n",
        "        self.conv5 = SeparableConv2d(48, 96, kernel_size=3, padding=1)\n",
        "        self.bn5 = nn.BatchNorm2d(96)\n",
        "        self.conv6 = SeparableConv2d(96, 96, kernel_size=3, padding=1)\n",
        "        self.bn6 = nn.BatchNorm2d(96)\n",
        "        self.drop5 = nn.Dropout(0.5)\n",
        "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)  # 64x64 -> 32x32\n",
        "\n",
        "        # Bottleneck convolutions with dense connections\n",
        "        self.conv7 = SeparableConv2d(96, 192, kernel_size=3, padding=1)\n",
        "        self.bn7 = nn.BatchNorm2d(192)\n",
        "        self.conv8 = SeparableConv2d(192, 192, kernel_size=3, padding=1)\n",
        "        self.bn8 = nn.BatchNorm2d(192)\n",
        "        self.drop6_1 = nn.Dropout(0.5)\n",
        "\n",
        "        self.conv9 = SeparableConv2d(192, 192, kernel_size=3, padding=1)\n",
        "        self.bn9 = nn.BatchNorm2d(192)\n",
        "        self.conv10 = SeparableConv2d(192, 192, kernel_size=3, padding=1)\n",
        "        self.bn10 = nn.BatchNorm2d(192)\n",
        "        self.drop6_2 = nn.Dropout(0.5)\n",
        "\n",
        "        self.concat1 = nn.Sequential(\n",
        "            SeparableConv2d(384, 192, kernel_size=3, padding=1),\n",
        "            SeparableConv2d(192, 192, kernel_size=3, padding=1)\n",
        "        )\n",
        "        self.drop6_3 = nn.Dropout(0.5)\n",
        "\n",
        "        # First Upsampling Block\n",
        "        self.up1 = nn.ConvTranspose2d(192, 96, kernel_size=2, stride=2)  # 32x32 -> 64x64\n",
        "        self.bn_up1 = nn.BatchNorm2d(96)\n",
        "        self.relu_up1 = nn.ReLU(inplace=True)\n",
        "        self.bidirectional_convLSTM1 = BidirectionalConvLSTM2D(input_channels=96, hidden_channels=192, kernel_size=3, num_layers=1)\n",
        "        self.swin_unet_D1 = SwinTransformerBlock(\n",
        "            dim=192 * 2,  # Adjusted for bidirectional output\n",
        "            num_heads=num_heads[0],\n",
        "            window_size=window_size,\n",
        "            shift_size=window_size//2 if True else 0,\n",
        "            mlp_ratio=mlp_ratio\n",
        "        )\n",
        "        self.conv11 = SeparableConv2d(192 * 2, 48, kernel_size=3, padding=1)\n",
        "        self.conv12 = SeparableConv2d(48, 48, kernel_size=3, padding=1)\n",
        "\n",
        "        # Second Upsampling Block\n",
        "        self.up2 = nn.ConvTranspose2d(48, 48, kernel_size=2, stride=2)  # 64x64 -> 128x128\n",
        "        self.bn_up2 = nn.BatchNorm2d(48)\n",
        "        self.relu_up2 = nn.ReLU(inplace=True)\n",
        "        self.bidirectional_convLSTM2 = BidirectionalConvLSTM2D(input_channels=48, hidden_channels=96, kernel_size=3, num_layers=1)\n",
        "        self.swin_unet_D2 = SwinTransformerBlock(\n",
        "            dim=96 * 2,\n",
        "            num_heads=num_heads[1],\n",
        "            window_size=window_size,\n",
        "            shift_size=window_size//2 if True else 0,\n",
        "            mlp_ratio=mlp_ratio\n",
        "        )\n",
        "        self.conv13 = SeparableConv2d(96 * 2, 24, kernel_size=3, padding=1)\n",
        "        self.conv14 = SeparableConv2d(24, 24, kernel_size=3, padding=1)\n",
        "\n",
        "        # Third Upsampling Block\n",
        "        self.up3 = nn.ConvTranspose2d(24, 24, kernel_size=2, stride=2)  # 128x128 -> 256x256\n",
        "        self.bn_up3 = nn.BatchNorm2d(24)\n",
        "        self.relu_up3 = nn.ReLU(inplace=True)\n",
        "        self.bidirectional_convLSTM3 = BidirectionalConvLSTM2D(input_channels=24, hidden_channels=48, kernel_size=3, num_layers=1)\n",
        "        self.swin_unet_D3 = SwinTransformerBlock(\n",
        "            dim=48 * 2,\n",
        "            num_heads=num_heads[1],\n",
        "            window_size=window_size,\n",
        "            shift_size=window_size//2 if True else 0,\n",
        "            mlp_ratio=mlp_ratio\n",
        "        )\n",
        "        self.conv15 = SeparableConv2d(48 * 2, 24, kernel_size=3, padding=1)\n",
        "        self.conv16 = SeparableConv2d(24, 24, kernel_size=3, padding=1)\n",
        "\n",
        "        # Output Layer\n",
        "        self.final_conv1 = nn.Conv2d(24, 2, kernel_size=3, padding=1)\n",
        "        self.final_relu = nn.ReLU(inplace=True)\n",
        "        self.final_conv2 = nn.Conv2d(2, 1, kernel_size=1, padding=0)\n",
        "        self.final_sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass of the Swin U-Net model.\n",
        "        Args:\n",
        "            x: Input tensor with shape (B, 3, 256, 256)\n",
        "        Returns:\n",
        "            torch.Tensor: Output segmentation mask with shape (B, 1, 256, 256)\n",
        "        \"\"\"\n",
        "        # Initial Convolutions\n",
        "        x1 = self.conv1(x)          # (B, 24, 256, 256)\n",
        "        x1 = self.bn1(x1)\n",
        "        x1 = self.conv2(x1)         # (B, 24, 256, 256)\n",
        "        x1 = self.bn2(x1)\n",
        "        p1 = self.pool1(x1)         # (B, 24, 128, 128)\n",
        "\n",
        "        # First Swin Transformer Block\n",
        "        p1_flat = p1.flatten(2).transpose(1, 2)  # (B, 128*128, 24)\n",
        "        swin_E1 = self.swin_unet_E1(p1_flat)     # (B, 128*128, 24)\n",
        "        swin_E1 = swin_E1.transpose(1, 2).view(-1, 24, 128, 128)  # Reshape for Conv2d\n",
        "\n",
        "        # Second Convolutional Block\n",
        "        x2 = self.conv3(swin_E1)    # (B, 48, 128, 128)\n",
        "        x2 = self.bn3(x2)\n",
        "        x2 = self.conv4(x2)          # (B, 48, 128, 128)\n",
        "        x2 = self.bn4(x2)\n",
        "        p2 = self.pool2(x2)          # (B, 48, 64, 64)\n",
        "\n",
        "        # Second Swin Transformer Block\n",
        "        p2_flat = p2.flatten(2).transpose(1, 2)  # (B, 64*64, 48)\n",
        "        swin_E2 = self.swin_unet_E2(p2_flat)     # (B, 64*64, 48)\n",
        "        swin_E2 = swin_E2.transpose(1, 2).view(-1, 48, 64, 64)  # Reshape for Conv2d\n",
        "\n",
        "        # Third Convolutional Block (Bottleneck)\n",
        "        x3 = self.conv5(swin_E2)    # (B, 96, 64, 64)\n",
        "        x3 = self.bn5(x3)\n",
        "        x3 = self.conv6(x3)          # (B, 96, 64, 64)\n",
        "        x3 = self.bn6(x3)\n",
        "        x3 = self.drop5(x3)\n",
        "        p3 = self.pool3(x3)          # (B, 96, 32, 32)\n",
        "\n",
        "        # Bottleneck Convolutions with Dense Connections\n",
        "        x4 = self.conv7(p3)          # (B, 192, 32, 32)\n",
        "        x4 = self.bn7(x4)\n",
        "        x4 = self.conv8(x4)          # (B, 192, 32, 32)\n",
        "        x4 = self.bn8(x4)\n",
        "        x4 = self.drop6_1(x4)\n",
        "\n",
        "        x5 = self.conv9(x4)          # (B, 192, 32, 32)\n",
        "        x5 = self.bn9(x5)\n",
        "        x5 = self.conv10(x5)         # (B, 192, 32, 32)\n",
        "        x5 = self.bn10(x5)\n",
        "        x5 = self.drop6_2(x5)\n",
        "\n",
        "        concat = torch.cat([x5, x4], dim=1)  # (B, 384, 32, 32)\n",
        "        concat = self.concat1(concat)         # (B, 192, 32, 32)\n",
        "        concat = self.drop6_3(concat)         # (B, 192, 32, 32)\n",
        "\n",
        "        # First Upsampling Block\n",
        "        up1 = self.up1(concat)                 # (B, 96, 64, 64)\n",
        "        up1 = self.bn_up1(up1)\n",
        "        up1 = self.relu_up1(up1)\n",
        "\n",
        "        # Prepare for BidirectionalConvLSTM2D\n",
        "        up1_seq = torch.stack([x3, up1], dim=1)  # (B, 2, 96, 64, 64)\n",
        "        bidir_convLSTM1_out = self.bidirectional_convLSTM1(up1_seq)  # (B, 192*2, 64, 64)\n",
        "\n",
        "        # Swin Transformer Block in Decoder\n",
        "        bidir_convLSTM1_flat = bidir_convLSTM1_out.flatten(2).transpose(1, 2)  # (B, 64*64, 192*2)\n",
        "        swin_D1 = self.swin_unet_D1(bidir_convLSTM1_flat)               # (B, 64*64, 192*2)\n",
        "        swin_D1 = swin_D1.transpose(1, 2).view(-1, 192*2, 64, 64)    # Reshape for Conv2d\n",
        "\n",
        "        # Further Convolutions\n",
        "        conv6 = self.conv11(swin_D1)        # (B, 48, 64, 64)\n",
        "        conv6 = self.conv12(conv6)          # (B, 48, 64, 64)\n",
        "\n",
        "        # Second Upsampling Block\n",
        "        up2 = self.up2(conv6)               # (B, 48, 128, 128)\n",
        "        up2 = self.bn_up2(up2)\n",
        "        up2 = self.relu_up2(up2)\n",
        "\n",
        "        # Prepare for BidirectionalConvLSTM2D\n",
        "        up2_seq = torch.stack([x2, up2], dim=1)  # (B, 2, 48, 128, 128)\n",
        "        bidir_convLSTM2_out = self.bidirectional_convLSTM2(up2_seq)  # (B, 96*2, 128, 128)\n",
        "\n",
        "        # Swin Transformer Block in Decoder\n",
        "        bidir_convLSTM2_flat = bidir_convLSTM2_out.flatten(2).transpose(1, 2)  # (B, 128*128, 96*2)\n",
        "        swin_D2 = self.swin_unet_D2(bidir_convLSTM2_flat)               # (B, 128*128, 96*2)\n",
        "        swin_D2 = swin_D2.transpose(1, 2).view(-1, 96*2, 128, 128)    # Reshape for Conv2d\n",
        "\n",
        "        # Further Convolutions\n",
        "        conv7 = self.conv13(swin_D2)        # (B, 24, 128, 128)\n",
        "        conv7 = self.conv14(conv7)          # (B, 24, 128, 128)\n",
        "\n",
        "        # Third Upsampling Block\n",
        "        up3 = self.up3(conv7)               # (B, 24, 256, 256)\n",
        "        up3 = self.bn_up3(up3)\n",
        "        up3 = self.relu_up3(up3)\n",
        "\n",
        "        # Prepare for BidirectionalConvLSTM2D\n",
        "        up3_seq = torch.stack([x1, up3], dim=1)  # (B, 2, 24, 256, 256)\n",
        "        bidir_convLSTM3_out = self.bidirectional_convLSTM3(up3_seq)  # (B, 48*2, 256, 256)\n",
        "\n",
        "        # Swin Transformer Block in Decoder\n",
        "        bidir_convLSTM3_flat = bidir_convLSTM3_out.flatten(2).transpose(1, 2)  # (B, 256*256, 48*2)\n",
        "        swin_D3 = self.swin_unet_D3(bidir_convLSTM3_flat)               # (B, 256*256, 48*2)\n",
        "        swin_D3 = swin_D3.transpose(1, 2).view(-1, 48*2, 256, 256)    # Reshape for Conv2d\n",
        "\n",
        "        # Further Convolutions\n",
        "        conv8 = self.conv15(swin_D3)        # (B, 24, 256, 256)\n",
        "        conv8 = self.conv16(conv8)          # (B, 24, 256, 256)\n",
        "\n",
        "        # Final Output Convolutions\n",
        "        final = self.final_conv1(conv8)      # (B, 2, 256, 256)\n",
        "        final = self.final_relu(final)\n",
        "        final = self.final_conv2(final)      # (B, 1, 256, 256)\n",
        "        final = self.final_sigmoid(final)    # (B, 1, 256, 256)\n",
        "\n",
        "        return final\n",
        "\n",
        "# ================================== Dataset Class ======================================\n",
        "\n",
        "class SegmentationDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Custom Dataset for image segmentation tasks.\n",
        "    Expects images in 'x' folder and masks in 'y' folder.\n",
        "    \"\"\"\n",
        "    def __init__(self, images_dir, masks_dir, transform=None):\n",
        "        super(SegmentationDataset, self).__init__()\n",
        "        self.images_dir = images_dir\n",
        "        self.masks_dir = masks_dir\n",
        "        self.transform = transform\n",
        "\n",
        "        self.images = sorted(os.listdir(images_dir))\n",
        "        self.masks = sorted(os.listdir(masks_dir))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Load image\n",
        "        img_path = os.path.join(self.images_dir, self.images[idx])\n",
        "        image = Image.open(img_path).convert('RGB')  # Ensure RGB\n",
        "\n",
        "        # Load mask\n",
        "        mask_path = os.path.join(self.masks_dir, self.masks[idx])\n",
        "        mask = Image.open(mask_path).convert('L')    # Grayscale\n",
        "\n",
        "        # Apply transformations\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "            mask = self.transform(mask)\n",
        "\n",
        "        return image, mask\n",
        "\n",
        "# =============================== Data Loading and Preprocessing ========================\n",
        "\n",
        "# Define image dimensions\n",
        "im_height = 256\n",
        "im_width = 256\n",
        "\n",
        "# Define transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((im_height, im_width)),\n",
        "    transforms.ToTensor(),  # Converts to [0,1]\n",
        "])\n",
        "\n",
        "# Paths to the dataset (update these paths as per your directory structure)\n",
        "train_images_dir = r'/content/drive/MyDrive/ML/dataset/ISIC2018_Task1-2_Training_Input'\n",
        "train_masks_dir = r'/content/drive/MyDrive/ML/dataset/ISIC2018_Task1_Training_GroundTruth'\n",
        "test_images_dir = r'/content/drive/MyDrive/ML/dataset/ISIC2018_Task1-2_Test_Input'\n",
        "test_masks_dir = r'/content/drive/MyDrive/ML/dataset/ISIC2018_Task1_Test_GroundTruth'\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = SegmentationDataset(train_images_dir, train_masks_dir, transform=transform)\n",
        "test_dataset = SegmentationDataset(test_images_dir, test_masks_dir, transform=transform)\n",
        "\n",
        "# Split training data into training and validation sets (80-20 split)\n",
        "train_size = int(0.8 * len(train_dataset))\n",
        "valid_size = len(train_dataset) - train_size\n",
        "train_subset, valid_subset = torch.utils.data.random_split(train_dataset, [train_size, valid_size])\n",
        "\n",
        "# Create DataLoaders\n",
        "batch_size = 5\n",
        "\n",
        "train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
        "valid_loader = DataLoader(valid_subset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
        "\n",
        "# =============================== Training Setup ==========================================\n",
        "\n",
        "# Instantiate the model\n",
        "model = SwinUNet(input_channels=3, output_channels=1, embed_dim=32, num_heads=[4, 8], window_size=4, mlp_ratio=4., depth=2)\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model = model.to(device)  # Move to GPU if available\n",
        "\n",
        "# Initialize weights using Kaiming Normal initialization\n",
        "def initialize_weights(module):\n",
        "    if isinstance(module, (nn.Conv2d, nn.ConvTranspose2d)):\n",
        "        nn.init.kaiming_normal_(module.weight, mode='fan_in', nonlinearity='relu')\n",
        "        if module.bias is not None:\n",
        "            nn.init.zeros_(module.bias)\n",
        "    elif isinstance(module, nn.Linear):\n",
        "        nn.init.kaiming_normal_(module.weight, mode='fan_in', nonlinearity='relu')\n",
        "        if module.bias is not None:\n",
        "            nn.init.zeros_(module.bias)\n",
        "    elif isinstance(module, nn.Conv3d):\n",
        "        nn.init.kaiming_normal_(module.weight, mode='fan_in', nonlinearity='relu')\n",
        "        if module.bias is not None:\n",
        "            nn.init.zeros_(module.bias)\n",
        "\n",
        "model.apply(initialize_weights)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = DiceLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "# Define learning rate scheduler and early stopping parameters\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.25, patience=5, verbose=True, min_lr=1e-9)\n",
        "early_stopping_patience = 9\n",
        "best_val_loss = np.inf\n",
        "epochs_no_improve = 0\n",
        "\n",
        "# =============================== Training Loop ===========================================\n",
        "\n",
        "num_epochs = 1  # You can adjust the number of epochs\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    train_loader_count = 0\n",
        "    for images, masks in train_loader:\n",
        "        train_loader_count += 1\n",
        "        print(f\"Train loader image count: {train_loader_count}\")\n",
        "        images = images.to(device)  # (B, 3, 256, 256)\n",
        "        masks = masks.to(device)    # (B, 1, 256, 256)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)      # (B, 1, 256, 256)\n",
        "        loss = criterion(outputs, masks)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader.dataset)\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for images, masks in valid_loader:\n",
        "            images = images.to(device)\n",
        "            masks = masks.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, masks)\n",
        "\n",
        "            val_loss += loss.item() * images.size(0)\n",
        "\n",
        "    val_loss /= len(valid_loader.dataset)\n",
        "\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, Training Loss: {epoch_loss:.4f}, Validation Loss: {val_loss:.4f}')\n",
        "\n",
        "    # Scheduler step\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "    # Early stopping\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        epochs_no_improve = 0\n",
        "        # Save the best model\n",
        "        torch.save(model.state_dict(), r'/content/drive/MyDrive/model/modelWeights_Swin_Trans_Weights_Swin_Trans_Leather.pth')\n",
        "    else:\n",
        "        epochs_no_improve += 1\n",
        "        if epochs_no_improve >= early_stopping_patience:\n",
        "            print('Early stopping!')\n",
        "            break\n",
        "\n",
        "# ================================== Prediction ==========================================\n",
        "\n",
        "# Load the best model weights\n",
        "model.load_state_dict(torch.load(r'/content/drive/MyDrive/model/modelWeights_Swin_Trans_Weights_Swin_Trans_Leather.pth'))\n",
        "model.eval()\n",
        "\n",
        "# Function to save predictions and ground truth\n",
        "def save_predictions(model, dataloader, save_dir_pred, save_dir_gt, device):\n",
        "    \"\"\"\n",
        "    Saves the predicted masks and ground truth masks.\n",
        "    Args:\n",
        "        model (nn.Module): Trained model.\n",
        "        dataloader (DataLoader): DataLoader for test data.\n",
        "        save_dir_pred (str): Directory to save predicted masks.\n",
        "        save_dir_gt (str): Directory to save ground truth masks.\n",
        "        device (str): Device to run the model on.\n",
        "    \"\"\"\n",
        "    os.makedirs(save_dir_pred, exist_ok=True)\n",
        "    os.makedirs(save_dir_gt, exist_ok=True)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (images, masks) in enumerate(dataloader):\n",
        "            if (i % 100 == 0):\n",
        "                print(f\"{i}th Test Image\")  # Adjust as per your dataset\n",
        "\n",
        "            images = images.to(device)\n",
        "            masks = masks.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            preds = outputs.cpu().numpy()\n",
        "            gts = masks.cpu().numpy()\n",
        "\n",
        "            for j in range(preds.shape[0]):\n",
        "                pred_mask = preds[j, 0, :, :]\n",
        "                gt_mask = gts[j, 0, :, :]\n",
        "\n",
        "                # Save predicted mask\n",
        "                pred_img = Image.fromarray((pred_mask * 255).astype(np.uint8))\n",
        "                pred_img.save(os.path.join(save_dir_pred, f\"{i * dataloader.batch_size + j + 1}.png\"))\n",
        "\n",
        "                # Save ground truth mask\n",
        "                gt_img = Image.fromarray((gt_mask * 255).astype(np.uint8))\n",
        "                gt_img.save(os.path.join(save_dir_gt, f\"{i * dataloader.batch_size + j + 1}.tiff\"))\n",
        "\n",
        "# Define directories to save predictions and ground truth\n",
        "save_dir_pred = r'/content/drive/MyDrive/output/segmented predicted images'\n",
        "save_dir_gt = r'/content/drive/MyDrive/output/segmented ground truth'\n",
        "\n",
        "# Save predictions\n",
        "save_predictions(model, test_loader, save_dir_pred, save_dir_gt, device)\n",
        "\n",
        "# =================================== Evaluation =========================================\n",
        "\n",
        "def evaluate_metrics_pytorch(model, dataloader, device):\n",
        "    \"\"\"\n",
        "    Evaluates various metrics for segmentation performance.\n",
        "    Args:\n",
        "        model (nn.Module): Trained model.\n",
        "        dataloader (DataLoader): DataLoader for test data.\n",
        "        device (str): Device to run the model on.\n",
        "    Returns:\n",
        "        dict: Dictionary containing average metrics.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    all_metrics = {\n",
        "        'Accuracy': [],\n",
        "        'Dice': [],\n",
        "        'Jaccard': [],\n",
        "        'Sensitivity': [],\n",
        "        'Specificity': [],\n",
        "        'Precision': [],\n",
        "        'Recall': [],\n",
        "        'F1-Score': []\n",
        "    }\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, masks in dataloader:\n",
        "            images = images.to(device)\n",
        "            masks = masks.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            preds = outputs > 0.5  # Binary mask\n",
        "\n",
        "            preds = preds.cpu().numpy().astype(np.uint8)\n",
        "            masks = masks.cpu().numpy().astype(np.uint8)\n",
        "            masks = (masks > 0).astype(np.uint8)  # Convert to binary masks\n",
        "\n",
        "            for pred, mask in zip(preds, masks):\n",
        "                pred_flat = pred.flatten()\n",
        "                mask_flat = mask.flatten()\n",
        "\n",
        "                # Calculate metrics\n",
        "                tn, fp, fn, tp = confusion_matrix(mask_flat, pred_flat, labels=[0,1]).ravel()\n",
        "\n",
        "                accuracy = (tp + tn) / (tp + tn + fp + fn + 1e-8)\n",
        "                iou = jaccard_score(mask_flat, pred_flat, zero_division=0)\n",
        "                dice = f1_score(mask_flat, pred_flat, zero_division=0)\n",
        "                specificity = tn / (tn + fp + 1e-8)\n",
        "                sensitivity = recall_score(mask_flat, pred_flat, zero_division=0)\n",
        "                precision = precision_score(mask_flat, pred_flat, zero_division=0)\n",
        "                recall = sensitivity\n",
        "                f1 = dice  # F1-Score is the same as Dice coefficient for binary classification\n",
        "\n",
        "                all_metrics['Accuracy'].append(accuracy)\n",
        "                all_metrics['Jaccard'].append(iou)\n",
        "                all_metrics['Dice'].append(dice)\n",
        "                all_metrics['Specificity'].append(specificity)\n",
        "                all_metrics['Sensitivity'].append(sensitivity)\n",
        "                all_metrics['Precision'].append(precision)\n",
        "                all_metrics['Recall'].append(recall)\n",
        "                all_metrics['F1-Score'].append(f1)\n",
        "\n",
        "    # Compute average metrics\n",
        "    avg_metrics = {metric: np.mean(values) for metric, values in all_metrics.items()}\n",
        "\n",
        "    print(\"Evaluation Metrics:\")\n",
        "    for metric, value in avg_metrics.items():\n",
        "        print(f\"{metric}: {value:.4f}\")\n",
        "\n",
        "    return avg_metrics\n",
        "\n",
        "# Evaluate the model\n",
        "metrics = evaluate_metrics_pytorch(model, test_loader, device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mh37fiQOLvJt"
      },
      "outputs": [],
      "source": [
        "!pip install torchinfo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GVicjKqLLSXV"
      },
      "outputs": [],
      "source": [
        "from torchinfo import summary\n",
        "\n",
        "# Define the input size based on your model's expected input.\n",
        "# For example, if your model expects images with 3 channels and 256x256 dimensions:\n",
        "input_size = (1, 3, 256, 256)  # (batch_size, channels, height, width)\n",
        "\n",
        "# Generate and print the model summary\n",
        "summary(model, input_size=input_size, device='cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ZXUJmRPLqjP"
      },
      "outputs": [],
      "source": [
        "# ============================== Imports and Dependencies ==============================\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "\n",
        "from sklearn.metrics import (\n",
        "    confusion_matrix,\n",
        "    jaccard_score, f1_score, precision_score, recall_score\n",
        ")\n",
        "\n",
        "# ================================ Separable Convolution =================================\n",
        "\n",
        "class SeparableConv2d(nn.Module):\n",
        "    \"\"\"\n",
        "    Implements a separable convolution layer using depthwise and pointwise convolutions.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, padding=1, bias=True):\n",
        "        super(SeparableConv2d, self).__init__()\n",
        "        # Depthwise convolution (groups=in_channels)\n",
        "        self.depthwise = nn.Conv2d(in_channels, in_channels, kernel_size=kernel_size,\n",
        "                                   padding=padding, groups=in_channels, bias=bias)\n",
        "        # Pointwise convolution\n",
        "        self.pointwise = nn.Conv2d(in_channels, out_channels, kernel_size=1,\n",
        "                                   padding=0, bias=bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.depthwise(x)\n",
        "        x = self.pointwise(x)\n",
        "        return x\n",
        "\n",
        "# ================================== ConvLSTM2D ========================================\n",
        "\n",
        "class ConvLSTMCell(nn.Module):\n",
        "    \"\"\"\n",
        "    Implements a ConvLSTM cell.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_channels, hidden_channels, kernel_size, bias=True):\n",
        "        super(ConvLSTMCell, self).__init__()\n",
        "\n",
        "        padding = kernel_size // 2  # To maintain spatial dimensions\n",
        "        self.input_channels = input_channels\n",
        "        self.hidden_channels = hidden_channels\n",
        "\n",
        "        self.conv = nn.Conv2d(in_channels=input_channels + hidden_channels,\n",
        "                              out_channels=4 * hidden_channels,\n",
        "                              kernel_size=kernel_size,\n",
        "                              padding=padding,\n",
        "                              bias=bias)\n",
        "\n",
        "    def forward(self, input_tensor, cur_state):\n",
        "        h_cur, c_cur = cur_state\n",
        "\n",
        "        # Concatenate input and hidden state\n",
        "        combined = torch.cat([input_tensor, h_cur], dim=1)  # along channel axis\n",
        "\n",
        "        # Compute all gates at once\n",
        "        conv_output = self.conv(combined)\n",
        "        cc_i, cc_f, cc_o, cc_g = torch.split(conv_output, self.hidden_channels, dim=1)\n",
        "\n",
        "        i = torch.sigmoid(cc_i)   # input gate\n",
        "        f = torch.sigmoid(cc_f)   # forget gate\n",
        "        o = torch.sigmoid(cc_o)   # output gate\n",
        "        g = torch.tanh(cc_g)      # gate gate\n",
        "\n",
        "        c_next = f * c_cur + i * g\n",
        "        h_next = o * torch.tanh(c_next)\n",
        "\n",
        "        return h_next, c_next\n",
        "\n",
        "    def init_hidden(self, batch_size, spatial_size, device):\n",
        "        height, width = spatial_size\n",
        "        return (torch.zeros(batch_size, self.hidden_channels, height, width, device=device),\n",
        "                torch.zeros(batch_size, self.hidden_channels, height, width, device=device))\n",
        "\n",
        "class ConvLSTM2D(nn.Module):\n",
        "    \"\"\"\n",
        "    Implements a ConvLSTM2D layer that processes a sequence of inputs.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_channels, hidden_channels, kernel_size=3, bias=True, num_layers=1):\n",
        "        super(ConvLSTM2D, self).__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.hidden_channels = hidden_channels\n",
        "\n",
        "        layers = []\n",
        "        for i in range(num_layers):\n",
        "            input_c = input_channels if i == 0 else hidden_channels\n",
        "            layers.append(ConvLSTMCell(input_c, hidden_channels, kernel_size, bias))\n",
        "        self.layers = nn.ModuleList(layers)\n",
        "\n",
        "    def forward(self, input_tensor, reverse=False):\n",
        "        # input_tensor shape: (batch, seq_len, channels, height, width)\n",
        "        batch_size, seq_len, channels, height, width = input_tensor.size()\n",
        "        device = input_tensor.device\n",
        "\n",
        "        # Initialize hidden and cell states for all layers\n",
        "        hidden_state = []\n",
        "        cell_state = []\n",
        "        for i in range(self.num_layers):\n",
        "            h, c = self.layers[i].init_hidden(batch_size, (height, width), device)\n",
        "            hidden_state.append(h)\n",
        "            cell_state.append(c)\n",
        "\n",
        "        # Iterate over time steps\n",
        "        if reverse:\n",
        "            time_steps = reversed(range(seq_len))\n",
        "        else:\n",
        "            time_steps = range(seq_len)\n",
        "\n",
        "        outputs = []\n",
        "        for t in time_steps:\n",
        "            x = input_tensor[:, t, :, :, :]  # (batch, channels, height, width)\n",
        "            for i, layer in enumerate(self.layers):\n",
        "                h, c = layer(x, (hidden_state[i], cell_state[i]))\n",
        "                hidden_state[i] = h\n",
        "                cell_state[i] = c\n",
        "                x = h  # input to next layer\n",
        "            outputs.append(x)\n",
        "\n",
        "        outputs = torch.stack(outputs, dim=1)  # (batch, seq_len, channels, height, width)\n",
        "        if reverse:\n",
        "            outputs = outputs.flip(dims=[1])  # Reverse back to original order\n",
        "        return outputs  # Return the sequence of outputs\n",
        "\n",
        "class BidirectionalConvLSTM2D(nn.Module):\n",
        "    \"\"\"\n",
        "    Implements a Bidirectional ConvLSTM2D layer.\n",
        "    Processes the input sequence in both forward and backward directions and concatenates the outputs.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_channels, hidden_channels, kernel_size=3, num_layers=1, bias=True):\n",
        "        super(BidirectionalConvLSTM2D, self).__init__()\n",
        "        self.forward_conv_lstm = ConvLSTM2D(input_channels, hidden_channels, kernel_size, bias=bias, num_layers=num_layers)\n",
        "        self.backward_conv_lstm = ConvLSTM2D(input_channels, hidden_channels, kernel_size, bias=bias, num_layers=num_layers)\n",
        "\n",
        "    def forward(self, input_tensor):\n",
        "        # input_tensor shape: (batch, seq_len, channels, height, width)\n",
        "        # Forward direction\n",
        "        forward_output = self.forward_conv_lstm(input_tensor, reverse=False)  # (batch, seq_len, hidden_channels, H, W)\n",
        "        # Backward direction\n",
        "        backward_output = self.backward_conv_lstm(input_tensor, reverse=True)  # (batch, seq_len, hidden_channels, H, W)\n",
        "        # Concatenate outputs along the channel dimension\n",
        "        output = torch.cat([forward_output, backward_output], dim=2)  # (batch, seq_len, hidden_channels*2, H, W)\n",
        "        # Since seq_len=2, we can take the last output\n",
        "        output = output[:, -1, :, :, :]  # Take the last output (batch, hidden_channels*2, H, W)\n",
        "        return output\n",
        "\n",
        "# =============================== Patch Embedding ========================================\n",
        "\n",
        "class PatchEmbed(nn.Module):\n",
        "    \"\"\"\n",
        "    Image to Patch Embedding.\n",
        "    \"\"\"\n",
        "    def __init__(self, img_size=256, patch_size=4, in_chans=3, embed_dim=32):\n",
        "        super().__init__()\n",
        "        self.img_size = img_size\n",
        "        self.patch_size = patch_size\n",
        "        num_patches = (img_size // patch_size) * (img_size // patch_size)\n",
        "        self.num_patches = num_patches\n",
        "\n",
        "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [B, C, H, W]\n",
        "        x = self.proj(x)  # [B, embed_dim, H/patch_size, W/patch_size]\n",
        "        x = x.flatten(2).transpose(1, 2)  # [B, num_patches, embed_dim]\n",
        "        x = self.norm(x)\n",
        "        return x\n",
        "\n",
        "# ============================== Swin Transformer Blocks ================================\n",
        "\n",
        "class WindowAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Window based multi-head self attention (W-MSA) module with relative position bias.\n",
        "    \"\"\"\n",
        "    def __init__(self, dim, window_size, num_heads, qkv_bias=True, attn_drop=0., proj_drop=0.):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            dim (int): Number of input channels.\n",
        "            window_size (tuple): Height and width of the window.\n",
        "            num_heads (int): Number of attention heads.\n",
        "            qkv_bias (bool): If True, add a learnable bias to query, key, value.\n",
        "            attn_drop (float): Dropout ratio of attention weights.\n",
        "            proj_drop (float): Dropout ratio of output.\n",
        "        \"\"\"\n",
        "        super(WindowAttention, self).__init__()\n",
        "        self.dim = dim\n",
        "        self.window_size = window_size  # Wh, Ww\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // num_heads\n",
        "        self.scale = head_dim ** -0.5\n",
        "\n",
        "        # Define a parameter table of relative position bias\n",
        "        self.relative_position_bias_table = nn.Parameter(\n",
        "            torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads)\n",
        "        )  # 2*Wh-1 * 2*Ww-1, nH\n",
        "\n",
        "        # Get pair-wise relative position index for each token inside the window\n",
        "        coords_h = torch.arange(self.window_size[0])\n",
        "        coords_w = torch.arange(self.window_size[1])\n",
        "        coords = torch.stack(torch.meshgrid(coords_h, coords_w, indexing='ij'))  # 2, Wh, Ww\n",
        "        coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n",
        "        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n",
        "        relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n",
        "        relative_coords[:, :, 0] += self.window_size[0] - 1  # shift to start from 0\n",
        "        relative_coords[:, :, 1] += self.window_size[1] - 1\n",
        "        relative_coords[:, :, 0] *= (2 * self.window_size[1] - 1)\n",
        "        relative_position_index = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n",
        "        self.register_buffer(\"relative_position_index\", relative_position_index)\n",
        "\n",
        "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)  # Query, Key, Value\n",
        "        self.attn_drop = nn.Dropout(attn_drop)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.proj_drop = nn.Dropout(proj_drop)\n",
        "\n",
        "        # Initialize relative position bias table\n",
        "        nn.init.trunc_normal_(self.relative_position_bias_table, std=.02)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: input features with shape of (num_windows*B, Wh*Ww, C)\n",
        "            mask: (num_windows, Wh*Ww, Wh*Ww) or None\n",
        "        \"\"\"\n",
        "        B_, N, C = x.shape\n",
        "        qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)  # 3, B_, nH, N, C//nH\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]  # each has shape (B_, nH, N, C//nH)\n",
        "\n",
        "        q = q * self.scale\n",
        "        attn = (q @ k.transpose(-2, -1))  # (B_, nH, N, N)\n",
        "\n",
        "        # Add relative position bias\n",
        "        relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(\n",
        "            self.window_size[0] * self.window_size[1], self.window_size[0] * self.window_size[1], -1\n",
        "        )  # Wh*Ww, Wh*Ww, nH\n",
        "        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n",
        "        attn = attn + relative_position_bias.unsqueeze(0)  # (B_, nH, N, N)\n",
        "\n",
        "        if mask is not None:\n",
        "            nW = mask.shape[0]\n",
        "            attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)\n",
        "            attn = attn.view(-1, self.num_heads, N, N)\n",
        "            attn = F.softmax(attn, dim=-1)\n",
        "        else:\n",
        "            attn = F.softmax(attn, dim=-1)\n",
        "\n",
        "        attn = self.attn_drop(attn)\n",
        "\n",
        "        out = (attn @ v).transpose(1, 2).reshape(B_, N, C)  # (B_, N, C)\n",
        "        out = self.proj(out)\n",
        "        out = self.proj_drop(out)\n",
        "        return out\n",
        "\n",
        "class SwinTransformerBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Swin Transformer Block with W-MSA and SW-MSA.\n",
        "    \"\"\"\n",
        "    def __init__(self, dim, num_heads, window_size=7, shift_size=0, mlp_ratio=4., qkv_bias=True,\n",
        "                 attn_drop=0., proj_drop=0., mlp_hidden_dim=512):\n",
        "        super(SwinTransformerBlock, self).__init__()\n",
        "        self.dim = dim\n",
        "        self.num_heads = num_heads\n",
        "        self.window_size = window_size  # W\n",
        "        self.shift_size = shift_size    # S\n",
        "        self.mlp_ratio = mlp_ratio\n",
        "\n",
        "        assert 0 <= self.shift_size < self.window_size, \"shift_size must be in [0, window_size)\"\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(dim)\n",
        "        self.attn = WindowAttention(dim, (window_size, window_size), num_heads, qkv_bias, attn_drop, proj_drop)\n",
        "\n",
        "        self.drop_path = nn.Identity()  # Can implement stochastic depth if desired\n",
        "        self.norm2 = nn.LayerNorm(dim)\n",
        "        # mlp_hidden_dim = int(dim * mlp_ratio)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(dim, mlp_hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(mlp_hidden_dim, dim),\n",
        "            nn.Dropout(proj_drop)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, H, W, mask_matrix=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: input features with shape (B, H*W, C)\n",
        "            H, W: spatial dimensions\n",
        "            mask_matrix: attention mask\n",
        "        \"\"\"\n",
        "        B, L, C = x.shape\n",
        "        assert L == H * W, \"Input feature has wrong size\"\n",
        "\n",
        "        shortcut = x\n",
        "        x = self.norm1(x)\n",
        "        x = x.view(B, H, W, C)\n",
        "\n",
        "        # Padding for window partition\n",
        "        pad_r = (self.window_size - W % self.window_size) % self.window_size\n",
        "        pad_b = (self.window_size - H % self.window_size) % self.window_size\n",
        "        x = F.pad(x, (0, 0, 0, pad_r, 0, pad_b))\n",
        "\n",
        "        _, Hp, Wp, _ = x.shape\n",
        "\n",
        "        # Cyclic shift\n",
        "        if self.shift_size > 0:\n",
        "            shifted_x = torch.roll(\n",
        "                x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2)\n",
        "            )\n",
        "            attn_mask = mask_matrix\n",
        "        else:\n",
        "            shifted_x = x\n",
        "            attn_mask = None\n",
        "\n",
        "        # Partition windows\n",
        "        x_windows = shifted_x.unfold(1, self.window_size, self.window_size).unfold(\n",
        "            2, self.window_size, self.window_size\n",
        "        )\n",
        "        x_windows = x_windows.contiguous().view(-1, self.window_size * self.window_size, C)\n",
        "\n",
        "        # W-MSA/SW-MSA\n",
        "        attn_windows = self.attn(x_windows, mask=attn_mask)\n",
        "\n",
        "        # Merge windows\n",
        "        attn_windows = attn_windows.view(-1, self.window_size, self.window_size, C)\n",
        "        shifted_x = attn_windows.view(\n",
        "            B, Hp // self.window_size, Wp // self.window_size, self.window_size, self.window_size, C\n",
        "        )\n",
        "        shifted_x = shifted_x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, Hp, Wp, C)\n",
        "\n",
        "        # Reverse cyclic shift\n",
        "        if self.shift_size > 0:\n",
        "            x = torch.roll(\n",
        "                shifted_x, shifts=(self.shift_size, self.shift_size), dims=(1, 2)\n",
        "            )\n",
        "        else:\n",
        "            x = shifted_x\n",
        "\n",
        "        # Remove padding\n",
        "        x = x[:, :H, :W, :].contiguous().view(B, H * W, C)\n",
        "\n",
        "        # FFN\n",
        "        x = shortcut + self.drop_path(x)\n",
        "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
        "\n",
        "        return x\n",
        "\n",
        "# =============================== Dice Loss Function ====================================\n",
        "\n",
        "class DiceLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Dice Loss function to maximize the Dice coefficient.\n",
        "    Suitable for binary segmentation tasks.\n",
        "    \"\"\"\n",
        "    def __init__(self, smooth=1.0):\n",
        "        super(DiceLoss, self).__init__()\n",
        "        self.smooth = smooth\n",
        "\n",
        "    def forward(self, y_pred, y_true):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            y_pred (torch.Tensor): Predicted mask probabilities with shape (B, 1, H, W)\n",
        "            y_true (torch.Tensor): Ground truth masks with shape (B, 1, H, W)\n",
        "        Returns:\n",
        "            torch.Tensor: Dice loss\n",
        "        \"\"\"\n",
        "        y_pred = y_pred.view(-1)\n",
        "        y_true = y_true.view(-1)\n",
        "\n",
        "        intersection = (y_pred * y_true).sum()\n",
        "        dice = (2. * intersection + self.smooth) / (y_pred.sum() + y_true.sum() + self.smooth)\n",
        "\n",
        "        return 1 - dice\n",
        "\n",
        "# =============================== Swin UNet Model ========================================\n",
        "\n",
        "class SwinUNet(nn.Module):\n",
        "    \"\"\"\n",
        "    Swin U-Net architecture for image segmentation.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_channels=3, output_channels=1,\n",
        "                 filter_num_begin=32, depth=4, stack_num_down=2, stack_num_up=2,\n",
        "                 num_heads=[4, 8, 8, 8], window_size=[4, 2, 2, 2], num_mlp=512,\n",
        "                 shift_window=True, **kwargs):\n",
        "        super(SwinUNet, self).__init__()\n",
        "\n",
        "        self.input_channels = input_channels\n",
        "        self.output_channels = output_channels\n",
        "        self.filter_num_begin = filter_num_begin\n",
        "        self.depth = depth\n",
        "        self.stack_num_down = stack_num_down\n",
        "        self.stack_num_up = stack_num_up\n",
        "        self.num_heads = num_heads\n",
        "        self.window_size = window_size\n",
        "        self.num_mlp = num_mlp\n",
        "        self.shift_window = shift_window\n",
        "\n",
        "        # Define the number of channels at each level\n",
        "        self.filter_nums = [filter_num_begin * (2 ** i) for i in range(depth)]\n",
        "        # Example: [32, 64, 128, 256] if depth=4\n",
        "\n",
        "        # Patch Embedding\n",
        "        self.patch_embed = PatchEmbed(\n",
        "            img_size=256, patch_size=4, in_chans=input_channels, embed_dim=self.filter_nums[0]\n",
        "        )\n",
        "\n",
        "        # Encoder\n",
        "        self.encoder_layers = nn.ModuleList()\n",
        "        for i in range(depth):\n",
        "            down_layers = nn.ModuleList()\n",
        "            for _ in range(stack_num_down):\n",
        "                down_layers.append(SwinTransformerBlock(\n",
        "                    dim=self.filter_nums[i],\n",
        "                    num_heads=num_heads[i],\n",
        "                    window_size=window_size[i],\n",
        "                    shift_size=window_size[i] // 2 if shift_window else 0,\n",
        "                    mlp_hidden_dim=num_mlp,\n",
        "                ))\n",
        "            self.encoder_layers.append(down_layers)\n",
        "\n",
        "            if i < depth - 1:\n",
        "                setattr(self, f\"downsample_{i}\", nn.Conv2d(\n",
        "                    self.filter_nums[i], self.filter_nums[i + 1], kernel_size=2, stride=2)\n",
        "                )\n",
        "\n",
        "        # Bottleneck\n",
        "        self.bottleneck_layers = nn.ModuleList()\n",
        "        for _ in range(stack_num_down):\n",
        "            self.bottleneck_layers.append(SwinTransformerBlock(\n",
        "                dim=self.filter_nums[-1],\n",
        "                num_heads=num_heads[-1],\n",
        "                window_size=window_size[-1],\n",
        "                shift_size=window_size[-1] // 2 if shift_window else 0,\n",
        "                mlp_hidden_dim=num_mlp,\n",
        "            ))\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder_layers = nn.ModuleList()\n",
        "        for i in range(depth):\n",
        "            if i == 0:\n",
        "                # The first decoder layer after the bottleneck\n",
        "                up_in_channels = self.filter_nums[depth - 1]\n",
        "                in_channels = self.filter_nums[depth - 1]\n",
        "                out_channels = self.filter_nums[depth - 1]\n",
        "            else:\n",
        "                up_in_channels = self.filter_nums[depth - i]\n",
        "                in_channels = self.filter_nums[depth - i - 1] * 2  # After concatenation\n",
        "                out_channels = self.filter_nums[depth - i - 1]\n",
        "\n",
        "            # Upsampling layer (except for the first decoder layer)\n",
        "            if i > 0:\n",
        "                setattr(self, f\"upsample_{i}\", nn.ConvTranspose2d(\n",
        "                    up_in_channels, out_channels, kernel_size=2, stride=2)\n",
        "                )\n",
        "\n",
        "            # Swin Transformer Blocks\n",
        "            up_layers = nn.ModuleList()\n",
        "            for _ in range(stack_num_up):\n",
        "                up_layers.append(SwinTransformerBlock(\n",
        "                    dim=in_channels,\n",
        "                    num_heads=num_heads[depth - i - 1],\n",
        "                    window_size=window_size[depth - i - 1],\n",
        "                    shift_size=window_size[depth - i - 1] // 2 if shift_window else 0,\n",
        "                    mlp_hidden_dim=num_mlp,\n",
        "                ))\n",
        "            self.decoder_layers.append(up_layers)\n",
        "\n",
        "        # Final Convolution\n",
        "        self.final_conv = nn.Sequential(\n",
        "            nn.Conv2d(self.filter_nums[0], self.filter_nums[0] // 2, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(self.filter_nums[0] // 2, output_channels, kernel_size=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Initial Patch Embedding\n",
        "        x = self.patch_embed(x)  # x shape: [B, num_patches, embed_dim]\n",
        "        B, N, C = x.shape\n",
        "        H = W = int(np.sqrt(N))\n",
        "\n",
        "        x = x.transpose(1, 2).view(B, C, H, W)  # [B, C, H, W]\n",
        "\n",
        "        # Encoder\n",
        "        encodings = []\n",
        "        for i, layers in enumerate(self.encoder_layers):\n",
        "            for blk in layers:\n",
        "                x = x.flatten(2).transpose(1, 2)  # [B, H*W, C]\n",
        "                x = blk(x, H, W)\n",
        "                x = x.transpose(1, 2).view(B, -1, H, W)\n",
        "            encodings.append(x)\n",
        "            if i < self.depth - 1:\n",
        "                downsample = getattr(self, f\"downsample_{i}\")\n",
        "                x = downsample(x)\n",
        "                _, _, H, W = x.shape\n",
        "\n",
        "        # Bottleneck\n",
        "        for blk in self.bottleneck_layers:\n",
        "            x = x.flatten(2).transpose(1, 2)\n",
        "            x = blk(x, H, W)\n",
        "            x = x.transpose(1, 2).view(B, -1, H, W)\n",
        "\n",
        "        # Decoder\n",
        "        for i in range(self.depth):\n",
        "            if i > 0:\n",
        "                upsample = getattr(self, f\"upsample_{i}\")\n",
        "                x = upsample(x)\n",
        "                _, _, H, W = x.shape\n",
        "\n",
        "                # Concatenate with skip connection\n",
        "                skip_connection = encodings[self.depth - i - 1]\n",
        "                x = torch.cat([x, skip_connection], dim=1)  # Concatenate along channels\n",
        "\n",
        "            for blk in self.decoder_layers[i]:\n",
        "                x = x.flatten(2).transpose(1, 2)\n",
        "                x = blk(x, H, W)\n",
        "                x = x.transpose(1, 2).view(B, -1, H, W)\n",
        "\n",
        "        # Final Convolution\n",
        "        x = self.final_conv(x)\n",
        "        x = torch.sigmoid(x)\n",
        "        return x\n",
        "# ================================== Dataset Class ======================================\n",
        "\n",
        "class SegmentationDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Custom Dataset for image segmentation tasks.\n",
        "    Expects images in 'x' folder and masks in 'y' folder.\n",
        "    \"\"\"\n",
        "    def __init__(self, images_dir, masks_dir, transform=None):\n",
        "        super(SegmentationDataset, self).__init__()\n",
        "        self.images_dir = images_dir\n",
        "        self.masks_dir = masks_dir\n",
        "        self.transform = transform\n",
        "\n",
        "        self.images = sorted(os.listdir(images_dir))\n",
        "        self.masks = sorted(os.listdir(masks_dir))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Load image\n",
        "        img_path = os.path.join(self.images_dir, self.images[idx])\n",
        "        image = Image.open(img_path).convert('RGB')  # Ensure RGB\n",
        "\n",
        "        # Load mask\n",
        "        mask_path = os.path.join(self.masks_dir, self.masks[idx])\n",
        "        mask = Image.open(mask_path).convert('L')    # Grayscale\n",
        "\n",
        "        # Apply transformations\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "            mask = self.transform(mask)\n",
        "\n",
        "        return image, mask\n",
        "\n",
        "# =============================== Data Loading and Preprocessing ========================\n",
        "\n",
        "# Define image dimensions\n",
        "im_height = 256\n",
        "im_width = 256\n",
        "\n",
        "# Define transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((im_height, im_width)),\n",
        "    transforms.ToTensor(),  # Converts to [0,1]\n",
        "])\n",
        "\n",
        "# Paths to the dataset (update these paths as per your directory structure)\n",
        "train_images_dir = r'/content/drive/MyDrive/ML/dataset/ISIC2018_Task1-2_Training_Input'\n",
        "train_masks_dir = r'/content/drive/MyDrive/ML/dataset/ISIC2018_Task1_Training_GroundTruth'\n",
        "test_images_dir = r'/content/drive/MyDrive/ML/dataset/ISIC2018_Task1-2_Test_Input'\n",
        "test_masks_dir = r'/content/drive/MyDrive/ML/dataset/ISIC2018_Task1_Test_GroundTruth'\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = SegmentationDataset(train_images_dir, train_masks_dir, transform=transform)\n",
        "test_dataset = SegmentationDataset(test_images_dir, test_masks_dir, transform=transform)\n",
        "\n",
        "# Split training data into training and validation sets (80-20 split)\n",
        "train_size = int(0.8 * len(train_dataset))\n",
        "valid_size = len(train_dataset) - train_size\n",
        "train_subset, valid_subset = torch.utils.data.random_split(train_dataset, [train_size, valid_size])\n",
        "\n",
        "# Create DataLoaders\n",
        "batch_size = 5\n",
        "\n",
        "train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
        "valid_loader = DataLoader(valid_subset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
        "\n",
        "# =============================== Training Setup ==========================================\n",
        "\n",
        "# Instantiate the model\n",
        "model = SwinUNet(input_channels=3, output_channels=1,\n",
        "                 filter_num_begin=32, depth=4, stack_num_down=2, stack_num_up=2,\n",
        "                 num_heads=[4, 8, 8, 8], window_size=[4, 2, 2, 2], num_mlp=512,\n",
        "                 shift_window=True)\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model = model.to(device)  # Move to GPU if available\n",
        "\n",
        "# Initialize weights using Kaiming Normal initialization\n",
        "def initialize_weights(module):\n",
        "    if isinstance(module, (nn.Conv2d, nn.ConvTranspose2d)):\n",
        "        nn.init.kaiming_normal_(module.weight, mode='fan_in', nonlinearity='relu')\n",
        "        if module.bias is not None:\n",
        "            nn.init.zeros_(module.bias)\n",
        "    elif isinstance(module, nn.Linear):\n",
        "        nn.init.kaiming_normal_(module.weight, mode='fan_in', nonlinearity='relu')\n",
        "        if module.bias is not None:\n",
        "            nn.init.zeros_(module.bias)\n",
        "    elif isinstance(module, nn.Conv3d):\n",
        "        nn.init.kaiming_normal_(module.weight, mode='fan_in', nonlinearity='relu')\n",
        "        if module.bias is not None:\n",
        "            nn.init.zeros_(module.bias)\n",
        "\n",
        "model.apply(initialize_weights)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = DiceLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "# Define learning rate scheduler and early stopping parameters\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.25, patience=5, verbose=True, min_lr=1e-9)\n",
        "early_stopping_patience = 9\n",
        "best_val_loss = np.inf\n",
        "epochs_no_improve = 0\n",
        "\n",
        "# =============================== Training Loop ===========================================\n",
        "\n",
        "num_epochs = 1  # You can adjust the number of epochs\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    train_loader_count = 0\n",
        "    for images, masks in train_loader:\n",
        "        train_loader_count += 1\n",
        "        print(f\"Train loader batch count: {train_loader_count}\")\n",
        "        images = images.to(device)  # (B, 3, 256, 256)\n",
        "        masks = masks.to(device)    # (B, 1, 256, 256)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)      # (B, 1, 256, 256)\n",
        "        loss = criterion(outputs, masks)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader.dataset)\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for images, masks in valid_loader:\n",
        "            images = images.to(device)\n",
        "            masks = masks.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, masks)\n",
        "\n",
        "            val_loss += loss.item() * images.size(0)\n",
        "\n",
        "    val_loss /= len(valid_loader.dataset)\n",
        "\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, Training Loss: {epoch_loss:.4f}, Validation Loss: {val_loss:.4f}')\n",
        "\n",
        "    # Scheduler step\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "    # Early stopping\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        epochs_no_improve = 0\n",
        "        # Save the best model\n",
        "        torch.save(model.state_dict(), r'/content/drive/MyDrive/model/modelWeights_Swin_UNet.pth')\n",
        "    else:\n",
        "        epochs_no_improve += 1\n",
        "        if epochs_no_improve >= early_stopping_patience:\n",
        "            print('Early stopping!')\n",
        "            break\n",
        "\n",
        "# ================================== Prediction ==========================================\n",
        "\n",
        "# Load the best model weights\n",
        "model.load_state_dict(torch.load(r'/content/drive/MyDrive/model/modelWeights_Swin_UNet.pth'))\n",
        "model.eval()\n",
        "\n",
        "# Function to save predictions and ground truth\n",
        "def save_predictions(model, dataloader, save_dir_pred, save_dir_gt, device):\n",
        "    \"\"\"\n",
        "    Saves the predicted masks and ground truth masks.\n",
        "    Args:\n",
        "        model (nn.Module): Trained model.\n",
        "        dataloader (DataLoader): DataLoader for test data.\n",
        "        save_dir_pred (str): Directory to save predicted masks.\n",
        "        save_dir_gt (str): Directory to save ground truth masks.\n",
        "        device (str): Device to run the model on.\n",
        "    \"\"\"\n",
        "    os.makedirs(save_dir_pred, exist_ok=True)\n",
        "    os.makedirs(save_dir_gt, exist_ok=True)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (images, masks) in enumerate(dataloader):\n",
        "            if (i % 100 == 0):\n",
        "                print(f\"{i}th Test Batch\")  # Adjust as per your dataset\n",
        "\n",
        "            images = images.to(device)\n",
        "            masks = masks.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            preds = outputs.cpu().numpy()\n",
        "            gts = masks.cpu().numpy()\n",
        "\n",
        "            for j in range(preds.shape[0]):\n",
        "                pred_mask = preds[j, 0, :, :]\n",
        "                gt_mask = gts[j, 0, :, :]\n",
        "\n",
        "                # Save predicted mask\n",
        "                pred_img = Image.fromarray((pred_mask * 255).astype(np.uint8))\n",
        "                pred_img.save(os.path.join(save_dir_pred, f\"{i * dataloader.batch_size + j + 1}.png\"))\n",
        "\n",
        "                # Save ground truth mask\n",
        "                gt_img = Image.fromarray((gt_mask * 255).astype(np.uint8))\n",
        "                gt_img.save(os.path.join(save_dir_gt, f\"{i * dataloader.batch_size + j + 1}.tiff\"))\n",
        "\n",
        "# Define directories to save predictions and ground truth\n",
        "save_dir_pred = r'/content/drive/MyDrive/output/segmented_predicted_images'\n",
        "save_dir_gt = r'/content/drive/MyDrive/output/segmented_ground_truth'\n",
        "\n",
        "# Save predictions\n",
        "save_predictions(model, test_loader, save_dir_pred, save_dir_gt, device)\n",
        "\n",
        "# =================================== Evaluation =========================================\n",
        "\n",
        "def evaluate_metrics_pytorch(model, dataloader, device):\n",
        "    \"\"\"\n",
        "    Evaluates various metrics for segmentation performance.\n",
        "    Args:\n",
        "        model (nn.Module): Trained model.\n",
        "        dataloader (DataLoader): DataLoader for test data.\n",
        "        device (str): Device to run the model on.\n",
        "    Returns:\n",
        "        dict: Dictionary containing average metrics.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    all_metrics = {\n",
        "        'Accuracy': [],\n",
        "        'Dice': [],\n",
        "        'Jaccard': [],\n",
        "        'Sensitivity': [],\n",
        "        'Specificity': [],\n",
        "        'Precision': [],\n",
        "        'Recall': [],\n",
        "        'F1-Score': []\n",
        "    }\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, masks in dataloader:\n",
        "            images = images.to(device)\n",
        "            masks = masks.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            preds = outputs > 0.5  # Binary mask\n",
        "\n",
        "            preds = preds.cpu().numpy().astype(np.uint8)\n",
        "            masks = masks.cpu().numpy().astype(np.uint8)\n",
        "            masks = (masks > 0).astype(np.uint8)  # Convert to binary masks\n",
        "\n",
        "            for pred, mask in zip(preds, masks):\n",
        "                pred_flat = pred.flatten()\n",
        "                mask_flat = mask.flatten()\n",
        "\n",
        "                # Calculate metrics\n",
        "                tn, fp, fn, tp = confusion_matrix(mask_flat, pred_flat, labels=[0,1]).ravel()\n",
        "\n",
        "                accuracy = (tp + tn) / (tp + tn + fp + fn + 1e-8)\n",
        "                iou = jaccard_score(mask_flat, pred_flat, zero_division=0)\n",
        "                dice = f1_score(mask_flat, pred_flat, zero_division=0)\n",
        "                specificity = tn / (tn + fp + 1e-8)\n",
        "                sensitivity = recall_score(mask_flat, pred_flat, zero_division=0)\n",
        "                precision = precision_score(mask_flat, pred_flat, zero_division=0)\n",
        "                recall = sensitivity\n",
        "                f1 = dice  # F1-Score is the same as Dice coefficient for binary classification\n",
        "\n",
        "                all_metrics['Accuracy'].append(accuracy)\n",
        "                all_metrics['Jaccard'].append(iou)\n",
        "                all_metrics['Dice'].append(dice)\n",
        "                all_metrics['Specificity'].append(specificity)\n",
        "                all_metrics['Sensitivity'].append(sensitivity)\n",
        "                all_metrics['Precision'].append(precision)\n",
        "                all_metrics['Recall'].append(recall)\n",
        "                all_metrics['F1-Score'].append(f1)\n",
        "\n",
        "    # Compute average metrics\n",
        "    avg_metrics = {metric: np.mean(values) for metric, values in all_metrics.items()}\n",
        "\n",
        "    print(\"Evaluation Metrics:\")\n",
        "    for metric, value in avg_metrics.items():\n",
        "        print(f\"{metric}: {value:.4f}\")\n",
        "\n",
        "    return avg_metrics\n",
        "\n",
        "# Evaluate the model\n",
        "metrics = evaluate_metrics_pytorch(model, test_loader, device)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}